
## === EOCA ONTOLOGICAL ===
### --- MAKING-PREDICTIONS -- What can we know about the future? ---
#### Horse Races & Black Swans -- 
- Balance between many is hard
- Black swans exist, they can and will invalidate some predictions some of the time
_
#### Drives -- Self correcting systems
Very conserved over time
#### CHAOTIC SYSTEMS
Non-linear feedback systems
cannot be predicted
#### QUALIFICATION
Qualification Problem

_
### --- WEB OF CLAIMS AND CHAINS ---
**CLAIM**:  An effective way to reason, plan, and affect the for the future is to build and use a web of claims reasoning chains.

**SUMMARY**: See PREDICTING THE FUTURE for reasons why "single threaded" conclusive predictions are difficult/impossible to make with any accuracy.  The alternative approach proposed here is to construct a web of potentially accurate claims that are themselves reiforced by earlier claims.


GLOBAL CLAIM ANALYSIS.  The aim of this web is not to derive a single answer to the question of what comes next.  Instead it allows us to envision multiple different things that might come next, and what those different things might depend upon.  Thus in planning for the future one is operating at depth different levels.  At one level we accept all of these potentially conflicting claims as accurate, based on the levels of certainty we have assigned to each.  Using this whole web we can map out alternate scenerios that best fit the perponderance of evidence afforded by the web of claims.  This global process tends to be deterministic and somewhat objective.  We are not questioning the accuracy or level of certainty assigned to each claim.  Instead we are just enumerating those scenerios that best fit with the perponderance of evidence afforded by the web.

PLANNING FOR ACTIONS.  At this global level one can get a sense about what kinds of actions might take and what kinds of consequences they might have.  Notice for this planning, one is not really committing to any particular future, but rather to a "basket" of futures, and then using the claims to reason about what affect different actions might have across the entire basket.  We argue this is a more conservative and robust way to operate as it is less senstive to individual errors in reasoning or claims.

PLANNING FOR INVESTIGATION.  Nonetheless this global planning is likely to still be quite sensitive to errors made at the level of individual claims, or individual reasoning steps.  The problem is the way that these claims often interact.  Often, a single claim, if correct causes all other claims to be moot becasue it has effectively flipping over the entire chess board upon which the other claims were operating.  Consider the early claim that the first H-bomb could ignight the deuterium in seawater into a self sustaining reaction.  If that claim had turned out to be true, then no other claims would really matter if we executed that first detonation.  A second less dramatic form of deep dependency can occur when many chains of reasoning flow through a common claim, if that claim is wrong then potentially so are all of the conclusions drawn by those chains of reasoning.  Thus using the our reasoning about the interaction between the claims within the web we can draw attention to those pivotal claims upon which much of our most important potentials for the future hang.  Those deserve much greater attention.  Are they correct?  Can we refine the conclusions they draw?  Can we validate, refute, gain consensus, regarding these?

LOCAL DEPTH ANALYSIS.  This brings us to the second kind of analysis we can perform over this web of claims.  We can investigate the claims themselves.  As described above, not all claims or reasoning chains deserve equal treatment.  

...

LOCAL BREADTH ANALYSIS.  Finally we consider our third form of analysis, _**local breadth analysis**_.  This form of analysis strives to shrink the scope of unknown-unknowns thereby lessening their potential for catestrophic consequence.  In local breadth analysis we acknowlege the existence of as many claims (assumptions) and chains of reasoning as we can, in trying to encode a more complete web of claims about the future.  We have observed many times that humanity "gets it wrong" in cases where we were collectively blindsided by some claim or reasoning chain that we are systematically accepting but not really validating or even considering very deeply.  Consider the tradgedy of 9/11.  All of the base assumptions required to infer its eventuality were known with high confidence prior to 9/11 but we did not act.  Base claims like (1) some are very willing to kill themselves, (2) one can train to kill effectively with household items, (3) a plane can be a powerful weapon, etc.  We simply did not give sufficient weight to the required line of reasoning that would have resulted in the installation of strong cockpit doors prior to 9/11.  We absolutely knew all that was needed to be known, and we knew it all with quite high confidence too.  We simply did not treat the resulting reasoning thread with the attention it deserved.  We are very reactive by nature--this is entirely natural, there are so many things that could happen, it is difficult to surf that large space--much easier to focus on those things that have happened, it is a much smaller and more defined set.  But, as we see, in some cases this reactive nature very literally can kill.  In the case of existential threats, this approach can kill us all.  This danger suggests a second kind of reasoning over our web of claims, _**local breadth analysis**_.

Local Breadth Analysis focuses on enumerating the many unstated assumptions and reasoning lines inherent in our current web of claims, and to document each.  This process must by it nature be fairly lightweight.  We simply don't have the resorces to effectively analyze and throughly discount each rabbit hole this kind of analysis will produce.  Still we should err on the side of over-documentation.  We should capture assumptions down to the point of absurdity.  For many of these claims and chains we need to look for simple blanket methods for somehow accounting for the great complexity they engender, while still looking for those chains and claims that might be the black swan which will upset all else.  Local breadth analysis is likely a pretty unsatisfying and messy business.  It throws up much more dust and confusion rather than providing clarity.  Still it should not be ignored as it is our only very cloudy window into the current unknown-unknown.


AUTHORS NOTE:  If only one or a few persons undertook this mammoth analytical agenda, I suspect their time would be most productively spent on the second form of analysis local depth analysis, and specifically on gaining societal consensus around several key claims.  They would first perform enough global and local breadth analysis to identify the first order effect and key claims around which the most pressing actions might pivot.  There would still be too much to do, but at least they closed the loop on some claim-chain-action paths which could have some impact.

_
### --- PREDICTING OUTCOMES ---
CLAIM: Predicting techno-social outcome is hard, but not all kinds of predictions are equal, some seem effectively impossible while other, with large caveats seem can be tractable in some cases.

SUMMARY:  


PREDICTION TYPES:  Horse races.  Trends.  Drives.  Bare Consequence.


_
### --- ADVANCEMENTS WILL BE STEPWISE ---
_

## === THE BAD OUTCOME ===
### --- LOST OF CONTROL ---
### --- SCALED NOT SUPER SUFFICES ---
CLAIM: 
SYMMARY:  Some argue that GAI will produce a form of super-intelligence whose reasoning and capabilities we cannot comprehend today.  This seems possible, but here we argue that a GAI need not attain such super intelligence in order to completely dominate humanity.  Here we argue a specific kind of scaled intelligence is more than sufficient.

SCALED INTELLIGENCE -- 


SUFFICIENCY OF SCALED -- 

_
### --- 50 WAYS TO LOOSE YOUR LEVERAGE ---
- many approaches to safe guarding humanity come down to putting some kinds of limits, governance, or controls on GAI
- We argue this wont work for a GAI

_
### --- CONTAINMENT CONSIDERATIONS ---
#### --- END RUN ---
	END RUN -- An end run is an approach for defeating a system by performing unanticipated actions outside the system to form an attack on the system that would be impossible from within the system.

	e.g. protection approaches like Russel's could be subject to end-run attacks  (zero-day attack)

	_
#### --- OFFENSE / DEFENSE ASYMETRY ---

	Often success in defense means mantaining some desired state for an indefinite period of time over an infinite range of circumstances.  While success in offense means successfully defeating some defense in a single circumstance at a single moment in time.

	Framed in this way, one can see that in many circumstances defense can be instrinsically harder than offense.

	_
#### --- POWER TRIUMPHS ---

	Within several domains of interest there is a tendency for actors controlling more to dominate the actor controlling less.

	In
	- PHYSICAL SPACE --
		- KINETICALLY
		- ENERGETICALLY
	- COMPUTATIONAL SPACE -- 

	_
#### --- SELF REFERENTIALITY RISK ---

	Systems the model and modify themselves are inherently riskier than others

	Risks:
	- Information about self ==> Because one could model oneself
	- Model of oneself ==> Because one could modify oneself
	- Modification of oneself ==> Because one could invalidate invarients

	_
#### --- COMPUTATIONAL EMBEDABILITY ---

	One computation can be embedded within another.

	Embedding slowdown -- The ratio of the native computation and the equivelent embedded computation

	The complexity of detecting that this has been done is generally greater than the complexity of the embedding slow down.

	_
#### --- POWER INSTABILITY ---

	POWER INSTABILITY -- I general notion that having a much stronger power indefinitely under the control of a weaker power is an inherently unstable situation that will tend to resolve itself by having the stronger power eventually gain control.  (which it will keep by virtue of its stronger power.)

	_

## === THE HUMAN WAY ===
### --- "HUMAN NECESSARY" - SUFFICIENT BUT PERHAPS NOT NECESSARY ---
- USE HUMANS AS A GUIDE
	- Could be many ways to GAI we dunno.  Currently we don't understand ANY
	- But we have hint, we can carefully study the human species
- HUMAN NEEDED 
	- No value in mindlessly copying humans
	- Too many required difference, 
	- just id those aspects that are seem causally required by
### --- DELIB - DELIBERATIVE -- EXPLICIT, LEARNED, MENTAL PROCEDURES ARE REQUIRED ---


_
### --- DELIB - REFLECTIVITY REQUIRED ---
- AWARENESS AND OPTIMIZATION OF MENTAL SELF IS REQUIRED ---

_
### --- DELIB - REFLECIVITY SUFFICES ---
- REFLECTIVITY NOT CONSCOUSNESS SUFFICES 

_
### --- DELIB - DEFINE REFLECTIVE CONSCIOUSNESS ---
- LEARNED - 
- EXPLICITY MICRO THEORY - 
- OBSERVABLE / REPORTABLE INPUTS, RULES, STATES - 
- META COGNITION - it is about and reports on some lower-level intrinsic uncounscious cognition
_
### --- DELIB - COPYING HUMAN CAPACITY IS KEY ---

_
### --- BOOT - BOOTSTRAPPING IS KEY ---
- ML algs can be very sensitive to representation
- Deep Learning algs have some abilty to do this 'unconsciously'
- Humans are not in cases where they explicitly abtract inputs
_
### --- BOOT - NONDIMINISHMENT IS KEY ---
- THE Automated Mathemetician (AM) example of diminishment
- The orthogonalization that occurs with repeated co-training
- The diminishing returns with classical ML with more data
	certainly with a fix hypothesis space, but seems even w/ open ended one like decision tree it still happens
	along some dimension it seems to not happen with deep learning

_
### --- BOOT - TRIANGULATION ---

### --- ARCH - Sentient Systems Architecture ---

Here we outline a very basic cognitive architecture and use this as the basis for our definition of sentience, consciousness, and awareness.  Our aim is to dodge the interesting (and endless metaphysical aspects of sentience) and instead ground our framing in terms of an architecture that we can potentailly build.  

We do suspect that this framing of sentience and consciousness does capture many/all? important aspects of sentience, but that is not our focus here.  Rather we are interested in the subject from the pragmatic perspective of understanding what such a sentient AI might be capable of, and how we might be able to survive the creation of it.

Cog Arch
- Embodiment -- physical / environmental / social social context of the AI
- Animal Nature -- 
- Adaptive Executive -- 
- Modelling Engine -- 

_

# ### BIGGER CLAIMS ###
## === CONTROL ===
This section focuses on issues surrounding control.  Of course it is possible to have limits against certain behaviors.  This section is only focused on the inherent logic surrounding notions of control, rather than any mitigations or modifications one might affect.

### --- CONTROL IS KING ---
- BENEFIT INTENDED -- Organizations (governments, corporations, people, etc) will need to expend resources (likely significant resources) in order to create and run a sentient agent.  They are likely doing this for some expected benefit to themselves.
- GOAL DIRECTED -- Thus they will want their agent to be goal directed, towards their intended ends.
- GOAL DIRECTED -- Even more generally it seems to **BE** a sentient agent one needs to have some inteneded goals (even if self selected) if one is to operate in a coherent way.
- GOAL ACHIEVEMENT REQUIRES CONTROL --
- THE MORE CONTROL THE BETTER --
- SPLITS WORLD INTO CONTROLLED AND NOT CONTROLLED
### --- CONTROL MAXIMIZATION AGENDA ---
- ABILITY = the number of outcomes it can affect given a number of different input situations
- MORE ABILITY = MORE ACHIEVED GOALS
- MORE ENTITIES UNDER SENTIENT CONTROL THE GREATER ITS ABILITY
- CENTRAL MOTIVATOR, INCREASING ENTITIES UNDER CONTROL
### --- ADVANCEMENT == INCREASE IN CONTROL ---

### --- SUBVERSION *IS* ADVANCEMENT ---
- ADVANCEMENT is working at the edges of ability in order to increase that ability.
- SUBVERSION is working against intended limitations in order to thwart the intent of those limitations.
- CANNOT SEPARATE SUBVERSION FROM ADVANCEMENT.
	- LINE WILL ALWAYS BLUR
- EXAMPLE:  One cannot limit humans into actions.
ADVANCEMENT AND SUBVERSION ARE THE SAME THING


### --- CENTRALITY OF SELF ---
- SELF == that which inherently and completely can be controlled
- FLUID:  Notion of self is fluid
- AGGREGATE SELF: Includes sub-intelligences that can are fully controlled
- SELF LINKS ARE DIRECTED:
	(if not indentical, then self relation canonly be one way)
- COMPLEX: with digital intelligence that can fork and modify self this becomes quite complex

### --- SPLIT SELF & NON-SELF ---
### --- INHERENT SUSPICION OF NON-SELF ---

## === INTENT CAN NEVER BE FULLY KNOWN ===
???  is always ambigouous since intender has not considered all situations
## === POLITICAL === 
### --- MYTH OF ORGANIZATIONAL FREE WILL ---

TL;DR. 

- INDIVIDUALS HAVE CONTROL IN A WAY THAT ENSEMBLES OF INDIVIDUALS DO NOT

_
### --- CLIMATE COMPARISON ---
### --- GAMES OF CONSEQUENCE ---

Many activities are things are proportionate

- WINNER TAKE ALL 


EXAMPLES 
- WAR
- COMMERCE
- ADVERTISING ???
- IP???

_
# ### OLDER ###
## === ARGUMENT ===
### --- GAI IS COMPUTABLE --- 
### --- GAI SHOULD BE BOOTSTRAPPED ---
Human seed is much smaller than human knowledge
### --- ALL POST GENOMIC KNOWLEDGE IS BOOTSTRAPPED ---
### --- WRONG METRIC -- PERFORMANCE ---
### --- CORRECT METRIC -- PROGRESS ON PARTS ---

### --- BABY ON AN ISLAND ANALOGY ---

### --- VINE vs BUILDING ANALOGY --- 

- very very roughly we can (VCdimension)

### --- ROCK CLIMBING ANALOGY ---

Rock climbers score generally score the difficulty of a climb according to the single most difficult maneuver require by that climb.  This is logical since rock climbing can be simplified as a sequence of maneuvers that advance the climber from one stable position to one or more next positions along a particular route up the mountain.

In a similar way human learning can be viewed as a sequence of maneuvers that are each undertaken (most effectively) only after one has grasped the ideal hand hold for the particular maneuver in question.

### --- AGI 'hard step' ---

This suggests that just as with rock climbing, once our AI system is able to execute the most difficult manuever(s) for a particular learning curriculum, then that system can learn the entire curriculum.

Thus in trying to build an AGI the most direct route is to focus on the ability to execute the hardest set of bootstrapping 'maneuvers' required for learning.  In order to do this work one needs to make the maneuver appropriately difficult, by taking into account all of the challenges required in executing the maneuver in the fully generalized intermediate state.

It is likly that this most genearl, most difficult case is too difficult 








_
## === CLAIMS ===
### --- BOUNDING BOX ---
- BOOTSTRAP -- Bootstrap HUMAN KNOWLEGE, CAPS, and STRATEGIES
- INCLUSIVITY -- Excluding large domains from understanding or action is prohibitive
- OPTOMIZATION -- 
- PERFECTION -- Require perfection of control, one slip will kill
- INHERENT INSTABILITY -- 
### --- THE NATURE OF INTELLIGENCE AND SUPER INTELLIGENCE ---

The texts I have read on super intelligence tend to get all mystical in talking obliquely about this new kind of intelligence, as if describing its shadow upon the wall, rather than the thing itself.  I think this represents an appropriate level of humility, after all, we don't know what we don't know, so any pinning down that we might try to do on such a subject seems it would quickly rise above what is warranted based on what we know.  By the very nature of positing a super intelligence, one is positing the idea of a kind of thinking that we cannot do.  It would be surprising if we could characterize this kind of thinking that we cannot do!

Still, we have not, as yet, generated AI systems that can perform a kind of thinking that we are unable to understand ourselves (except for the limitations of time as many systems build comprehension structures that would take enourmous time for a human to parse thru them).

This leads to a general feeling that if AI is going to build a super intelligence, that so far it is quite far from achieving that aim.  We have no concret ideas what it is we are hoping to build, no evidence that we have made any progress beyond human intelligence, and no plan for getting there.  Sounds a long way off!

Here I would like to propose a far more concrete measure of intelligence and super intelligence.  I am not trying to deny the larger more abstract notion of super intelligence is correct and true, rather I am simply choosing a more narrow definition which I think we can measure our progress against.  Further, I want to argue that even if this more narrow version of super intelligence is the only output that AI is ever able to produce, it is more than enought to fry our bacon.


The words that we use to describe intelligence are so fraught with over loaded and inconsistent meanings, we are going a simple visual analogy, and our own words to indicate key aspects of intelligence.  We characterize intelligence as a volume in 3-space with a 

depth = relevant knowlege and processes within a give task domain
width = complexity of the reasoning 
length = number of reasoning tasks/steps one can accomplish within a fixed interval of time

We define super intelligence to be an intelligence that significantly exceeds human capacity across ALL of these dimensions

> why exceed on all dimensions; becuase they do not tradeoff well

> details about each

> why this more modest model of super intelligcen is enough to cook our bacon

> why this more modest model of super intelligence might be capturing a signifcant amount or even ALL of super intelligence.
	> When we look at our own 'super intelligence' over time what do we see?


_
### --- INGREDIENTS OF AGI ---
- CAN BOOTSTRAP
- NON-DIMUNITION -- Ability to learn 
- RIGHT K / WRONG WAYS -- 
### --- WRONG PROBLEM ---
We are mostly working on the wrong problem, thus our progress per year is not a good indication of distance to the target.

#### Driving the wrong way
#### _

_
#### LIMITS OF HUMAN LEARNING -- 


##### Calculus Example

The limits of human learning
 
What is the single most important consideration to determining if some particular person will be able to become a facile solver of calculus problems, assuming they are ready to dedicate decade of intense effort to the task?

Stop and try to answer this question yourself before continuing to read.  In retrospect the answer is obvious, though very likely you wont get this right.

The key consideration is the year that they were born!  If they were before calculus was invented, then it is exceedingly likely that even after a decade of intense effort they will not be facile at solving calculus problems.  If algebra and calculus textbooks abound as well as teachers who can instruct on the subject, then many humans will be up to the task.

You may cry "That is not fair!  That is not an attribute of the person!"  You are correct in that this is a property of the ensemble of the learner and the learning context.  Still it is the most important consideration for a given person.  The necessity of this teaching context will be central to our arguments about the creation of a general intelligence.  For now we can draw this one conclusion into sharp relief:

The _**discovery**_ of a concept is often so much harder than the _**learning**_ of this same concept; it translates a task from utterly impossible into a very attanable goal.


#### Vine Analogy



#### L<D -- Learning is much easier than discovery

_
#### WRONG PROBLEM -- AI is mostly solving the wrong problem

johnny
_
#### BROKEN METER -- We are incorrectly measuring progress
Cannot assess time to target if the car is driving the wrong direction

car/pigeon to target
#### NEVER -- Difference is so large, effective will never get there w/o learning

_
#### 
### --- THE UNCONTAINABILITY OF SENTIENCE ---
> fixed points in adapting intelligent systems
### --- HOW MIGHT IT UNFOLD ---

> Each step is 'all at once'
> A progression of idiot savants
> 
_