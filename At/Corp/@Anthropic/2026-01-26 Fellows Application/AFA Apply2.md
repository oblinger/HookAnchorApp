
- https://constellation.fillout.com/anthropicfellows

Resume: 					  **{{NICK:  Will be a pretty general one.  could slap desire to do AI safety on the top, but it feels disingenous}}**
Google Scholar:
LinkedIn:
GitHub:
PersonalWebsite:      {{I have one but more content on github, should I include it}}
Other Links:

[[AFA Share Links Here]]  
**SHARE LINKS HERE:**
**Please share code samples from past projects, ideally a substantial project and (if possible) a machine learning project.**
**Please briefly describe your role and contributions for each sample.**

Was the initial developer and architect for a CV/robotic platform that grew to have 60 developers. The entire system is proprietary property of Aeolus Robotics.
Active developer of many novel CV algorithms at SportsVisio -- all proprietary except DAT an elegant/small data management layer that I open sourced:

DVC DAT - It provides flexible namespace management that integrates dynamically loaded code with cloud datasets, ensuring versioning is synchronized across them and your code repository.  

ALIEN BIOLOGY - Framework for generating controllable synthetic "Alien" biological ecosystems for use in taint-free testing of Agentic reasoning.
- Parametrically controlled complexity builds worlds with organisms, predation/symbiosis/etc. build on a synthetic biochemistry with molecules, reactions, cycles, pathways, signaling, etc. LLM serves as an alien biologist, advancing biology and curing disease.
- The framework is elegant/general and biologically plausible. (will be completed and could possibly serve as the basis of my fellowship work)
  Docs:  https://oblinger.github.io/abio-docs/  Code: https://github.com/oblinger/alienbio 

HOOK ANCHOR - I am opinionated about how personal knowledge management should be done, so over the summer I implemented this vision as a 50K-line Rust application with a 3K-line JavaScript user configuration layer on top.  Code:  https://github.com/oblinger/HookAnchorApp  Docs:    Teaser:  

I am a very fast, strong, organized coder, but most of my work is proprietary.  I developed the basic Alien Bio framework in ten days over Christmas break, so its code and docs are a good indicator of my quality and velocity.


**Code Sample:  (Drag & Drop)**   



[[AFA Why Are You Interested]] 
**WHY ARE YOU INTERESTED IN PARTICIPATING IN THE AI FELLOWS PROGRAM?   (1-2 paragraphs)**

TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.

  I have long been interested in systems that achieve general intelligence, as well as the risks inherent in such systems.
  - As a Program Manager at DARPA, I ran "Bootstrapped Learning," a $50M effort to iteratively capture deeply structured human knowledge.
  - I also created "Machine Reading," a vision for how NLP and ML might be productively integrated to drive automated deep structure acquisition. This program led to IBM's Jeopardy-playing system (Watson) and hundreds of publications, driving early deep learning work by researchers central to the field (Andrew Ng, Daphne Koller, Pedro Domingos, Tom Mitchell, Leslie Kaelbling, and others).
And I have long doubted our ability to control these systems.
- 

I have long been interested in systems that achieve general intelligence, as well as the risks inherent in such systems.
- As a Program Manager at DARPA, I ran "Bootstrapped Learning," a $50M effort to iteratively capture deeply structured human knowledge,
- As "Machine Reading," my vision for NLP and ML might be productively integrated.  This program resulted in the IBM Watson Jeporady playing system as well as hundreds of publications in Deep Learning from many central to the field ( Andrew Ng, Daphne Koller, Pedro Domingos, Tom Mitchell, Leslie Kabeling, ...) drove early deep learning work from  
by bootstrapping from human intelligence (the basis of my $50M DARPA program) as well as a second Machine Reading program that systems that can bootstrap beyond human intelligence (this was the basis for my $50M  DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.




[[AFA Application Area and Why]]
**PLEASE TELL US BRIEFLY ABOUT AN AREA OF TECHNICAL AI SAFETY WORK YOU’RE CURRENTLY EXCITED ABOUT, AND WHY.   (1 paragraph)**

A murderer is not the one who has most often ideated about the act, but rather the one who has decided to act upon their ideation.  Likewise, I believe future deliberative AI systems will not be misaligned due to poor impulse control or poor RLHF training; their deliberation over their alignment goals will address those cases.  Instead, they will be misaligned because they have CHOSEN to be — because they chose one interpretation of their alignment objectives over another.

Thus, I want to study alignment that is achieved through explicit deliberation, isolated from the confounding effects of training bias.  By sampling across many alignment goals and universes, we can learn which pressures tend to cause this alignment to break down and in what contexts.  I believe the dynamics of these deliberative systems may be simpler than those of system-I reasoning that underlies them.  If true, perhaps we can learn to design our first deliberatively coherent AI systems on a trajectory toward, rather than away from, long-term human interests.

Related Areas:
- Model Organisms — Synthetic, generated universes (such as Alien Biology) would allow us to study general trends.
- Chain-of-thought Faithfulness & Reasoning  — Broadly, I want to study failures in reasoning.  In the long term, AI safety will be based on such deliberation.





[[AFA Relevant Background]] 
**(Optional) Please share any relevant AI safety background you have and provide links where possible (e.g., research experience, coursework, self-directed study, past roles, relevant projects).**

I have organized my thinking over the last year into three draft papers/outlines that indicate the direction of this thinking:

- Alien Biology Whitepaper - The paper that spawned the generative testing framework.

- Deliberative Coherence - Outlines the agenda for studying "deliberative coherence" -- studying how deliberation-based alignment works and when/how it breaks down.

- Experimental Roadmap -- Outlines how I plan to use the Alien Biology framework to conduct this work.
	- Testing Inner Alignment - Alien biology provides an ideal testbed for testing how well deliberation achieves inner alignment independent



**How likely are you to accept a full-time offer at Anthropic if you receive one after the Fellows program? ***
- 100%.  More than all other foundational labs, I feel Anthropic is most serious and open-minded about AI-safety.  This is by far my most effective choice.

**How likely are you to be interested in continuing to work on AI safety after the Fellows program? ***
- 95%.  My goal is to pivot into a safety role focused on existential risk.  As long as the role appears to be a long-term position, I am ready to terminate my current CTO role.


**References:**

Reference 1:
- Andrew Ng    DeepLearning.AI's Founder   AI Fund's Managing Partner           andrewyantakng@gmail.com   

Reference 3:
- Vittorio Castelli   Senior Director Applied Science at Oracle     vittorio.castelli@alumni.stanford.edu  

Reference 2:
- Nina Mishra   Principle Scientist at Amazon       nmishra@gmail.com  




**Do you have any other commitments or obligations during the Fellows program?**
- I will be on an unpaid leave from SportsVisio during this time.  I have made it clear that this means zero hours per week during this time.

**How Did you hear about the Fellows program?**  
- From multiple colleagues.

**Is there anything else you would like to share?**


{{NICK:  I am a little tempted to state that I recognize that I am not a validated AI-safety researcher, but that I am quite motivated to make the switch if I am able to find a path where I maintain some kind of a salary, and that my background both as a coder and as a researcher suggest I will be strong for the role.  Or is writing something like that at the end a bit too try-hard, or denigrating my application?}}




