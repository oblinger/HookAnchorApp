
- https://constellation.fillout.com/anthropicfellows

Resume: 					  **{{NICK:  Will be a pretty general one.  could slap desire to do AI safety on the top, but it feels disingenous}}**
Google Scholar:
LinkedIn:
GitHub:
PersonalWebsite:      {{I have one but more content on github, should I include it}}
Other Links:

[[AFA Share Links Here]]  
**SHARE LINKS HERE:**
**Please share code samples from past projects, ideally a substantial project and (if possible) a machine learning project.**
**Please briefly describe your role and contributions for each sample.**

Was the initial developer and architect for a CV/robotic platform that grew to have 60 developers. The entire system is proprietary property of Aeolus Robotics.
Active developer of many novel CV algorithms at SportsVisio -- all proprietary except DAT an elegant but small data management layer we open sourced:

DVC DAT - It provides flexible namespace management that integrates dynamically loaded code with cloud datasets, ensuring versioning is synchronized across them and your code repository.  

ALIEN BIOLOGY - Framework for generating controllable synthetic "Alien" biological ecosystems for use in taint-free testing of Agentic reasoning.
- Parametrically controlled complexity builds worlds with organisms, predation/symbiosis/etc. build on a synthetic biochemistry with molecules, reactions, cycles, pathways, signaling, etc. LLM serves as an alien biologist, advancing biology and curing disease.
- The framework is elegant/general and biologically plausible. (will be completed and could possibly serve as the basis of my fellowship work)
  Docs:  https://oblinger.github.io/abio-docs/  Code: https://github.com/oblinger/alienbio 

I am a very fast, strong, organized coder, but most of my work is proprietary.  Alien Bio was developed over my Christmas break, so it is a good indicator of my quality and velocity.

{{NICK:  Over the summer, I wrote a 35K line personal knowledge management tool in Rust, Javascript and Swift. It is not open source, but I could share the codebase if it would add value}}


**Code Sample:  (Drag & Drop)**   
{{NICK:  I only plan to provide Github links}}



[[AFA Why Are You Interested]] 
**WHY ARE YOU INTERESTED IN PARTICIPATING IN THE AI FELLOWS PROGRAM?   (1-2 paragraphs)**

TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.

I have long been interested in systems that can bootstrap beyond human intelligence (this was the basis for one of my DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.




[[AFA Application Area and Why]]
**PLEASE TELL US BRIEFLY ABOUT AN AREA OF TECHNICAL AI SAFETY WORK YOU’RE CURRENTLY EXCITED ABOUT, AND WHY.   (1 paragraph)**

A murderer is not the one who has most often ideated about the act, but rather the one who has decided to act upon their ideation.  Likewise, I believe future deliberative AI systems will not be misaligned due to poor impulse control or poor RLHF training; their deliberation over their alignment goals will address those cases.  Instead, they will be misaligned because they have CHOSEN to be — because they chose one interpretation of their alignment objectives over another.

Thus, I want to study alignment that is achieved through explicit deliberation, isolated from the confounding effects of training bias.  By sampling across many alignment goals and universes, we can learn which pressures tend to cause this alignment to break down and in what contexts.  I believe the dynamics of these deliberative systems may be simpler than those of system-I reasoning that underlies them.  If true, perhaps we can learn to design our first deliberatively coherent AI systems on a trajectory toward, rather than away from, long-term human interests.

Related Areas:
- Model Organisms — Synthetic, generated universes (such as Alien Biology) would allow us to study general trends.
- Chain-of-thought Faithfulness & Reasoning  — Broadly, I want to study failures in reasoning.  In the long term, AI safety will be based on such deliberation.





[[AFA Relevant Background]] 
**(Optional) Please share any relevant AI safety background you have and provide links where possible (e.g., research experience, coursework, self-directed study, past roles, relevant projects).**



**How likely are you to accept a full-time offer at Anthropic if you receive one after the Fellows program? ***
- 100%.  More than all other foundational labs, I feel Anthropic is most serious and open-minded about AI-safety.  This is by far my most effective choice.

**How likely are you to be interested in continuing to work on AI safety after the Fellows program? ***
- 95%.  My goal is to pivot into a safety role focused on existential risk.  As long as the role appears to be a long-term position, I am ready to terminate my current CTO role.


**References:**

Reference 1:
- Andrew Ng    DeepLearning.AI's Founder   AI Fund's Managing Partner           andrewyantakng@gmail.com   

Reference 3:
- Vittorio Castelli   Senior Director Applied Science at Oracle     vittorio.castelli@alumni.stanford.edu  

Reference 2:
- Nina Mishra   Principle Scientist at Amazon       nmishra@gmail.com  




**Do you have any other commitments or obligations during the Fellows program?**
- I will be on an unpaid leave from SportsVisio during this time.  I have made it clear that this means zero hours per week during this time.

**How Did you hear about the Fellows program?**  
- From multiple colleagues.

**Is there anything else you would like to share?**


{{NICK:  Anything here?}}




