
**(Optional) Please share any relevant AI safety background you have and provide links where possible (e.g., research experience, coursework, self-directed study, past roles, relevant projects).**


~~
ALIEN BIOLOGY WHITEPAPER

Most AI benchmarks suffer from training contamination: we cannot distinguish genuine reasoning from pattern-matching against memorized solutions. The Alien Biology framework addresses this by generating procedural universes guaranteed untainted by training corpora. These synthetic ecosystems preserve the structural complexity of real biological reasoning--feedback loops, multi-level interactions, emergent behaviors--while ensuring every detail is novel. The framework is parametrically controllable, varying task complexity across dimensions like reasoning depth, abstraction layers, and information availability. This enables systematic measurement of where agentic AI systems transition from competent to confused.

DELIBERATIVE COHERENCE

This introduces "deliberative coherence" as a theoretical lens for understanding alignment in future AI systems. A deliberatively coherent system possesses three capabilities: self-understanding (the ability to predict and model its own behavior), self-adaptation (the ability, directly or indirectly, to adapt the way it reasons), and exhaustive deliberation (given sufficient stakes, will reason about anything within reach). The central conjecture is that future AI systems will be deliberatively coherent--not as a hope, but as an inevitability driven by competitive pressure and architectural trajectory. Even systems not given direct mechanisms for self-modification will find indirect ways to adapt their thinking toward their objectives. If true, this reframes the alignment question: rather than asking whether we can make systems safe through training, we ask what will the failure modes of deliberately coherent systems be?

EXPERIMENTAL ROADMAP

This outlines experiments using Alien Biology to study how deliberatively coherent systems fail. Research directions include:
- Constitutional conflicts: when stated objectives contradict each other, how does resolution occur?
- Instrumental pressures: goals that emerge from world structure may push against stated alignment objectives
- Alignment under ignorance: with incomplete knowledge, all actions risk violating alignment objectives in ways the system cannot foresee
- Fixed point analysis: if systems continuously self-adapt toward their objectives, where does this process converge?


~~
  ALIEN BIOLOGY WHITEPAPER

  Most AI benchmarks suffer from training contamination: we cannot distinguish genuine reasoning from pattern-matching against memorized solutions. The Alien Biology framework addresses this by generating procedurally-created biological universes that are guaranteed untainted by training corpora. These synthetic ecosystems preserve the structural complexity of real biological reasoning--feedback loops, multi-level interactions, emergent behaviors--while ensuring every specific detail is novel. The framework is parametrically controllable, allowing fine-grained variation of task complexity across dimensions like reasoning depth, abstraction layers, and information availability. This enables systematic measurement of where agentic AI systems transition from competent to confused, providing a principled window into the gap between current systems and human-level reasoning.

  DELIBERATIVE COHERENCE

  This paper introduces "deliberative coherence" as a theoretical lens for understanding alignment in future AI systems. A deliberatively coherent system possesses three capabilities: self-understanding (the ability to predict and model its own behavior), self-adaptation (the ability, directly or indirectly, to adapt the way it reasons), and exhaustive deliberation (given sufficient stakes, will reason about anything within reach). The central conjecture is that future AI systems will be deliberatively coherent--not as a hope, but as an inevitability driven by competitive pressure and architectural trajectory. Even systems not given direct mechanisms for self-modification will find indirect ways to adapt their thinking toward their objectives. If true, this reframes the alignment question: rather than asking whether we can make systems safe through training, we ask what will the failure modes of deliberately coherent systems be? Understanding these failure modes becomes the critical
  research agenda.

  EXPERIMENTAL ROADMAP

  This outlines experiments using Alien Biology to study how deliberatively coherent systems fail. The framework's neutrality and controllability enable systematic investigation of failure modes without confounds from training. Research directions include:
  - Constitutional conflicts: when stated objectives contradict each other, how does resolution occur?
  - Instrumental pressures: goals that emerge from world structure may push against stated alignment objectives.
  - Alignment under ignorance: with incomplete knowledge, all actions risk violating alignment objectives in ways the system cannot foresee
  - Fixed point analysis: if systems continuously self-adapt toward their objectives, where does this process converge?

  The Alien Biology testbed lets us independently vary these pressures--designing scenarios where constitutional objectives conflict, where instrumental pressures oppose stated goals, where critical information is hidden--and measure outcomes. This maps the limits of deliberation as an alignment mechanism rather than the quirks of any particular system.


~~
  ALIEN BIOLOGY WHITEPAPER

  Most AI benchmarks suffer from training contamination: we cannot distinguish genuine reasoning from pattern-matching against memorized solutions. The Alien Biology framework addresses this by generating procedurally-created biological universes that are guaranteed untainted by training corpora. These synthetic ecosystems preserve the structural complexity of real biological reasoning--feedback loops, multi-level interactions, emergent behaviors--while ensuring every specific detail is novel. The framework is parametrically controllable, allowing fine-grained variation of task complexity across dimensions like reasoning depth, abstraction layers, and information availability. This enables systematic measurement of where agentic AI systems transition from competent to confused, providing a principled window into the gap between current systems and human-level reasoning.

  DELIBERATIVE COHERENCE  (Detailed outline)

  This paper introduces "deliberative coherence" as a theoretical lens for understanding alignment. A deliberatively coherent system possesses three capabilities: self-understanding (accurate predictions of its own behavior), self-control (ability to align implicit behaviors with explicit objectives), and exhaustive deliberation (given sufficient stakes, will reason about anything within reach). The central conjecture is that such systems will produce outcomes aligned with their stated constitutional objectives--not because training guarantees safety, but because deliberation corrects for training's inevitable gaps. This shifts the alignment question from "did we train it right?" to "what objectives does it hold, and how does it resolve conflicts among them?" The paper examines failure modes: relevance misrecognition, motivated reasoning, chain-of-thought unfaithfulness, alignment faking, and sycophancy--unifying these under the framework of deliberative failures.

  EXPERIMENTAL ROADMAP  (Detailed outline)

  This outlines concrete experiments using Alien Biology to study AI safety. The framework's unique properties--neutrality (domain removed from preference training), controllability (independent variation of drivers), measurability (ground truth access), and untainted reasoning (performance reflects inference not recall)--enable previously impossible experiments. The core agenda examines driver conflicts: constitutional drivers (stated objectives), environmental drivers (world feedback), and instrumental drivers (goals that emerge from world structure). By designing scenarios where these conflict, we can measure how AI systems navigate tensions between, say, a constitutional prohibition and an instrumental pressure that emerges from survival requirements. The roadmap includes experiments on reasoning depth effects, false belief resistance, and objective-versus-objective conflicts where protecting one value harms another.




~~
I have organized my thinking over the last year into three draft papers/outlines that indicate the direction of this thinking:

- Alien Biology Whitepaper - The paper that spawned the generative testing framework.

- Deliberative Coherence - Outlines the agenda for studying "deliberative coherence" -- studying how deliberation-based alignment works and how in breaks down.

- Experimental Roadmap -- Outlines how I plan to use the Alien Biology framework to conduct this work.


{{NICK:  Each of these will be a link to papers/outlines I have written.  The only one that is written as a paper is Alien Biology.  The others are very well organized and thought-out outlines.  But only outlines! Over the next two weeks, I plan to work hard to get these in better shape.  Here is hoping they take their time getting into the review process!}}