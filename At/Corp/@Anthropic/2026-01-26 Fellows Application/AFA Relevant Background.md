
**(Optional) Please share any relevant AI safety background you have and provide links where possible (e.g., research experience, coursework, self-directed study, past roles, relevant projects).**

I have organized my thinking over the last year into three draft papers/outlines that indicate the direction of this thinking:

- Alien Biology Whitepaper - The paper that spawned the generative testing framework.

- Deliberative Coherence - Outlines the agenda for studying "deliberative coherence" -- studying how deliberation-based alignment works and how in breaks down.

- Experimental Roadmap -- Outlines how I plan to use the Alien Biology framework to conduct this work.


{{NICK:  Each of these will be a link to papers that}}