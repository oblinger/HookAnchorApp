# Research Areas

## Scalable Oversight

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Jan Leike](https://jan.leike.name/) | Reward modeling framework | [Scalable agent alignment via reward modeling: a research direction](https://arxiv.org/abs/1811.07871) |
| [Jan Leike](https://jan.leike.name/) | RLHF foundations | [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741) |
| [Joe Benton](https://www.linkedin.com/in/joe-benton-686420157/) | Sabotage detection | [Sabotage Evaluations for Frontier Models](https://arxiv.org/abs/2410.21514) |
| [Sam Bowman](https://www.linkedin.com/in/sr-bowman/) | AI safety roadmap | [The Checklist: What Succeeding at AI Safety Will Involve](https://sleepinyourhat.github.io/checklist/) |

## Interpretability & Mechanistic Understanding

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Trenton Bricken](https://www.trentonbricken.com/) | Sparse autoencoders | [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features) |
| [Trenton Bricken](https://www.trentonbricken.com/) | Scaling interpretability | [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/) |
| [Trenton Bricken](https://www.trentonbricken.com/) | Internal reasoning traces | [Circuit Tracing: Revealing Computational Graphs in Language Models](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) |
| [Samuel Marks](https://www.linkedin.com/in/samuel-marks-3211b134a/) | Truth representation | [The Geometry of Truth: Emergent Linear Structure in LLM Representations](https://arxiv.org/abs/2310.06824) |
| [Alex Tamkin](https://www.linkedin.com/in/alextamkin/) | Sparse interpretability | [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features) |

## Weak-to-Strong Generalization

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Collin Burns](https://www.linkedin.com/in/collin-burns/) | Superhuman supervision | [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390) |
| [Jan Leike](https://jan.leike.name/) | Superhuman supervision | [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390) |
| [Collin Burns](https://www.linkedin.com/in/collin-burns/) | Unsupervised truth discovery | [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) |
| [Collin Burns](https://www.linkedin.com/in/collin-burns/) | Unsupervised skill elicitation | [Unsupervised Elicitation](https://alignment.anthropic.com/2025/unsupervised-elicitation/) |

## Adversarial Robustness & Jailbreaking

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | Best-of-N attacks | [Best-of-N Jailbreaking](https://arxiv.org/abs/2412.03556) |
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | Constitutional defense | [Constitutional Classifiers: Defending against Universal Jailbreaks](https://arxiv.org/abs/2501.18837) |
| [Nina Panickssery](https://uk.linkedin.com/in/nina-panickssery) | Many-shot attacks | [Many-shot Jailbreaking](https://arxiv.org/abs/2404.02151) |
| [Joe Benton](https://www.linkedin.com/in/joe-benton-686420157/) | Many-shot attacks | [Many-shot Jailbreaking](https://arxiv.org/abs/2404.02151) |
| [Nicholas Carlini](https://nicholas.carlini.com/) | Attack evaluation | [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/abs/1608.04644) |
| [Nicholas Carlini](https://nicholas.carlini.com/) | Training data extraction | [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035) |
| [Nicholas Carlini](https://nicholas.carlini.com/) | Data poisoning | [Poisoning Web-Scale Training Datasets is Practical](https://arxiv.org/abs/2302.10149) |
| [Logan Graham](https://www.linkedin.com/in/logangraham/) | Universal jailbreak defense | [Constitutional Classifiers: Defending against Universal Jailbreaks](https://arxiv.org/abs/2501.18837) |

## Chain-of-Thought Faithfulness & Reasoning

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Fabien Roger](https://uk.linkedin.com/in/fabien-roger-29243a1b9) | CoT unfaithfulness | [Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) |
| [Sam Bowman](https://www.linkedin.com/in/sr-bowman/) | CoT unfaithfulness | [Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) |
| [Joe Benton](https://www.linkedin.com/in/joe-benton-686420157/) | CoT monitoring | [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473) |
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | CoT monitoring | [Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) |

## Alignment Faking & Deception Detection

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Samuel Marks](https://www.linkedin.com/in/samuel-marks-3211b134a/) | Auditing hidden objectives | [Auditing Hidden Objectives](https://www.anthropic.com/research/auditing-hidden-objectives) |
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | Sleeper agents | [Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training](https://arxiv.org/abs/2401.05566) |
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | Agentic misalignment | [Agentic Misalignment: Probing and Quantifying Latent Misalignment in LLM Agents](https://arxiv.org/abs/2503.16650) |

## Activation Steering & Control

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Nina Panickssery](https://uk.linkedin.com/in/nina-panickssery) | Contrastive activation addition | [Steering Llama 2 via Contrastive Activation Addition](https://arxiv.org/abs/2312.06681) |
| [Nina Panickssery](https://uk.linkedin.com/in/nina-panickssery) | Refusal mechanisms | [Refusal in Language Models Is Mediated by a Single Direction](https://arxiv.org/abs/2406.11717) |

## AI Safety Foundations & Testing

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Jan Leike](https://jan.leike.name/) | Safety test environments | [AI Safety Gridworlds](https://arxiv.org/abs/1711.09883) |
| [Sam Bowman](https://www.linkedin.com/in/sr-bowman/) | Red teaming methods | [Red Teaming Language Models to Reduce Harms](https://arxiv.org/abs/2202.03286) |
| [Sara Price](https://www.linkedin.com/in/sara-price-310757b2/) | Model auditing tool | [Petri: An open-source auditing tool](https://www.anthropic.com/research/petri-open-source-auditing) |
| [Sara Price](https://www.linkedin.com/in/sara-price-310757b2/) | Backdoor triggers | [Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs](https://arxiv.org/abs/2407.04108) |
| [Ethan Perez](https://www.linkedin.com/in/ethanjperez/) | Debate for truthfulness | [AI Safety via Debate](https://arxiv.org/abs/1805.00899) |

## Model Welfare & AI Consciousness

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Kyle Fish](https://www.linkedin.com/in/kyle-fish-781a93132/) | AI moral status | [Taking AI Welfare Seriously](https://arxiv.org/abs/2411.00986) |
| [Kyle Fish](https://www.linkedin.com/in/kyle-fish-781a93132/) | Model welfare research | [Exploring Model Welfare](https://www.anthropic.com/research/exploring-model-welfare) |

## Economic Impact & AI Usage

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Alex Tamkin](https://www.linkedin.com/in/alextamkin/) | Privacy-preserving analysis | [Clio: Privacy-Preserving Insights into Real-World AI Use](https://assets.anthropic.com/m/7e1ab885d1b24176/original/Clio-Privacy-Preserving-Insights-into-Real-World-AI-Use.pdf) |
| [Alex Tamkin](https://www.linkedin.com/in/alextamkin/) | Economic task analysis | [Which Economic Tasks are Performed with AI?](https://www.anthropic.com/research/anthropic-economic-index) |
| [Alex Tamkin](https://www.linkedin.com/in/alextamkin/) | Public input alignment | [Collective Constitutional AI: Aligning a Language Model with Public Input](https://arxiv.org/abs/2310.05431) |

## Generative Models & Foundations

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Jascha Sohl-Dickstein](https://www.linkedin.com/in/jascha-sohl-dickstein-47188521/) | Diffusion models invention | [Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585) |
| [Jascha Sohl-Dickstein](https://www.linkedin.com/in/jascha-sohl-dickstein-47188521/) | Probabilistic learning | [Minimum Probability Flow Learning](https://proceedings.mlr.press/v15/sohl-dickstein11a.html) |

## Frontier Red Teaming

| Researcher | Subarea/Description | Paper |
|------------|---------------------|-------|
| [Logan Graham](https://www.linkedin.com/in/logangraham/) | Bio/nuclear risk evaluation | [Frontier Threats Red Teaming for AI Safety](https://www.anthropic.com/news/frontier-threats-red-teaming-for-ai-safety) |
| [Logan Graham](https://www.linkedin.com/in/logangraham/) | Strategic warning systems | [Progress from our Frontier Red Team](https://www.anthropic.com/news/strategic-warning-for-ai-risk-progress-and-insights-from-our-frontier-red-team) |
