
**WHY ARE YOU INTERESTED IN PARTICIPATING IN THE AI FELLOWS PROGRAM?   (1-2 paragraphs)**

TL;DR.  I have a research agenda that is ripe for execution in your program, and I want to transition to full-time AI safety work.

I have long been interested in systems that can bootstrap beyond human intelligence (this was the basis for one of my DARPA programs), and my Machine Reading program also drove early ML + NLP work in that direction.  But it has also left me skeptical that humanity can maintain control of an intelligence long after it has far outstripped by it.  Leaving DARPA, I consciously chose a more applied AI path because I did not want to be part of that agenda.  Try as I might, I could not see a real path to enduring safety---that these systems will be involved in creating an endless succession of successor systems; one slip in that chain is all it takes.

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning in its training data.  I have been reluctant to run with this idea, however, since it seems more valuable for advancing AI reasoning than AI safety.  Now I see a glimmer of possibility!  When future AI systems use deliberation to achieve internal alignment, I believe they will have instrumental reason to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator and simulator, and to sketch how it could be used to study how deliberatively achieved alignment does or does not hold when pressured by conflict, instrumental, or environmental pressures.  I think it is 

