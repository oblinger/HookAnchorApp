
**WHY ARE YOU INTERESTED IN PARTICIPATING IN THE AI FELLOWS PROGRAM?   (1-2 paragraphs)**

TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.

I have long been interested in systems that can bootstrap beyond human intelligence (this was the basis for one of my DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.





TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.

I have long been interested in systems that achieve general intelligence by bootstrapping from human intelligence (the basis of my $50M DARPA program) as well as a second Machine Reading program that systems that can bootstrap beyond human intelligence (this was the basis for my $50M  DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.

