
**WHY ARE YOU INTERESTED IN PARTICIPATING IN THE AI FELLOWS PROGRAM?   (1-2 paragraphs)**

TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.


  I have long been interested in systems that achieve general intelligence, as well as the risks inherent in such systems. As a Program Manager at DARPA, I ran "Bootstrapped Learning," an effort to iteratively capture deeply structured human knowledge, and created "Machine Reading," a vision for how NLP and ML might be productively integrated to drive automated deep structure acquisition—this $150M program that led to IBM's Jeopardy-playing system (Watson) and hundreds of publications driving early deep learning work by researchers central to the field.

  Yet even then, I harbored doubts about our ability to control these systems: before joining DARPA, I wrote a 4-page note to myself reasoning through whether it was acceptable to drive this work, and upon leaving, I chose a more applied AI path rather than continue pushing toward AGI. This tension—between fascination with capable AI systems and concern about their risks—is precisely why AI safety work feels so urgent and personal to me now.


  ~~
  I have long been interested in systems that achieve general intelligence, as well as the risks inherent in such systems. As a Program Manager at DARPA:
  - I ran "Bootstrapped Learning," a $50M effort to iteratively capture deeply structured human knowledge.
  - I created "Machine Reading," a vision for how NLP and ML might be productively integrated to drive automated deep structure acquisition—a program that led to IBM's Jeopardy-playing system (Watson) and hundreds of publications driving early deep learning work by researchers central to the field.

  Yet even then, I harbored doubts about our ability to control these systems: before joining DARPA, I wrote a 4-page note to myself reasoning through whether it was acceptable to drive this work, and upon leaving, I chose a more applied AI path rather than continue pushing toward AGI. This tension—between fascination with capable AI systems and concern about their risks—is precisely why AI safety work feels so urgent and personal to me now.


~~
I have long been interested in systems that can bootstrap beyond human intelligence (this was the basis for one of my DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.



~~
TL;DR.  I want to transition to full-time AI safety work and I have a research agenda that is ripe for execution within your program.

I have long been interested in systems that achieve general intelligence by bootstrapping from human intelligence (the basis of my $50M DARPA program) as well as a second Machine Reading program that systems that can bootstrap beyond human intelligence (this was the basis for my $50M  DARPA programs), and a second Machine Reading program also drove early ML + NLP work in that direction.  But I am so skeptical of our long-term ability to control this technology that I wrote a 4-page justification for myself about why it was acceptable to join DARPA to drive this work.    Leaving DARPA I chose a more applied AI path for this reason.  

At the beginning of this year, I drafted a white paper, "Alien Biology."  It is a clever way to measure AI reasoning performance that is provably untainted by reasoning provided in the training data.  More exciting, I think I see a way this platform can drive AI safety work.  I believe that future AI systems will use deliberation to achieve internal alignment and will have instrumental reasons to strive to increase that internal alignment within themselves and their progeny.  Perhaps this can afford a natural long-term alignment of their interests with humanity's?

When I learned of the Anthropic Fellows Program, I dove in to implement the Alien Biology universe generator/simulator.  After a decade of wringing my hands on the sidelines, I finally have a direction that seems at least plausible for humanity's most pressing threat.

