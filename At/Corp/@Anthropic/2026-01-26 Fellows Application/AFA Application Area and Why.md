 [[AFA]] 
### BACKGROUND INFO

Relevant Research Areas:
- **Model organisms** -- To prepare for future risks, we create controlled demonstrations of potential misalignment–“model organisms”–that improve our empirical understanding of how alignment failures might arise.
- **Chain-of-thought Faithfulness & Reasoning** --
	- [Reasoning Models Don't Always Say What They Think](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) 
	- [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473) 
- **Mechanistic Interpretability:** Advancing our understanding of the internal workings of large language models to enable more targeted interventions and safety measures.

Application Call
- [[AFA Anthropic AI Safety Fellow]], [[AFA Fellows Program Blog]],
- [[2026-01-26 Fellows Application]] 


**please tell us briefly about an area of technical ai safety work you’re currently excited about, and why.**
## Long Summary
