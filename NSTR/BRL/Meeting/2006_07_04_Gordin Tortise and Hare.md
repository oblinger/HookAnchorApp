# BRL.Meeting.2006\_07\_04\_Gordin Tortise and Hare --

    What am I not claiming?  Here are a couple of cognitive claims I am not making:
    1.  The AS (Artificial Student) may not learn like people do.
    2.  The inputs and outputs of the AS need not resemble the I/O of real students, e.g., it may be using a special notation that is devised for this project and that the end users are able to learn (cf: Palm Graffiti looks different than how people normally write).  

    What is my real goal: I want to make computers as teachable as possible -- this means I tell them stuff and then they know how to do it.  I want computers to flexibly take instruction from a human.  In this way, a computer learns from a human what it is they are able to do.  Simply stated: I want to produce a flexible performance transfer system where a teacher performs (and teaches) a task and thereby helps the electronic student to learn to that task as well  

    Now, you may ask, "Why do I keep referencing teachers and students?"  I do so, because it is the only thing that I know of that displays this type of behavior, that is, this flexibility in learning.  So, I look to human students as my inspiration and model.

    Note that I'm using "learn" in the human sense of acquire ability, rather than the Machine Learning (ML) sense of sufficiently characterize a statistical pattern so that new inputs can be appropriately categorized.  Note that these two senses of learning are fundamentally distinct insofar as human learning is not well modeled as ML and is best understood as action in a social context which is entirely missing from the ML model. 

    <<>> How will human teachers instruct the AS?  Will it primarily be by demonstration?
    <<>> No ... to instruct a computer (or person) need more than one "pragmatic" type of interaction (protocol) -- need some programming, some demonstration, etc.  You can think of this instruction as end-user programming ....e.g., say you want to help diagnose a nuclear power plant .. I give you documentation, I give you strategies, I give you examples, then I use test cases to assess your knowledge.  The goal is for the AS to be so damn good it feels like you're instructing a person.  A key difference with existing end-user programming systems is that they often assume (or require) a fixed scope of what can be programmed ( e.g., music or sketching) but the goal here is to maximize scope so that the AS can learn just about anything the teacher is able to teach. 

    <<>> Does this mean that teachers can only teach those things you know how to do (cf: Neural networks that learn things to do things better than humans can do them).  What  qualifications does the teacher need to have?  Do teachers need the ability to assess student competence?  Do they need to know how to program?  What is teacher's model of student?  What do they need to know about them (model them)? 

    <<>> Does a teacher assume responsibility for not only teaching some new local competency but for successfully integrating that new knowledge with all the other learning the AS has done?  For example, suppose I teach the AS to recognize faces.  Previously the AS has learned to monitor the horizon for approaching tanks.  But facial recognition requires a very different focus setting on the camera.  Am I responsible for helping the AS figure out how to decide when to focus on the horizon versus the faces and how to strategically decide when to switch back and forth.  Even worse, suppose part of the recognition scheme I teach gets interwoven with the recognition procedures for objects on the horizon such that the later no longer works as well.  Is the teacher responsible for realizing this "negative transfer" has occurred?  for fixing the problem?  If the teacher is not responsible, then who is?  If the responsible party is the AS itself, isn't there a problem that the procedures required to fix the reduction of prior competence might be the very procedures that have just been degraded ( i.e., you can't get a core dump to fix itself because its core is lying all over the ground)?

    <<>> What tasks will the initial focus?  You want to be able to do everything but you have to start somewhere?
    <<>> Yes there will be initial focus tasks.  But I will choose more than one to get some generality. 

    <<>> Let's think about this in the context of a specific problem: reading zip codes on a letter.  This is a problem on which ML works well.  How would it work here?
    <<>> The essence of the solution is in deciding the feature set.  Currently, this is done by a ML researcher who by hand using extensive knowledge of the domain and the algorithms decides on the feature set.  This is the key step that makes the ML techniques able to work in solving the problem.  So, the big advance in state of the art, is we want the AS to take an active role in picking the feature set.  This requires picking a mapping function that goes from the real world setting to a feature set vector.  In this case, going from a letter containing a zip code to a set of features that the a back propogation system is initially trained on and then used as a classification system to decode zip codes. 

    <<>> What is a better example?
    <<>> Let's focus on a war simulation where fighters are taking on complex roles and the goal is for these fighters to autonomously carry out their fighting duties.  Say, a tank commander needs to take orders and then execute them intelligently.  This intelligence could include realizing the communication channel itself was compromised and so the orders were not given literally but need to be reinterpreted in light of the commander's dual goals of giving orders while not giving them away to a listening adversary.  So, the orders may be to attach but really the appropriate course of action is to sit tight.  Or the orders could be intended literally, but the agent realizes there is critical evidence the commander can not be aware of that requires the orders be superseded.  Of course, in cases where the orders are meant to be taken literally, we don't want the autonomous agent reinterpreting them to mean something else.
    <<>> But doesn't this require the AS to have common sense?  And common sense is the sword on which so much of AI has repeatedly fallen?  Why drive full speed into this cul de sac full of crashed cars? 
    <<>> I have a dream ... It involves microworlds ... But it needs to become more articulate.

    <<>> Fundamentally there appear to be two research challenges:
     1.  "End user programming" or "End user teaching" where non-AI experts are able to successfully program (teach) ASs to learn complex behaviors. The challenge here is about a non-programmer and a person who doesn't understand the ins and outs of the AI algorithms programming the AS without actually doing anything that looks like programming.  However, the assumption is that an AI expert programmer could readily produce an AS with similar behavior although through typical technical means of production such as training a back propagation system on a carefully devised set of features.
     2.  Devising new AI or ML algorithms that enable ASs to learn, that is, for ASs to acquire complex behaviors that we do not currently know how to teach ASs to have.  This challenge is apart from the interface issues of end-user programming.  So, this challenge is developing new algorithms and ML and AI techniques to produce new behaviors.  Currently, these algorithms don't exist so it is difficult if not impossible to acquire these behaviors even if we had Vapnik himself programming our AS. 
    Note the combination of these challenges is considerably harder than the sum of their parts.  Hence, it may be best to try to make progress on these independently, instead of betting the bank on achieving both in combination.  Right?

    <<>> I don't shirk from risk and I'm damn ambitious.  So, I'm going to bet the bank and try to do both.  In fact, my grand vision doesn't really take nicely to being decomposed into these two independent challenges.  Here's my vision of how its going to work.  There will be a number (let's say 6 to 10) instructional protocols that the end user can use to teach the AS.  The protocols are the ways that that challenge (1) is realized.  These protocols include things like programming by demonstrations, explanatory textual material, specific rules that guide AS behavior, ontologies such as IS-A hierarchy, FOPL, description logics that use subsumption, etc.  These various protocols get translated into the interal representation system of the AS.  Now, there will not be  a single form of representation.  Instead the AS will include a whole bunch of representational systems (indeed the internal reps could include some of the teaching protocols systems as FOPL, IS-A hierarchies, description logics -- making it clear that these representations are really not that end-user friendly after all!).  A primary responsibility for the AS will be navigating between its various internal representations.  Its too ambitious to require consistency.  But some form of interaction must occur.  In general, when we get to this point -- namely, how the AS will coordinate representations of what it has learned the vision gets really fuzzy and I have to say 'a miracle happens here'. 

    <<>> So, what I described as Challenge 2 above, that is, finding new algorithms that manifest new behaviors really can be subdivided into two parts:
     2-a.  New algorithms that manifest novel behaviors in isolation
     2-b.  New algorithms that integrate across isolated behaviors such that the integrated whole represents a useful merging of both and avoids degrading the contribution parts.  You might say, an integrating algorithm that ensures the AS knowledge base is monotonically increasing.
    But doesn't this raise another dark spectre, namely, what happens when the AS is taught bad stuff, that is, the teacher gives buggy knowledge, either too specific, or too general, or just plain wrong?   How does the AS ever recover from this, especially since the teacher (end user) isn't granted direct access to the "code" of the AS and so must find indirect means to undo the prior indirect teaching that occurred.  Of course, drawing on experience with actual programming, there is every reason to expect that attempts to fix these bugs will introduce new bugs.  Thus, the eventual "state" of the AS can be quite complex and poorly understood by both the teacher and the AS.  Given all this are you aiming at creating an AS that is really able to be responsible for its knowledge state in face of potentially flawed instructional material?  Or are you taking the more modest position (but hard to realize) of requiring all the instructional material to be perfectly accurate?
    <<>>  Tough questions!  I don't have an answer, but in general I see the general mode of teaching to be of adaptation  that is the teacher is usually not engaged in direct programming so much as nudging the AS to better behaviors.  In modeling terms I'm talking about perturbing an existing model rather than building one from whole cloth.  This mechanism is also called theory refinement. Of course, theory refinement is usually restricted to computational systems that are tractable.  And it is clear how I can use any form of restricted computational system (eg regular languages) given how ambitious the behaviors I want.  But I'm talking about theory refinement anyway. I guess this might count as the second miracle I need to accomplish.  In any case, some prior art which is at least evocative of the approach I'm describing includes Tieresias, SHRDULU, and SIR.  Or maybe think about it as Winograd's ELIZA meets Samuel's checker program.  What I mean by that is suppose a checker expert could design an expert AI checker playing system just be playing checkers with the system and talking to it in natural language or some as yet unspecified representational system where it can be programmed but without the pain and agony of programming. 

    <<>> Let's drill down on the first challenge a little more.  Personally, I think you might very well want to drop the second challenge and just concentrate on the first.  What's involved with the first challenge?  The main issue is what sort of representations or symbolic systems the end user teacher will use.  Let's take the example of a tank commander who wants to program the AS to serve his role in a distributed simulation.  How does the tank commander teach the AS when and where to attack?
    <<>> This is the place we need the softies -- I mean the HCI experts, cognitive scientists, and educators who understand how to meet the following two challenges.
     1-A.  Design representational systems by which the end user can "program" the AS.  This includes answering: What interfaces are use?  What forms of feedback are provided so the teacher gets feedback on the AS comprehension? 
     1-B.  Devise the minimal amount of training a tank commander might need -- and could successfully perform -- so that these representational systems are quick to learn and powerful for AS instruction. 
    Note this still leaves the very thorny question of how the teacher deals with AS instruction that spans either multiple occasions with one instructor or multiple instructors and the resulting necessity to worry about "negative transfer" from new learning. 

    <<>> When you phrase the problem that way it makes me think the right way to conceptualize our task is putting Vapnik in a jar. That is, how can we make ML and AI programming (given its current limits) something that any old tank commander can do, or, most conservatively, how can the tank commander do it given some minimal learning intervention.  This idea of minimal learning intervention is well summarized by the case of the Palm handwriting recognition where the Grafiti system led to its wide scale success whereas previous version that tried to do the full task of handwriting recognition were out right miserable failures. 
    <<>> Yes!  This is a great place to get the educators involved. 

    <<>> Who should I talk to about who should I talk to on how to reduce my teaching protocols to specific end-user interfaces and representations such that they are both effective and the end users can readily learn them? 

    <<>> Talk to Winograd at Stanford, Alan Lesgold at LRDC, Roy Pea at Stanford, John Bransford at University of Washington, Elliot Soloway at University of Michigan, diSessa at Berkeley, Charniak at Brown, Birnbaum at Northwestern.

     
