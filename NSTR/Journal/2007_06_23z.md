# Journal.2007\_06\_23z --

    BL METRIC


    Not all of you are up on all of the context in this discussion, but I thought I would
    send this note along anyway.  Just to frame the purpose of this discussion:

    We are in the final days before I brief DIRO.  I will not be making any large
    course corrections in my "going in" position at this point.  This dicussion is for
    me to have in my back pocket.  

    How we measure success is always key in any DARPA program, and I want to be ramped 
    up on all possibilities.  If Tony drags me away from my story in the first 10 seconds
    (it has been known to happen!) then I want to already know where I want to go,
    in these alternate universes.

    Still if you guys see a clear winner among the three I show below, or one know listed,
    dont be shy about saying so....


    thanks,
    Dan
     





    (1) SETUP
        Pick specialist domains that relys on little commonsense knowledge
        Ask experts what are the important questions to ask in this domain
        Reduce these to an ontology of possible questions and optional related knowledge
        Obtain natural text corpus matching to serve as source for answers to questions
        Ask experts to produce a list of important questions in ontology answered by corpus


    (A) THE ENCUMBENT "F-MEASURE" METRIC
        * Compute Correctness and Completeness for machine algorithms using a TREC-like
        process for generating & vetting the gold standard reference.
        * Compute Human Effort Reduction (HER) by measuring K engineer effort tweaking the MR
        system versus projected time needed for a K engineer to manually construct the
        KB generated by the MR system.

    (B) THE HYBRID HEAD TO HEAD
        Here we pit an MR+human team, MT,  against a pure human team, HT.
        * HUMAN RELATIVE CORRECTNESS AND COMPLETNESS -- We ask both the MT and HT 
          teams the same questions.  As with TREC top answers are pooled and vetted by
          seperate human judges for accuracy.  Resulting gold standard answers are used
          to measure both the MT and HT teams for accuracy and completeness as above.
          but the program targets are not percents of perfect gold standard performance
          but rather are percents of human (sub 100%) performance as I did in the BL program.
        * AMORTIZED EFFORT REDUCTION -- We measure the K engineer tweaking time
          for a new domain, MTsetup.  Presumably the human setup for a new domain
          is near zero, e.g. HTsetup==0.  Now a block of test questions are provided
          both to the MT and HT, and an average time per question is computed for each:
          HTperQ and  MTperQ.   
          Now by choosing, A, a number of questions to amortize over we can directly
          compare the human costs of the hybrid verses human approaches to the domain
          in question.  The AER (Amortized Effort Reduction) is the ratio of
               HTsetup + A * HTperQ    
               MTsetup + A * MTperQ


    -----------------------
    (C)  PURE HEAD TO HEAD USING NATURAL QUESTIONS

    (1)  Each performer is required to develop a question-answering module in addition to the learning module.  Both modules are driven off of the same NLP engine.  This may be the first time that a general-purpose AI system has undergone a head to head test with a human (of course there have been head to head tests of special-purpose AI systems such as chess players, etc.).  That is, for multiple domains of discourse both the computer and the human(s) receive exactly the same test--in the same language (restricted English) and exactly the same words!  We have just eliminated one of the biggest drawbacks to head to head testing:  the fact that the interface to the computer has always had to be dumbed down, thereby introducing some level of doubt about the test results.

    (2)  The evaluation contractor develops a library of test questions for each domain selected.  The library consists of, say, 10 to 50 topic areas that cover the key things that should have been learned about the curriculum.  For each topic area, some number of specific questions are developed, say, 3 to 10.  Each test then becomes a selection (either random or explicit) of some to all of the topic areas, followed by a random selection of one specific test question in each topic area.

    (3)  There are three classes of domains used in the program:

    (a)  A single independent domain.  You pick a smallish domain and the best textbook or textbook chapter on it.  You have each performer's computer read the textbook.  You have a statistically significant group of appropriately selected human subjects read the same textbook.  You test all computers and humans.

    The metric is the computer must do at least the mean score of the human cohort, after bias elimination (see below).

    (b)  Repeated domains.  In each phase of the program you retest the same domain.  This measures the improvement of each performer's reading and learning system during this phase.

    The metric is the delta between the current score and the score of the previous phase.  Unclear you want to use this as a go/no-go metric because it's not an absolute measure of performance.  But it does show you if the performers are making technical progress!

    (c)  Related domains.  You build on previous knowledge learned, similar to Bootstrapped Learning.  Here you pick a larger domain and find the best textbook.  You have your computers and human subjects read Chapter 1 and test.  Then Chapter 2 and test.  This tests the ability to accumulate more sophisticated concepts based on previously learned ones.

    The metric is the delta between the current score and the previous score, after bias elimination (see below).

    (4)  Bias elimination.  In all classes of domains, you give computers and humans a pretest to establish a baseline of knowledge.  That is, you give the first test prior to any reading.  Ideally everyone gets 0% of the questions right.  Probably the computers get 0%, but some of the humans get a few right.  This establishes the baseline of prior human knowledge.  You have to craft this pretest so that it doesn't "give too much away" about the upcoming post-test, while still testing the subject area.  Perhaps you pick half of the topic areas to be covered in each test so there is no overlap.  Or perhaps you present one third of the topics in only the pretest, one third in only the post-test, and one third in both.

    For each metric above, you eliminate bias by subtracting out the baseline score.  This eliminates the human bias from common sense, general knowledge, or randomly accumulated relevant knowledge.  I think this allows us to explore a wider range of domains, not just obscure technical ones.  Not sure how far you could push this idea though.
    ------------







    *** COMPARING 'A' to 'B' ***

    Now let us grip the table tightly and think about what consequences follow from 
    these metrics.  Here is what I see:
    (1) At 50,000 feet I agree with my skeptics.  A head to head comparison feels more
        natural and justified.
    (2) Comparing to human "less than perfect" accuracy and completness seems natural 
        and a lower target than comparing to perfection.
    (3) The AER calculation will be massively sensitive to the value of A selected for
        amortization.  (I would love to get rid of that factor, but the only way I see
        to do it, is to produce an impossibly large test set which would provide
        enough questions for the MT to be able to amortize MTsetup over.)
    (4) I believe that the difference between HER and AER is that 'A' is implicitly 
        set by the number of facts in the initial corpus.  This has two programatic
        effect.
        I.   I don't have to explicitly talk about this ugly integer
        II.  I believe it will be more natural to generate corpora that 
             result in a HER calculations with large reductive targets like 1000x
             I believe (without doing any actual calculations to back it up) that 
             that I will not be able to convincingly argue for values of 'A' that 
             will result in AER values of 1000x.  (Though perhaps 10x is still in 
             range if a find domains where the questions experts want answered are
             relatively stable, and there are mountains of them.)





    *** COMPARING 'A' to 'C'
    (1) Using real questions is the most natural of all, but it is biting off a larger
        problem.  Mobius has shown that simply using the same NLP system won't work.  
        The text of a question is tiny, thus it is critical to get the right interpretation
        of every word in the questions to answer it correctly.  this is not true for
        the source corpus.
    (2) It seems that option 'C' is a move away from an apriori known formally
        specified target for all questions.  Again more natural, and again harder.
        The question is, can we expect a system to perform well over an unspecifed 
        space of questions?  It needs to learn the relevant K structures and then 
        use them.  Or the questions need to be simple enough that they are mostly 
        restatements of single sentences in the corpus.  the latter seems to violate
        the goals of the program, but is the former plausible?
    (3) Of course if we allow humans to tweak the system after the test questions are 
        known, perhaps we could handle open ended questions, but then we need enough 
        questions that we could show large time reductions between tweaking time, and
        simple answering questions time.  I doubt such a large set of questions will 
        come from a chapter in a text book.  We would need to feed a much larger corpus.

    I think my assessment is that I love 'C' most of all, but I fear that we are not
    ready to take it on.
        







    Thoughts on all of this?
