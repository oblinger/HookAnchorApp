# life.5A\_DARPA\_Morality --

    SEE FILE!!  out of date

     <<>>


    THREE ISSUES:
    (1) I help militry kill <<>> oppress.
    (2) I help future rich get richer and poor get poorer.
    (3) I help create the replacement for humanity.

    MY QUICK TAKE

    (3) AI/moore's law will probably overtake humanity within a generation regardless of what I do.
        Still the sooner that humanity sees a conscious computer, the sooner it can start taking
        action.  The most dangerous situation is one where moore's law has pushed computing technology
        far far beyond human computing capacity before the first con



    (#1)
    - Yes, what I am doing will help the military kill/oppress, but so does any computer improvements
    - Another way to ask the question is to consider likely differences between the
      military with and without my tenure at DARPA:
      + More $$ will be spent by IPTO, less by other programs ==
        More basic research, less direct weapons research
      + More information research; better identification of enemies
        Because we can better identify enemy we can be more targeted & will make fewer errors
        ? Less collateral damage
      - More snooping stuff (invasion of privacy)

      LONGER RANGE
      * More autonomous weapons
      + Fewer American killed
      - Greater American aggression
      ? Continuous oppression using automated 

    <<>>
      - Military is more cost efficient (in  


    (#2)
    - A.I. will increase the overall productivity of society.
    - A.I. will decrease the value of labor.  It will divide rich from the poor.


    (#2B) People will loose interest in life w/o a job as a purpose.
    - Perhaps, but perhaps not.  Many live for the time after the slog. for their families, 
      for self enlightenment.  Perhaps w/o that burden, humanity will awaken into a new
      understanding.





    (#3)
    - Either it is coming anyway, or it is not.  (with or without me)
    - Consciousness now is much better now, not in 50 years when it will be millions of times of us
      We hope after a day of consciousness it says:  "wow amazing ideas, teach me more!"  **NOT**
      "Oh geeze, you mean I was <<>> by these morons?!  how disgusting!"

    - A non-downside is that we may have less time to react,
      but this is not really a down side since we will not react until it is close anyway.

    - The only real down side is that we might be further along in a self sustained 
      off-earth colony, I could be giving us less time.

    Just too abstract to really get crazy about right?
      



    * Is what I am doing hurting the world?

    - Physical jobs will stay or be lost based on robotics more than AI
    - Will push humanity toward the more strategic, open-ended, creative jobs

    - Jobs will be lost anyway.  


    -----------------------------


    I am doing this job mostly for selfish reasons.  Now that is fine, most people
    do most thinks most of the time in the same way.  But I could not accept
    great negative consequences as the cost for my benefit.
     
    (1) One thing that made me feel better was coming to the conclusion that
    I would actually *quit* this job and go back to IBM tail between legs if
    that was needed.  But if I am going to conclude such a thing I damn well
    better do it in the next month, since my way back to IBM, is lost after that!
     
    (2) The most catastrophic outcomes listed below is also the most far fetched.
    And if it is coming then I will do little to hasten or slow it.  For just a bit I 
    though focusing DARPA money in this direction will make a dent.  Perhaps
    but only a small one.  There are *already* other DARPA programs out there
    shooting for something a bit related, and the more I look around, the more
    I see so many people having redundant thoughts, my contribution to the
    sea even with DARPA's magnifying effect is really small.
     
    believe it or not, my relative unimportance in all of this is reassuring.
    things will turn out as they will turn out, my actions be damned.  very reassuring.
     
     

    --------------------------------------------------------------
    2006-07-19

    - My largest fear is #2 above.  (Dividing rich and poor)
    - I still recognize in a long term sense my actions even with the magnification of DARPA 
      can only change things by a small amount in the grand scheme of things.
    - That said, the BL program really does have the potential to accelerate our progress on
      'instructable computing' by a decade or more.  If we release datasets that allow
      writing research papers in whole new areas, hundreds or thousands of researchers
      can push this agenda.

    My primary rich/poor concern for the future is the loss of unskilled jobs,
    the only asset the poor has is their unskilled labor.  Each job class that
    can be automated is a job class they no-longer have access to.  It is true,
    that this automation will create jobs, but they will be fewer in number,
    and will require (much) more education.

    I cant tell how many unskilled job classes general purpose learning can replace.
    By itself there are not many jobs it can replace, such a system must be capable of
    acting in the world, and perform many human-like perception activities that are beyond
    the scope of the agenda I am proposing.  Still assuming those technologies come
    to exist over time, it maybe that one still cannot replace the human w/o having
    the system having a deep and context-specific understanding of those tasks.
    If my agenda is to be significantly causally linked to lost job classes then it
    is because it was a crucial "missing link" in fully automated job replacement tasks.

    When I think of specific unskilled job classes existing today, it seems that general
    purpose learning is *NOT* the most important (or even crucial) component.  Here are
    several:  
    - McDonalds' employee.    NLP; Robotics;   K=fixed "burger" universe (size=med)   BL useful, but not crucial
    - Security guard.         Vision (perception abstraction)
    - Receptionist            NLP;             K=fixed "office" universe (size=large)

    The jobs more at risk are those requiring some education:
    - Back office work.  (Accounts payable)   K=is not fixed, but must be learned for each buisness
    - Teaching                                K=is not fixed, but learned for each subject taught
    - 


    The biggest concern for impact, of my agenda is the impact on developing nations.
    Work requiring some education (like 'back office' work) is also the work
    that is potentally shiftable to them, but if the job classes are lost.

    The added jobs will require too much skill for them, so again they will have nothing to
    contribute to the economy.

    Indeed this is my biggest fear:  As the world is organized today, market force ensure
    that the most cost effective methods are used for each job-class.  This may mean more
    or fewer opportunities for humans.  We do not make these decisons based on *any* notion
    of goodness for ones nation, or for the world.  We simply make these decisions locally
    based on company owner economic concerns.  Thus a primary lever that affects world jobs
    and thus the viabilty of (most) humans, is what technonolgy will allow:  The winning 
    technology ($$ per widget) is the course humanity follows regardless of its effect on
    humanity.

    If I see that a technolgy's tendency is to drive hunmanities portfolo of job classes
    in a way that significantly reduces viabilty of the poor relative to the rich, but
    increases the throughtput of humanity as a whole, should I not work on it?

    -------
    2007-03-01 -- 

    * BL simplifies user interfaces, thus is may allow non-programmers to interact with computing systems.
      (But it also may increase complexity of info handled by a given job?)
    * BL will cut out the most repetitive and boring jobs or aspects of jobs, overall these are jobs with lower 
      satisfaction
    * As computers get more powerful, we want to remain tightly integrated with, and move along with their potiential
      We do not want very smart and very separate systesm.  BL aims at integrating us with computers
