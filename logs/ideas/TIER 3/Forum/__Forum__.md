# _
~~~~
Here's a guy from the right who might see value in the modified discussion thread project: 

https://time.com/6087577/michael-fanone-january-6-interview/

-- 
ysl
~~~~~~~

Al:

Good to electronically meet!  It sounds like we share a common interest regarding disinformation and hyper partisanship.


For me, this has been a line of thinking that is many years in the making, though I have never really done anything with it, since I was never clear if I could turn it into some kind of sustaining business  (I am still not clear about that).

My aim is a discussion forum that synthesizes succinct (single phrase) summaries of what different communities think about important issues.  e.g. is there significant evidence of climate change?  is there significant evidence of 2020 election fraud?  The first aim would not be to arrive at a single “correct” answer, but rather succinct summaries of different schools of thought.

The second analysis, however, would be to try to push as much consensus as possible around these issues.
e.g.  conservatives agree giving global consensus that certain minimal statements about climate change are empirically supported, while liberals agree giving global consensus that other stronger statements are not supported.

The result could be looked as a spectrum of statements (which could be succinctly shown on a single page!)
At the ends of the list would be statements broadly agreed, with hotly contested statements in the middle.

Such a summary could really advance the societal discussion, since it could show everyone which statements are not really even supported by their own clan when their members look deeply at the topic.

~~~~

But that only works if one can trust the analysis system itself, and only if the system can good judges from all political sides of an issue.


_
# ### OVERVIEW ###
## === FACT CHECKER CANVAS ===
### --- PROBLEM ---
Provide accurate, accepted fact-based judgements about specific media as measured against some published rubrick
_
## === FORUM LEAN CANVAS ===
### --- parts ---


Lean Canvas Order of Exploration
1 PROBLEM		Top 3 Problems
2 SEGMENTS		Target Customer Segments
3 VALUE PROP	Unique Value (& high lev)
4 SOLUTION		Top 3 Features
5 UNFAIR ADV	Things that cannot be easily copied or bought
6 REVENUE		Revenue Streams
7 COST			Cost Structure: Cust Acquisition; Distribution Costs; Hosting; People
8 METRICS		Key Metrics
9 CHANNELS		Paths to Customers

Layout:  1 4/8  3  5/9 2  lower: 7  6


~~~~~  [10-20-30](https://entrepreneurshandbook.co/guy-kawasakis-10-20-30-rule-for-perfect-presentations-129ba78d2fc1) 
    Title
    Problem/Opportunity
    Value Proposition
    Underlying Magic
    Business Model
    Go-To-Market Plan
    Competitive Analysis
    Management Team
    Financial Projections and Key Metrics
    Current Status
_
### --- PROBLEM ---

Leverage the energies of authors with conflicting perspectives over some scope of human discourse to synthesize a concise, cohesive presentation of the range of thought on the subject.


SYNTHESIZE -- Generate parsimonious, researched, supported "best" summaries of perspectives on important issues from which readers can quickly draw their own informed conclusions 
 of key stances and beliefs 

FACILITATE INFORMED OPINION FORMATION, VALIDATION, AND REVISION -- Provide readers with "best" summaries of competing perspectives on issues of interest in order to enable them to most efficiently derive a well informed personal opinion on the topic.

BUILD TRUST -- Build reader trust in the forum mechanism itself so they can trust summary materials when forming their opinions based on summaries of all available evidence.

MOTIVATE AUTHORS -- 
of materials so they can confidently rely on their 

ADJUDICATE -- provide ruling the larger population can trust
_
### --- SEGEMENTS ---

Authors
- Online forum style discussion platform users:
	Reddit, Quora, Medium

Readers



_
### --- VALUE PROP ---

MECHANISM & INCENTIVES

- TURN DISCUSSION INTO SYNTHESIS -- 

- BUILDS TRUST since majority cannot silence truth -- 

- PROMOTES POLITENESS, FACTS & OPENNESS



**MECHANICALLY UNBIASED** -- The forum itself is an entirely mechanical process that is designed to be resistant to gaming by determined and/or numerically superior subgroup of participants.  The system aims to present the full range of user perspectives down to some minimum perspective size with similar weight.

**FREEDOM AND COHESION** -- Artfully integrate competing objectives of discussion freedom and discussion cohesion.

**INCENTIVIZE SYNTHESIS** -- Strongly drive participants to author content the captures the essense of each subtree of discourse

**SEPARATE CONSENSUS & CONFLICT** -- Drive disparate groups to try to find and agree upon the parts of an issue where consensus can be found, so system can highlight the bits of each issue that are truly fully conflicted ideas.

DISCUSSION RESULTS IN AUTHORING

For Authors
- IMPACT -- Authors want to be heard, and they want to have impact on issues that they care about.
- DISCUSSION -- Authors want to discuss issues that matter to them.

For Readers
- SYNTHESIS -- Readers want to quickly understand what conclusions certain others have come to regarding a topic after they have put large collective effort into undertanding all info relevant to the topic.
- TRUST -- Readers want to trust the expertise, biases, and motivations of those providing syntesis conclusions.
- AGGREGATION -- Readers want to quickly get the aggegated benefit of the many authors writing on a given topic.
_
### --- SOLUTION ---

FORUM AUTOMATION MECHANICALLY
- ENSURES THOUGHT DIVERSITY 
- VISUALLY HIGHLIGHTS THOUGH QUESTIONS
- INCENTIVISE PIECEMEAL SYNTHESIS -- Drive Users
- ELEVATE CONVERSATION


REDUCES BIAS == ENSURE THOUGHT DIVERSITY

_
# ### PRESENTATION ###
## === Fact Checker Business ===
### _

    Title
    Problem/Opportunity
    Value Proposition
    Underlying Magic
    Business Model
    Go-To-Market Plan
    Competitive Analysis
    Management Team
    Financial Projections and Key Metrics
    Current Status
_
### --- PROBLEM ---
- Agendas really MATTER 
- Every person and every orgnaization has an agenda
- Thus there is no single person or org that all resonable people should trust

_
### --- SOLUTION ---
- Explicitly balance competing agendas against each other mechanically in order to triangulate the truth

_
### --- SECRET SAUCE ---
- Triangulation -- using many judgement from other agenda's 
	- Do you agree with your future self?
	- Do you change your mind when you dig deeper?
	- Are you consistent with others that share your agenda?
	- Are your judgements coorelated with your agenda (E.g. always in favor)
	- Are you consistent with high confidence judgements?

_
### --- BUSINESS MODEL ---
- Non-profit community-based discussion board
- Sell adjudicator services to social media companies

_
### --- GO-TO-MARKET ---


_
# ### SOLUTION DETAILS ###
## === KEY IDEAS ===
### _

- **CONSTRUCTIVE** -- System is designed to support positive/constructive discussion and debate among folks with holding wildly different beliefs about issues of interest.  Just as with wikipedia today, ones privledges are tied to past behavior.  Those who are not constructive in their interactions are pushed down in visibility so the overall tenor of the debate can be quite pointed with folks strongly disagreeing, but doing so in constructive ways.

- **TRIANGULATION** -- using feedback from multiple authors to auto test accuracy, bias, etc. in them.

- **PLACEMENT** -- algorithms that use history of stances and support to 'place' both stances and authors on various spectrums.  (e.g. conservative vs. liberal, or control-focused vs. freedom-focused)

- **COMPRESSION** -- succintly, accurately, unbiasedly summarizing some content using some much shorter content.   (e.g. a one sentence compression of 50 studies on a topic might be:  most studies had small sample sizes, among those with larger sample sizes a slight (.2-.6%) increase in death was observed.)

_
### --- MISSION ---

Build an organization whose judgements can be trusted by all since they are informed by a range of agendas

#### Take all concerns of all groups seriously
Today we very often take shortcuts.  Based on our own agenda, we decide which questions should be taken seriously, and what kinds of analysis are appropriate for analysis.

This leads to a predicable consequence: those who don't share our agenda don't listen to our results.

It may seem prudent to NOT indulge "wrong" concerns and "wrong" approaches since one lends them creedance in even discussing them.  But in the end this only fuels more suspicion about our results and our agenda.


We propose an alternate path:
-- take every serious concern of every reasonably sized group seriously.
-- honestly take conclusion as valid and try hard to support those (sometimes crazy) conclusions
-- attempt to incorporate most every form of evidence which that group finds valuable
   see how far those arguments can go, THEN contrast those with the best counter arguments

e.g. AVOID "shut down" style arguments as a sole




 
_
### --- PLACEMENT --- 

- Use self-reported question answering against a short-fixed questionare in order to get an initial assessement of author's placement on relavant axes.  (e.g. ask them if they are a conservative, and ask if they support gun rights etc.)

- Use "hubs and authorities" style of iterative assigment algorithm to provide assignments of both authors and stances to places on different axes of relevance.

- Algorithm select placement assignments that minimizes the conflict between placement of authors and their supported stances.

Output is a numerical score for each author and stance on each axis of interest.

Users can see, but not edit their scores.

_
### --- TRIANGULATION ---

Steps:
1. High accuracy, disinterested authors are presented with specific adjudications in order to offer an opinion.
2. Both past and future authors with an interest in this topic are presented with the same adjudication for consideration.


The degree to which an author's adjudications are DECORRELATED with their personal support for stances on related issues is a measure of the level of adjudication BALANCE they have.

The degree to which they adjudications match consensus adjudications overall is a measure of their ACCURACY.


~~~~~~~
_NOTE about unbiasedness & accuracy:  One may have a strongly biased opinion over some area (e.g. be strongly biased towards liberal or conservative stances) and yet be an accurate and unbiased adjudicator over those same issues.  Such an unbiased accurate adjudicator will judge AGAINST content whose conclusions match stances that they support in cases when the content is not logical, not supported by evidence, etc.  So we seek to measure adjudication bias and adjudication accuracy independent of personal bias towards/against particular placement positions._

~~~~~~ 
**How this works in detail**

1. Content is continously adjudicated as it is read.  Any reader is invited to offer constructive feedback on the content they read.  Feedback should be written in a constructive, positive, and as narrow a way as possible even when one is commenting on stances that one strongly disagrees with.
	- When possible feedback should be tied to the phrase, sentence or paragragh to which it applies.
2. Authors are given opportunity to edit their phrasing etc. to "fix" the issue raised... even if they dont fully agree with the criticism if they can make an edit to make their audience "happy" they may do this, just to minimize the number of highlighted issues raised with their content.
3. Each point of feedback on each stance is itself a stance and others may agree or disagree with this feedback itself.  A stance on a stance is called an ADJUDICATION

highly rated stances are the ones most often shown to a viewer when they look at some content area.  this provides opportunity to agree with or disagree with each stance (and feedback stances on stances too)

4. Over time we the system continuously measures the level of bias occuring within the adjudications over each piece of content.   e.g. if many conservatives rate some content as illogical while many liberals rate it as logically sound then the system knows is bias happening in this adjudication.  This is known since the adjudication stances are splitting strongly based on the author's conservative-liberal placement we KNOW there is high bias involved on one or both sides.... but we don't how much is happening from each side.

In these cases the system secretly forwards this same content for review by DISINTERESTED users (those that are not strongly conservative or liberal), or those who have high IMPARTIALITY scores.

These triangulation scores provide mechanical feedback on the biases of the specific adjudications provided earlier in the process.

This process DOES require strongly impartial folks at the top of the pyramid.  That is ensured via other means.  Triangulation itself is a mechanism that amplifies the scope of impact of those smaller number of impartial judges to have a ripple effect over the entire population of adjudicators.

~~~



Assuming that we get strong consensus from this adjudication set, then we have a good indication of what the ground-truth regarding the adjudication of this underlying content.  We even have the written consensus commentary from these adjudicators.  

The system then uses this consensus adjudication result to look for BALANCED respondants that may nonetheless may place as strongly liberal or conservative based on their supported stances.  Such users were willing to "go against" their underlying preference and adjudicate this content in a way OPPOSITE to their underlying bias on the issue.

Such reversal cases are really the hallmark of an UNBIASED adjudicator.

And pure consensus matching is the hallmark of an ACCURATE adjudicator.


Not only do we provide a bias/accuracy score for each user, we also can provide human written commentary to back up this score.

we can show a piece that you scored as logical and empirically justified right along side consensus views that agreed with you when they did, AND show the consensus comments that disgreed with your assessment along with human written justifications.


~~~~~~~~~~~
OUTCOME: My thinking is that one would not need to frequently adjudicate-the-adjudications in order to really change the dynamic of providing feedback in this system.

I think many folks pride themselves on being UNBIASED and ACCURATE, and they are going to COMPETE to get good scores on these measures.  But in order to get a good score, they are going to have to NOT overshoot or undershoot when giving feedback.

~~~~~~~~~~~~~~
OUTCOME OF THE OUTCOME:  Once I have have a subset of users that are biased liberals and strongly biased conservatives who are nonetheless UNBIASED and ACCURATE adjudicators we are in good position to generate quality outputs.

Now among strongly supported liberal content and strongly supported conserviative content we and let the CONSERVATIVE users select the most accurate, empirically supported, etc. stances FOR their conservative position, and those same users can provide the most accurate commentary on the accuracy and empirical support for liberal stances which they may disagree with.

_See what we are doing here?_
Creating consensus voices of reason that all can trust and listen to:
We are creating/identifying users that a conservative can trust to review and frame stances from both sides.  At the same time we have liberal users that liberals can trust too.

Both liberal and conservative reviewers were selected for thier willingness to call BULLSHIT on their own side -- even if they are strongly biased towards one side or the other mathematically we have aimed to ensure their adjudications are not biased!

And I think there voices can best be HEARD by all users.

Afterall if I know deep deep deep in your heart you are a trump supporter, but still you are calling bullshit on some argument... well now I have to pause... maybe it IS bullshit.


~~~~~~~





_
### --- EDIQUETTE / CONSTRUCTIVITY ---

Feedback on non-constructive 


_
### --- ADJUDICATION ---
### --- MECHANICALLY UNBIASED ---

The system provides a collection of judges:
1. Whose biases are mechanically selected to reflect the biases of the population at large, 
2. Who have been shown to render impartial judgements as measured mechanically in comparison with their peers
3. Who show significant engagement with the platform in rendering opinions.


- The tool focused participants on important questions of bias and of fact, as suggested by the participants and as suggested by sponsors.
- Participants author opinions on these questions, and then discuss and refine those opinions in a collaborative fashion.  
- The collaboration itself is monitoried and participant's ability to author material and render judgements which stand the test of time, and stand the test of subsequent scruitiny, serve as a measure of their impartiality.
- Importantly only the judgements of others with similar overall biases are used to assess ones impartiality.
- System actively canvases to gain participation across the full spectrum of social opinion within the population.
	- Multiple surveys are used to model the larger population, as well as the paricipant population.
- Mid-tier judge selection.
	- UNIFOMLY REPRESENT POPULATION -- Are selected at random in order to uniformly match the demographics of the larger population.
	- HAVE HIGH IMPARTIALITY AND EXPERTISE SCORES -- 
	- HAVE MODERATELY HIGH ACTIVITY 
- Senior Judge selection
	- VERY HIGH IMPARTIATLITY SCORES
	- AGREE TO BE PAID AND GET TRAINED
	- GET TRAINED TO FOLLOW A GIVEN STANDARD OF JUDGEMENT


SCORE DRIFT
- CONSENSUS SCORING -- No impartiality scoring when there is a lack of consensus.
	In order to avoid any kind of scoring drift anytime there is a lack of consensus regarding a particular judgement, the adjudications in that case are not used to update anyone's impartiality scoring.  In this way we avoid cases where the entire set of judges impartiality scores somehow skew over time into some self re-inforcing pattern. It is ones ability to render adjudications reliably in cases where a clear consensus exists that provide the sole measure of ones impartiality.
- RANGE FLATTENING -- Knowlege of consensus scoring itself can provide an incentive towards watering down owns own judgements in order to avoid ever going against a consensus ruling and their by loose impartiality points.  This perverse incentive could cause an undesirable uniform squeeze towards the middle as judges shy away from adjudications that they feel might result in a consensus against their opinion.  In a worse case this could yeild a pressure towards a reinforcing pattern of systematic mis-judgement towards the center.  To counter this effect an explicit flattening weighting is applied to the impartiality updating.  We ensure that any such tendency is countered by over weighting cases where one renders judgements in favor of the edges.

_
### --- OVERVIEW VALUE PROP ---

THE SYSTEM IS RIGGED NOT TO BE RIGGED
- When consensus statements are presented with a high degreee of validation, once can be confident they truely do represent a consensus of those that share ones own bias and are vetted by the most impartial among those sharing that consensus.



_
## === UX - USER EXPERIENCE ===
### --- DATA PRESENTATIONS ---

NOTE VIEW -- Viewing a single note
- BODY -- A note's hook, body, parts and links are rendered for view according to the markdown provided by the note's data contents.
- COMMENTARY -- Leading commentary for a note is provided as annotations within the text of the note, and as a list of comments in the right margin by the note's content.

HOOK VIEW -- Area notes do not display their contents only their hook is shown followed by leading commentary


LEADING COMMENTARY -- List of notes that maximize coverage

_
### --- Reader User Flows ---

GOOGLE SEARCH
A reader who is interested in a particular topic does a google search for material on this topic and finds themself on one of our content pages.  Reader does not know about the forum or its purpose, they have just clicked.
- User sees a wikipedia style page that describes a particular area of human interest along with a taxonomy of sub-topics for this area, OR
- User sees a single issue page that describes a particular issue upon which different authors might express different stances toward the issue.  Along with a summary of the major stances taken along with links to the best representatives of each.
- User sees a single stance taken on a particular issue.


BLOG READER FLOW
A user of the forum decides they would like to stay abreast of discussion within areas of their personal interest.

_
### --- Reader Interface ---

The markdown text view of a note's content is shown with bold, italic, and heading text rendered appropriately, along with tree of subnotes if a note has them.

Just as with MS work or google docs this basic text is marked up with different kinds of highlights on the text representing feedback from others on this piece.  Because a give popular piece may have many many comments on it, these are generally reduced to the most supported comments first.  Still since we care to get adjudication on less popular feedback we to randomly show all feedback, at least on occasion.


PARTS
- CONTENT PANE -- 
- FEEDBACK MARKS -- 


FEEDBACK MARK TYPES
- UNDERBAR -- critical marks shown as red underline
	- UNDERBAR "J" -- 
	- UNDERBAR "
- OVERBAR -- postive marks are shown using a green overbar

_
### --- Reader Feedback ---

- HIGHLIGHT -- Reader optionally selects a range of text to comment upon.
- START -- Using a keyboard shortcut or hitting the pencil icon a user may provide commentary or feedback on this selected content, or upon the entire note.
- CHOOSE TYPE -- Using key or click the user then selects the type of feedback being provided from the feedback types list.
- BODY -- The 

_
### --- Feedback types ---

- COMMENT -- Reader is just providing discussion about a given topic.
- JUSTIFICATION -- Reader is claiming that some part of the content is making a claim that requires justification beyond what has been provided.
- CLARITY -- Reader 
- BIASED LANGUAGE --

_
### --- Content Markdown ---

" # "	INLINE SUB HOOK MARK
" . "	REFERENCED SUB HOOK MARK
" [] "	REFERENCED HOOK MARK
"-->"	CONCLUSION MARK
"_xx _"	ITALIC MARK
" + "	AGGREGATION INCLUSION MARK




_
## === DATA ===
### --- NOTES ---
#### -- Note Fields --
- PARENT 	--  Link to the note where the author feels this note belongs
- LINK		--  Relationship beween this note and its parent
- SELECTION	--  Interval of parent reference by this note (OPTIONAL)
- OWNER		--  The user / usergroup that constrols this note
- ID			--  Unique ID for this particular note/post
- NUMBER		--  A unique version number for this partiular 'post' or 'edit' of the note
- NAME		--  Nickname for this note (OPTIONAL)
- HOOK		--
- BODY		--  Markdown string that includes part and link references.
	- PARTS	--  
	- LINKS 	--  In markdown
	- ATTR	--  Key/Value attributes specified in the note's body


JUSRISDICTION -- Each jurisdiction has its own root and all descendents of that root operate according to the rules of the jurisdiction.

_
#### -- Note Types & Link Types --

CHILD NOTE INENTION
- AREA		--	Defines a sub area of social discourse that discussion may be "inside of" or not.
- ISSUE		--  Defines a specific issue upon which different people might adopt difference stances
- STANCE		--	Defines a specific stance on an issues that people might support or oppose
- PART		--  Defines a 'part' of parent intended to be referenced explicitly within parent body.
- +1			--  Expresses a supporting "vote" for its parent stance, issue, or area
- -1			--  Expresses an opposing "vote" against its parent stance, issue, or area
- FEEDBACK	--  Provides structured feedback based on one of the enumerated discourse rubricks
- POST		--  Author is contributing to discussion about the parent note

NOTE RELATED
- COHORT 	-- The subset of a notes children that have the same intention and selection
- CONSENSUS	-- A designation on a note intended by its author to summarize all peers in its cohort


INCLUSION TYPES
- EMBEDDED	-- The full text of the note is embedded in it parent
- HOOKED 	-- Just the hook text is shown on its own lines, and is expandable to full note
- FOOTNOTE	-- The name of the note is included inline, and its hook text is included as an 		
				  associated, clickable footnote at bottom of content.
- INLINE		-- The hooktext is included as a clickable inline text.
- IMPLICIT	-- The note is not referenced by the parent's body at all, so it is listed below body

LATER 
- ALIAS		--  Defines an alias for one or more nodes that redefines 


_
##### ,
- PART	-- Author views this node as a part of its target node 
- == 	-- Author views this node as an alternative (of same type) for its target node

- CHILD
- PART
- REF
- HOOK	-- shows hook only
- ALIAS
- **ALT**	-- Author views this node as an alternative (of same type) of its parent node
				Delta:	Title describing goal of this ALT
- **ALIAS**	-- A note that stands in for another note
- **PART**	-- Author views this node as a subpart (of same type) as its parent node.
- **HOOK** 	-- Author views this node as taxonomic sub-area of its parent's domain node.
				This node defines some domains or area of human discourse
				text: defines the boundaries of the area (what is in/out)
				parts: incomplete list author views as "in" area
- **ISSUE**	-- Author views node as question or topic upon which one might have a stance
- **STANCE** -- Author views node as a stance, a position or perspective on an issue 
- **VOTE**	-- Author is asserting relative support (or anti-support) for item
				aspect: specifies the aspect being voted upon:
					support, cooperative, accuracy, consistency
- **CHAT**	-- Author adding commentary or discussion about the parent node.

	_
#### -- Note Tree --

**DISCOURSE TREE** -- Tree of all notes and posts

**NOTE** -- A a single cohesive, versioned piece of content that was contributed created by a specific user at a specific monent in time.

**COHORT** -- The set of children notes that share a common: parent, intention type, and parent selection.
- Each note is a member of exactly one cohort and each cohort is a child of exactly one parent note.
- Thus each notes and cohorts form an alternating tree.


_
#### -- Note Coverage --
	
**COVERAGE LIST** -- The "cover" for a note is the sequence of child notes that best cover all user perspectives.

- **CHILD** -- x is a _**child**_ of y, if the parent of x is y.  Thus any user may add a child to any other note (though the added not may have little or no support).

- **VOTE** -- A vote note simply lists zero or more stances support by a given user on a given issue.
	The user (or user group) may change their votes by simply publishing a different vote at any time.


**COVERAGE ALGORITHM** -- Computes the sequence of note children that best cover the range of user stances for that note.
- Once a child note supported by a user is included on the coverage list, the algorithm considers that users perspective "covered" and no longer considers that support for subsequent notes.
- At each step the algorithm consider computes the level of support for all remaining children notes.
	The number of "uncovered" users that have expressed any level of support for a given note.
- The note with the highest level of support is added to the coverage list, and covered users are removed from the list of "uncovered" users.



	**NOTE CHILDREN**
	Children(x):
		BestCovering( {n in NOTES where n.hook = x} )		if x is a Hook
		x.parts												if x is a Note


	**COVERING ALG**
	Given: list of children, CHILDREN, and list of supporting users, USERS, th is the min threshold 
	Let REMAINING <- USERS, TOP <- []
	Loop:
		For c in CHILDREN\TOP: supporters(c) = count of users in REMAINING that support c
		let t = argmax{ i in c of  len(supports(i)) } 
		if len(supporters(t) < th * len(USERS) then return TOP
		TOP <- TOP + t
		REMAINING <- REMAINING \ supporters(t)

	**AGGREGATIONS / OVERVIEWS NOTES**
	A note may aggregate or summarize many other areas/stances into a single summary note.

	Anyone can cover many areas or many stances with a single summary or aggregation note.
_
##### -- How data changes over time --

	HOOK CHANGES
	- Any change to any hook string will cause ALL hooks containing that string to be updated.
	- Authors of those sub hooks will be prompted to accept the new positioning, if they reject it, then they will keep the original parent hook.
	- Authors whose hook change results in many rejections are noted as being non-constructive
		(they should have created a NEW hook instead of editing the old one, and allowed folks to migrate)
	- Frequent hook changes with many children are also considered non-constructive

_
### --- USERS ---

**USER** -- those that author and read the forum notes.  Each note:
- Has a unique username for identification
- Has a profile note "USER.username" that announces existence of this user and provides profile info 
- Has a public/private key pair used to identify user (part of the profile note)
- Has different privileges within different user groups.
- Has authoring control over all notes whose name is prefixed by their username

#### -- User Groups --

**USER GROUP** -- a grouping of users that share some common perspective, goals, or association.

Each group:
- Has a unique groupname for identification
- Has has a profile note "GROUP.groupname" that announces the existence of this group & provides info
- Has a list of senior members listed in the note "GROUP.groupname.board"
- Any board member can control any aspect of a group.
- Each board member can unilaterally execute any action on behalf of the group.
- Board membership is the only group property that requires a majority of group members to sign in order to execute the change.

Change of the board.  
- A majority of current board members must support any edit that changes the list of board of directors.  
- Thus the system will not accept any change to the note "GROUP.groupname.board" until that note has the support of 1/2 of the current board.  That is until there are sufficient notes: "USER.username.VOTE.group.groupname.board" that support a particular new slate of board members.
- Thus it is important the groups keep their board member lists up to date, since the algorithm will require 1/2 of all listed accounts (even if those users are no longer active).

#### --- User Data Fields ---
- HANDLE
- UUID
- PKI

- UNBIAS
- ACCURACY
- POLITENESS
- CONSTRUCTIVENESS

- BIAS
- ILLOGIC
_
### --- HOOKS ---

**HOOK** -- A sequence of phrases used to identify a specific "place" within the space of human discourse

**Properties**

- **TREE** -- All Forum Data is organized into a single global tree of hook phrases

- **COVERAGE BASED VISIBILITY** -- When viewing a node in the hook tree, those notes with greatest coverage AT ANY LEVEL below this hook level are presented in coverage order w/o regard to their depth.
	- SUMMARY NOTES -- Authors may summarize a range of notes within a kind of overview.  Once enough users that support the notes summarized by a particular note also support the summary of that note, then the sub-note will not be visible, except as listed in the summary note.
	- AGGREGATION HOOKS -- Any hook that begins with a "+" is an aggregation hook.  These hooks are not presented as alternatives to each other, but rather as different instances of the same kind of data

- **CONTROL** -- Who can edit what
	- HOOKS -- controlled by their author (at each level in the tree)
	- NOTE -- controlled by their author
	- SUPPORT -- Each user controls the support the provide to each note.
	- VISIBILITY -- is indirectly controlled to maximize coverage of most supporting users

- **CONSENSUS GRAPH** -- A majority in of high constructive members of all major placement groups must agree on adding or editing the consensus graph.  In rare cases one can create a placement specific hooks when different groups disagree.  e.g.:  liberal-XXX and conservative-XXX
	Consensus graph construction minimizes conflict by not defining any intrinsic meaning for any given hook phrase, and by being permissive in allowing varient entries.  Each community can utilize their own taxonomy, and these alternatives can be combined and recombined as needed.

_
#### --- HOOK DATA ---

**HOOK PHRASE** -- A short string of text identifying a conceptual "place" in space of ideas

**HOOK** -- A sequence of hook phrases

Properties: 
- TREE -- hooks form a tree
- PARENT -- the parent of each hook note is another hook note
- CONTROL -- Each hook note is controlled over time by its author.  
- AGGREGATION HOOK -- A hook phrase the is prepended by a "+".  These phrases are aggregated when included within the body of another note.
- UNIQUENESS -- If two (non-aggregation) hooks at within a sub-tree are identically named, then the largest distinguishing user group name covering its author and its supporters is pre-pended to its display form in order to differentiate it:
	conservative: summary    liberal: summary

- Each hook string is associated with a parent hook, 
- ??? and other side up linkages
- Author of a hook has full control over that hook

#### -- Data Tree Presentation --

Presenting a place (a hook) within the data tree is done by 

- NOTES PRESENTED -- Consider all notes that link up to this parent.
- BEST COVERING -- Select best covering regardless of depth of note

_
## === DYNAMICS ===
### ...

**DYNAMICS** -- The interplay of the forum's software behavior against the agendas and behaviors of the range of readers, authors, and board-members.

Achieving our aim of driving productive discussions towards concise, unbiased summaries of human thought will require careful tweaking of forum execution rules inorder to drive the intended dynamics.

The three big dynamics control mechanism in the forum are:
- **ETTIQUETTE** -- written rules of ettiquette for guiding participant behavior.
- **COVERAGE** -- mechanism providing increased visibility for content that captures competing consensuses
- **ADJUDICATION** -- a back-stop mechanism for driving constructive behavior.

_
### --- ETTIQUETTE ---

**ASSUMTIONS / OBJECTIVES** -- Form system behaviors are designed around the following assumptions/goals for user behavior:
- **CONSTRUCTIVE** -- Users are expected, encouraged, and occasionally forced to operate behave in contructive ways within the Forum.  
- **GENTLE** -- At all levels within the Forum the least aggressive responses are taken before harsher measures are taken.  For example a private note from a senior member of your community is less aggressive, than a note from a conflicting community, is less agressive than temporary reduction of priveldges is less agressive than longer term reduction in in privledges is less aggressive than outright banning.
- **COMPETITIVE** -- While are expected to be polite and constructive, that does not preclude them from being pointed/blunt in their attack of an idea.  Further they may reason and collaborate strategically in order to get views they believe in to have greater visibility, as long as they are not engaging in "non-constructive" strategies to achieve this end.  (see rules of etiquette)
- **EGO-AWARE** -- In many cases both and author's ego and an author's identity may be a significant motivator for their participation within the forum.  This reality is understood, accepted, and to a degreee supported.  At the same time such ties to ego and identity can foment many non-constructive behaviors.  Thus the etiquette and forum operations (specifically ownership/versioning rules) are designed to balance desires of ego/ownership against productive forum outputs. 

**RULES OF ETIQUETTE** -- Community expectations are written out, complete with examples, so users understand and can understand and encourage the right levels cooperations/competition.  There will be opportunities to "hijack" the system and boost ones views beyond the support they "naturally" should have.  The community itself can consider the pros and cons of different rules and adjudicate this list over time to evolve constructive / productive / fun / engaging / blunt / polite interactions.

As the system evolves it is possible that different subcommunities might want to split into different regions allowing different different rules of ediquette to apply.  This would be adjudicated just as any simple rule change would be adjudicated.


**SOME EXAMPLES OF LIKELY ETIUQUETTE RULES**:
- Creating duplicate / fake user accounts strongly violates eiquette and if found should have more agressive conseuqences (perhaps for all accounts)
- Falsely claiming support for positions in order to pervert the voting within those communities would also be a break with etiquette.
- Cursing in ones messages might be a light violation of etiquette.  Ad hominim attacks might also be a violation.

_
### --- COVERAGE ---

COVERAGE -- The bottom-up method to ensure subtree summarizations were unbiased and representative of the diversity of user opinion within the indicated subtree.

_
### --- ADJUDICATION ---
#### ...

**ADJUDICATION** -- An _**adjudication**_ is a human judgement (stance or vote) regarding some specfic human generated content. 

The forum automatically combines feedback from multiple readers to form a consensus adjudication on the underlying content.  These consensus adjudications do not assess the conclusions drawn by the underlying content, rather they focus on judgements about several carefully prescribed dimensions expected of constructive human discourse.  Does the level of certainty expressed in it assertions match the level of supporting data or links provided?  Is it compressive?  Is it emotionally constructive discourse?  Does it use biased language?

Having precise measures for these dimensions is difficult and not necessary here.  Instead what is needed are rubricks of behavior which are clear enough that well intentioned participants can at least agree when instances of egregious violations of contructive discourse have occurred.

Arriving at these consesnus adjudications on behavior allows the Forum mechanism to take the minimially-disruptive actions to discourage these non-constructive behvaiors.  The whole point of the Forum is to avoid systematic bias, thus any global-unilaterial mechanim must be viewed with extreme suspicion lest that mechanism be used to squelch legitimate diversity in human thought.  At the same time, various trolling behaviors must not be allowed to be an effective way to amplify ones visibilty, lest it become a dominate dystopian mode of discourse.  Adjudication rules must thread carefully between these two pitfalls.



These adjudications are key to having "good" content bubble to the top, and having non-constructive, non-justified, etc. content drops into the background.  Notice no attempt is made to have certain stances drop to the background...  if a sizeable fraction of the readership supports a stance then it is listed up front, it is just that the "best" version of that stance which is listed up front.


**AN EXAMPLE**:  No attempt is made to draw mechanised consensus about the correctness of any stance taken by authors.  (e.g. Are masks effective against Covid?)  Some will have stances in favor while others against.  The system itself supports no one opinion over another intrinsically on this or any question.  Readers will however provide adjudicative feedback on the content they read.  Suppose there are several stances expressed in favor of masks and others against them.  Readers will provide feedback intended to improve these stance statements.  This feedback (if valid and ignored) also serves as a kind of adjudication against that content.  These automated adjudications provide a way to "bubble up" good versions above versions that make emprically founded assertion, or use biased language, etc.

_
#### -- Adjudication Dimensions --

**ADJUDICATION DIMENSION** -- One of the enumerated aspects of human discourse identified within the Forum as so important that significant violations of them must be flagged and eventually acted upon in order to keep Forum discourse constructive and productive.

Dimensions include:

- BACKED -- A measure of the degree to which assertions made in some content are backed by the supporting material for that content.  So if an author states "I believe wearing masks to be an over-reaction to covid" this should get a high support score since each person is an always an expert on their own opinion. If they wrote "wearing masks is an over-reaction to covid" this would only get a high support score if they had strong empirical studies showing such a result.  Otherwise the would need to say something like.  "I know lots of folks not wearing masks and they dont appear to be getting covid".  This could receive a strong support score, but is clearly a weaker statement.

- COHESION -- A measure of the degree to which some content forms an internally consistent coherent whole.  In the case of an argument, it is the degree to which the argument is logical in the connections it makes.

- BIASED LANGUAGE -- A measure of the amount of language biasing being done.  Biasing refers to the use of emotion laden descriptions in cases where neutral alternatives exist.

- CONSTRUCTIVE -- A measure of the degree to which content is intentionally aimed at subverting execution of the forum mechanism in some way.  e.g. 

_
#### -- Adjudication Rubricks --

ADJUDICATION RUBRICK -- A written description of what constitutes good and bad content according to this dimension.  

The rubrick should be precise enough that well-intentioned individuals adopting dramatically opposed stances on issues of common interest can nonetheless generally agree on adjudication of content within their area.  This need for general agreement on adjudications means that the adjudication dimension must be limited to areas that are narrow enough and can be made precise enough that well intentioned individuals can agree on the dimensions even when they disagree on their support for the content itself.


EXAMPLE:
For example well-intentioned individuals who are both for or against particular groups should agree that refering to those groups as "freedom fighters" or "terrorists" is a more biased adjective than callin those same groups "combatants".  Notice these groups likely can agree that both freedom fighter and terrorist are more biased than combatant even though they would strongly disagree on which biased term was more appropiate.

Each rubrick is defined by a totally ordered scale of adjudicative stances one might take.
For example, the bias dimension might be broken into a four point scale:
- clinical -- content is studiously choosing to use the absolutely least biased terms for all aspects of the content.  
- neutral -- generally using neutral, non-emotionally loaded words to describe concepts
- biased -- readers viewing the words in this content can clearly assess the bias of the author by looking at the descriptors and language being used w/o its surrounding content, and alternatives do exist which still convey underlying meaning w/o bias.  Still author appears to be attempting to reign in their biased language, and while biased this langauge is not factually without merit, nor chosen singualarly for its emotional connotations.
- flaming -- Author is flagarently using biased language with little regard for neutrality.

This rubrick would includ dozens of specific examples of text that had been adjudiated to global consensus on what kinds of language consitute specific levels of bias.

_
#### -- Adjudication Voting -- 

VOTING -- just publish a note 

+feedback

_
#### -- Adjudication Consequences --

Our objective here is to take the smallest possible corrective action sufficient to keep the Forum fairly constructive.

Corrective Actions:
- WITTEN FEEDBACK -- Notice the act of voting on a dimension is tied with the act of simply providing feedback to that user about their content.  This encourages the first key line of defense is constructive non-corercisve feedback on how to improve their content.
	- PRIVATE FEEDBACK -- The lightest possible consquence would be providing private feedback.
	- PUBLIC FEEDBACK -- The next level 
	- MODERATOR FEEDBACK

- SCORE FEEDBACK -- Once enough readers have provided public feedback the system will score content according to the readers average feedback

- CONSENSUS FEEDBACK -- 
_
#### -- Adjudication Automation --

ADJUDICATION AUTOMATION -- Once defined levels of consensus in adjudication against some content, some user, or some usergroup, the Forum itself will take defined corrective action.


IF-THEN-RULES:  Just as with etiquette rules these rules are written down both in english as well as in forum executable "if-then" style.  consequences can range from simply downgrading an items score, to reducing its visibility, to downgrading priveldges, and finally to deletion or banning.

RULES OF ENGAGEMENT:  The collection of if-then rules used by the forum to operate the forum are called the rules of engagement, and they are an agreed upon computable adjudication-action plan that serves a the last line of defense in driving constructive forum discourse.

ADJUSTED BY COMMUNITY:  Also just as with etiquette rules, these rules would be adjusted over time by community consensus.  Consensus on rule updates would themselves automatically be adopted by the forum software as the communities achieved sufficient consensus on particular rule change.

POTENTIALLY BIFURCATED:  The rules of engagement are coercive.  They ultimate can ban a user or remove content.  Some communities may want to keep a tighter reign on discourse, while others may want to allow a more rough and tumble play.  

SUBTREE JURISTICTION:  the root of the discourse tree is adjudicated according to the root or default adjudication rules.  But any child topic area created may specify a different adjudication jurisdiction by simply referencing this new adjudication automation document.  Any content within this subtree will then play by those new rules.  Just as with every thing else in the forum, if the larger community does not want to play by those rules, they will "vote with their feet" and create and move to that same topic to a jurisdiction where they do like the rules.  (see eyes and feet)





, and it is possible that different domains of discourse would want to adopt different form of adjudication automation.  Some communities might favor 

Specified levels  of adjudication against ertain levels of adjudication certainty and 


_
#### -- Adjudication Meta Adjudication --

- Does your narrow assessments match your broad assessments within group



- Errors of systematic bias will occur "in favor" of your group

_
### --- REVIEWER SCORING ---

- AGENDA CHECK -- compare judgement against those with contrasting beliefs
- UP CHECK -- comparing judgements to those with higher score
- DEPTH CHECK --
- FORWARD CHECK -- compare judgement against future judgements you or others make
- BREADTH CHECK -- compare judgement to judgements made on same question by many other judges
- TRIANGULATION CHECK --
- SPECIALIST CHECK -- compare judgements with those made by relevant specialists


DIVISIVE ADJUDICATION -- An adjudication that has a strong systematic judgement delta between a group and the population as a whole.

ADJUDICATION EXCENTRICITY -- The difference between the mean adjudication rate for a specific judgement within a group relative to the population as a whole.


CONSENSUS GROUP -- Group of adjudicators that render similar judgements.
CONSENSUS SCORE -- A measure of how well a reviewer matches the median judgement of their closest group.






FUTURE SCORING -- Does the future validate or refure ones adjudications

WEIGHT SCORING -- Does greater scrutiny validate or refute ones adjudications

EXPERT SCORING -- Do highly rated adjudicators validate or refute ones adjudications

MIDDLE SCORING -- Do those leaning towards you validate or refute ones adjudications








### --- COHESION VERSES FREEDOM ---
#### _

A key aim of the forum is artful combining of two opposing forces towards _**Freedom**_ and towards _**Cohesion**_ 

**FREEDOM** -- the ability of any user or group to carryout their discourse without external constraint. 

**COHESION** -- the expressing a range of human though within a singular simple whole which captures the key aspects of that full range of thought in the simplest most concise way possible.

The Forum has two mechanisms to manage these competing objectives:
- Voting with eyes and feet
- Top-downing and bottom-upping

_
#### -- Voting with ones eyes and feet ---

**VOTING WITH YOUR FEET** -- Anytime a reader or author feels strongly that a particular scope of discourse has been "poisoned" by a non-constructive issue framings, non-constructive published stances, non-constructive cliques of other users, non-constructive rules of engagement, non-constructive rules of etiquette, _they can just leave_!  They can create their own verions of that same area of discourse, but with a context that is "not poisoned" by a non-constructive context.  Notice this judgement is theirs to make, there does not exist any meta judgement upon judgements of constructivity itself.  Any user or group and just pick up their marbles and go elsewhere (potentially ?maybe? creating a full copy of the content subtree as a starting point if they like)


But "voting with your feet" taken to the extreme will completely fail.  It will yield as many copies of human discourse as there are humans.  There will be no cohesion as all.  Thus:

**VOTING WITH YOUR EYES** -- The more engagement that a subtree has relative to other peer subtrees (readership, authorship, and voting) the more _**visibility**_ that tree will by default when any user approches this areas of the tree.  

Thus each authors will has a tradeoff to make.  No community, no topic framing, etc. will be perfect in their estimation, the dominant published stances will not be exactly right, etc.  So either they can create a whole new topic area try to build a community from scratch, or perhaps just create a new forumlation of a given stance and try to garner support.  Otherwise they can just edit and contribute to the dominate scope which already has great visibility.  


All areas of the forum will have some level of freedom and cohesion, by adjusting the forum rules one adjust propensity toward freedom vs cohesion.


_
#### -- Top-downing and bottom-upping --

Dynamics combine best aspects of top-down and bottom-up to yield a workable non-coercive, cohesive, multi-perspective synthesis of user thought on topics of discourse.  Specifically:

**TOP-DOWN SYNTHESIS** -- Any user or usegroup may author and maintain a cohesive, top-down, single-author-perspective of some area, some issue, some stance, some body of data.  
- COHESIVE -- Such summaries are good because they can sythesize multiple variational thoughts into a comprehensible cohesive presentation of a body of thought.  
- COMPREHESIBLE -- Such summaries, being human generated can be more comprehnsible than a machine combined summary of the same area.
- CONCISE -- A well written synthesis can concisely express the core themes underlying huge range of discussion/debate on a topic.
- COERSIVE -- Such summaries are coercive in that they inevitably adopt a framing which better matches certain lines of reasoning over others.  Further as it is a summarization potentially of hundreds or thousands of human perspectives it will necessarily be coercive in selecting a small fraction of all thought to represent the whole.
- BIASED -- Such summaries by additionally be biased as they come from one group or one user.  It is possible the for the author to take great pains to avoid such bias, nonetheless it continues to be large concern for any top-down summarization.

**BOTTOM-UP SYNTHESIS** -- By contrast, using expressing areas, issues, stances etc. using a covering set of perspectives has almost the opposite set of properties:
- FRACTURED / NON-COHISIVE -- Multiple, independently-authored, competiting perspectives are simply presented as a list of alternatives as a covering of an area, issue, or stance.  Such a list will necessarily be much less cohisive than an authored synthesis.
- COMPREHENSIBLE / CONCISE -- A list competing idea will likely be harder to understand as a whole, and will be much less concise than an authored summary.  (To mitigate this effect the algorithm strives to select a small number of "covering" pieces in order minimize this bloat, and rules
- NON-COERCIVE / UNBIASED -- Covering content lists are much less coercive/biased than top down authoring is two ways.  First, simply the act of providing multiple, chosen-diverse documents instead of a single document is inherently less biased, coercive since each new document cover intentionally opposing perspectives.  Second, by ordering presentation with greatest coverage first and allowing users to support multiple alternatives we are putting pressure towards consensus documents that effectively combine the capturing of ideas from multiple directions with in ways that are supported by many users.



**TOP-DOWN GAMIFICATION** -- The coverage mechanism and presentation mechanics are tuned to drive otherwise coercive / biased users to strive to publish more neutral single author top-down content.  When this succeeds one can get the both concise/cohesive AND non-coerecive/non-biased.  How does this work?

- System mechanically computes bias/coercion score for each each potential summaries for each scope.  If there is one or two notes with high consensus scores across ALL interestd groups then this consensus these consensus document and will not present any single author sumaries for an area that when there are no 

_
## === METRICS ===
### ...

Forum automation performs it decison making on the basis of a handful of key metrics.

The key idea of the Forum is to have an entirely mechanical process at it center.
This does not mean the materials curated by the Forum are free from bias, it means that the forum itself is designed specifically to avoid having any instrinsic bias between biases.  It seeks to summarize and present the range human thought on topics of human interest.

_
### --- KP-SPLIT ---

Identify a covering set of maximally distinct schools of thought within a given population.

**KP-SPLIT** -- Finds the k _most distinct_ schools of thought among the most compact p-sized aggregates.

Given:
	_G(i)_ a set of groups of population memebers.  (the entire population may be expressed as a set of singletons)
	_k_ is the number of distinct schools of thought to return
	_p_ is the minimum size of the compact aggregations (expressed as a percentage of the whole population)

	let _pop_ = |union(_G_)|											// the size of the population
	while min( |_G(i)_| ) < _pop_ * _p_:
		_smallest_ = argmin _g_ in _G_ { |_g_| }							// Pick the smallest aggregate
		_closest_ = argmin _g_ in _G_ \ _smallest_  { dist(_g_, _smallest_) }	// Pick is closest neighbor
		_G_ = G\smallest\closest + union(_smallest, closest_)			// merge these two to form a bigger aggregate
	while |_G_| > _k_:
		a,b = argmin a,b in G x G { dist(a, b) }					// Pick the closest pair of aggregates
		G = G\a\b + union(a, b)										// Merge these into a larger aggregate
	return _G_														  // Return the _k_ most distinct aggregates
	
_
### --- BIAS - a measure of the uneveness in support provided ---

_**NoteBias(note, [usergroups])**_ -- Note bias is a measure of the uneveness of its support across a range of user groups.

for each note and each user it is counted only if that user has supported at least one note peer (including itself)

note bias is presented as a gaussian combination of the bias distribution for each group.

if bias is presented as a single number then it is the lower bound of the 95th percentile of its distribution.


_
### --- COVERAGE - an ordered subset of a notes children that "cover" perspectives of interested users --

**COVERAGE** -- an ordered subset of a notes children that best represent the full range of perspectives on a particular scope of disucssion.

- CONCISE -- The coverge sequence is ordered to present the broadest range of represented human thought with fewest entries at front of list.
- REPRESENTATIVE -- The sequence maximizes _diversity of thought_ covered, it does NOT maximize the number of users covered.


REPRESENTATION OF THOUGHT **NOT** REPRESENTATION OF USERS
For a variety of reasons it is likely that the forum will user counts will significantly under or over represent certain lines of thinking (maybe many more liberals than conservative will sign up).  And even if one were to obtain representative samples of people that should still not be use as a measure of the value of a thought.  Right is right even if only a minority see it!

So instead of a popularity contest of ideas, we aim for a _**representative covering**_ of thought.
These thoughts are maximially diverse and are yet still well regarded by at least some minimal size subgroup of people.


**COVERAGE CALCULATION**
The coverage for a note, N, within the discourse tree is an ordered sequence of its children that best "cover" the opinions of those in the user groups that have voting rights on N:

- let _Bests_ be [], Scores be []						-- Extended one "best" perpective at a time
- let _Coverage_ be 0
- let _C(i)_ be the children of _N_						  -- These are the possible key perspectives
- let G(g) be the user groups allowed to vote on N	  -- These are the opinions we must cover
- let _S(g)_, the kp-split usergroups, G, 				-- Major "schools of thought"
		based on their voting record on C.
- let SW(g) be 1/n where 'n' is the					  -- Scale group by its size
	number of users in split S(g)
- let W(u) be the max SW(g) the u is member of		  -- Scale each user by their rarest group
- while _Coverage_ < threshold
	- Let _V(g,i)_ be the voter support rate 			-- Updates each childs support after removing 
		for child C(i) in split group S(g) 					first "Best" children from consideration
		after dicounting children in Bests
	- Select b the C(i) with greatest cumulative 	  -- Find the next best child
		support Sum(i) across all thought groups.
	- Append b to Bests; Append Sum(i) to Scores
	- Let Coverge = Coverage - Sum(i)				  -- update coverage 
- return Bests, Scores								  -- Returns 'best' representatives for tree


_
### --- CONSENSUS ---

Dendgram-based survey of opinions.

**A MOTIVATING EXAMPLE**:  
Suppose out of 90-liberals and 10-conservative we have 90% support for a note, 90 votes of support from the liberals, and zero from the conservatives.  Can we say we have reached a consensus on this note since we now have a 90% support rate?  Hell no!  But suppose instead we have 77% support from 70 liberals and 7 conservatives?  Now that seems much more like consensus even though the raw percentage of support is lower.


COMPUTATION
(1) Compute a kp-slice of the group or user dendogram
(2) Compute the support rate for each of the k slices of users
(3) Return the minium support rate as your consensus score

For any given mesure of consensus we specify some minimum threshold and say consensus was achieved if that threshold is exceeded.  The threshold may be require a working consensus or an absolute consensus, and it may require a higher 2/3rds or 4/5ths consensus as well.


Absolute Support(Note) -- Fraction of all members of a group that support this note above any other note
Simple Support(Note) -- Fraction of all members of a group that support this note relative to the number that have expressed an opinion on this note or any of its alternative peer children.



_
### --- RATIFICATION ---

RATIFICATION -- Board members request members of a group to consider support for some note or notes.

The can be done to short circuit noisy discussions if they feel that consensus is possible.
They can do this in many ways, but need to no "harass their members" with too many questions.

- CALL FOR CONSENSUS -- They look at best parts of multiple stances and write what they see as their groups consensus.  In order to achieve consensus they may need to highlight both sides of one or two contensions in the group, thus everyone can support since everyone feels heard.
- SUPER CONSENSUS -- The board band together with the boards of like minded groups to create a super consensus spanning these multiple groups.  Then each group can ask its own members to ratify it.
- CRYSTALIZED CHOICE -- If the board sees several key stances emerging, but it does not know which is best support by their members they could ask them to consider voting for one or all of them.

_
### --- INTEREST - measures how much a set of users cares about a particular scope of discussion ---

**INTEREST** -- measures of how much a group of users cares about a particular scope of discussion.

- BROKEN INTO SEVERAL MEASURES OF INTEREST:  NUMBER OF READS, COMMENT, VOTES, and EDITS executed by the group.
- EACH CAN BE EXPRESSED as 0/1 or as mean number of occurances
- SCALED BY SIZE -- larger groups will naturally have larger amounts of activity thus larger numbers of interest indicating actions.  Thus we consider the number of these interactions as a fraction of the group size to account for this.
- CORRECTED FOR SMALL GROUP SIZES -- This calulation over weights very small groups since it is much easier to obtain 100% interest as a group becomes tiny (or just one person).  There are several correction factors that can be applied for this.

_**SimpleInterest(users, subtree)**_ -- Fraction of users within the group that have cast any vote within a scope.   (This score would be trimmed or ignored for very small group sizes)

~~~~

_**PostingVolume(entity, subtree)**_ -- The number of posts that user or group of users have posted within a subtree of the discourse tree.

_**VotingVolume(entity, subtree)**_ -- Similarly voting volume is the number of support assertions made within a given subtree.

_**VotingPolarity(entity, subtree)**_ -- One minus the 95th percentile lower bound on the entropy of voting for children at each level within the subtree.
     (really need to work on this metric!)


_
### --- GROUP MEASURES ---
#### --- GROUP DENDOGRAM - the dendogram of user groups ---

**GROUP DENDOGRAM** -- the dendogram of user groups as orgnaized by a measure of group similarity.

_
#### --- GROUP SIMILARITY - measures how similar the discourse behavior is between two groups of users ---

**GROUP SIMILARITY MEASURE** - any real-valued computation based upon two groups of users along with their record of posts.

**VOTING SIMILARITY** - the voting similarity of two groups of users is the count-weighted average KL-distance between the pdfs of their voting record at each note in the discourse tree.
_
#### --- GROUP BEHAVIOR - a measure of the discourse behavior of a group ---

**GROUP BEHAVIORAL MEASURE** -- any function of the posts from a group (possibly scaled by the posting behavior of non-group members)

**VOTING BEHAVIOR** -- a mapping of notes onto an integer number of votes cast for that note.

_
#### --- K-SLICE - a partitioning of users or usergroups into k sets with maximally distinct thought ---



_
#### ...
- ALT SET -- List of nodes within a given altset defined by transitive closure of ALT
- COMPETENCE -- Authors are scored on their competence along several dimensions:
	- ADJUDICATION BIAS -- 
	- ADJUDICATION ACCURACY -- 




_
_
## === GLOSSARY ===

**DISCOURSE TREE** -- the global tree of all hook names

**NOTE** -- The current state for a named bit of human thought.  (see note data type)

**POST** -- A message containing the state of a note at a given moment in time (the tree of notes is updated by posts overwrite old notes contents)

PUBLISH -- A user 

**SUBTREE** -- A scope is the subset of all notes within the global tree of notes.The scope for a note within the note tree 

_
## === EXAMPLE ===

**GUN RIGHTS ISSUE AND STANCE**
Joe creates "Gun Rights" issue under the "Policy Topics" node
Joe creates a stance "Guns ownership is a key right in a democracy"

Several others add different stances and comments to this issue.

**GUN RIGHTS CONSENSUS STANCE**
Joe creates a consensus note and selects 4 of the top 9 stances claiming his note covers them.  He does not get consensus from the supporters of those 4 stances, but 3 out of the 4 groups do agree with him, so he removes the one stance froming his consensus note, thus forming a narrower conensus of his gun rights stance.

Jeff creates a consensus stance "Guns Save Lives" which merges 3 more gun related stances.. it also pro-gun, but is takes a really different than the gun rights angle Joe used. 

**ANTI-GUN ISSUE AND STANCE**
Chris does not like Joe's "Gun Rights" framing of the issue itself.  It frames the discussion purely in terms of rights and freedoms which effectively biases towards his own stance.
So Chris creates the issue "Gun Control" under "Policy Topics" as new issues statement and claims it is a "consensus" version of Joe's original "Gun Rights" issue.  He also creates a strong anti-gun stance called "Guns Kill" under these two versions of the issue.

The larger community cannot agree on the "gun right" vs "gun control" version of the issue to present, so both are are listed as alternatives when you search under "Guns"

**CONSENSUS GUN ISSUE**
Kim has lots of very highly visible consensus pieces, and she sees an opportunity to build a new consensus piece, she is a firm gun control advocate, but still does a passable job of summarizing the three key stances on the issues:  Guns are rights, Gun save lives, and Guns kill.  She struggles finding a super neutral title, but eventually lands on "Gun Policy".  She searches for highly rated pro-gun supporters of other stances, and finds Jeff who has also authored many consensus pieces too, but he is very pro gun.  He edits her piece and after several rounds they are both satisfied, and they jointly publish this piece to their respective board of directors as a consensus piece.

**BOARD SUPPORT**
Their boards recognize this is a very contention issue and edit its permissions as board-editable only by both board-groups as well as by the original authors.  They push a request for ratification to all in their groups who have supported any stance on any related issue framings, along with the unanimous support from both boards.

**RATIFICATION**
The larger communities ratify this note as a consensus framing of the issue.  
Now when users type "gun" into the search bar this consensus "gun policy" note comes up first, and all existing issues that are identified as synthesized by this consensus will also have a link back to this consensus note that a consensus was formed, so readers can follow that link to go to the consensus note.


_
## === ADJUDICATION MECHANICS ===

1. TOPIC -- Sponsor or community select topics for consideration.

=== COLLABORATION PERIOD ===
2. INITIAL AUTHORING -- Left and Right authors are selected at random from appropriate quality class based on resources provided.
3. PAIRWISE COLLABORATIVE REFINEMENT -- Each pen their own separate analysis, and collaborate with each other to constructively revise their own analysis based on feedback from their counter part.
4. CONSENSUS STATEMENT -- They also work with their counterparts to collectively author a list of consensus points that both accept as supported.
5. OBJECTION CODIFICATION -- In cases where one side does not accept an edit from the other, the objection is formalized as objection or a qualifying adjudication.

=== FEEDBACK PERIOD ===
6. COMMUNITY FEEDBACK -- The larger community is then invited to comment on all parts of the analysis.  The consensus points made, the left and right framings of the issue, as well as all objecting adjudications made by the initial collaborators.
7. RECURSIVE FEEDBACK -- Depending on the resources allocated to this topic, and the amount of contention found a certain amount of recursive adjudication of community adjudications is also performed.
8. SEALED HOLDBACK -- 20% of the adjudications are randomly sealed.  The results of the adjudication vote is counted in as part of the feedback process, but the name of the adjudicator and the text of their analysis is sealed by the system.
9. POSITION EVOLUTION -- Over time the system maintains a current "best" leading edits of the original position, as well as a best list of supports and objections for this best piece.  (NOTICE: only those on the left are free to suggest edits to the left position, and to suggest edits to objections to the right, etc.)
10. CONSENSUS SUMMARY -- After sufficient multi-round feedback and quiecence is reached, the final consensus statement, left and right positions as well as best objections are captured as a completed analysis for the topic in question.  This is published as an completed analysis by the forum.
11. EVOLUTION -- Over time as interest indicates, or as additional facts become available participants may continue to augment or shape the resulting consensus.  Thus important issues may remain "live" for an extended time.  Others will simply stand as an analysis done at a specfic point in time.
12. EDITS -- During feedback each adjudication is added with visibility into all over adjudications and comments made prior to its submission.  Judgements about this adjudication are based upon the precise state of all relevant adjudications at the moment of submission.  But later as new adjudications are added and new discussions are had, each participant is free to EDIT their adjudication's body text, as well as the adjudication's vote as well.  The analysis of each adjudication is performed with the benefit of all published info at the time of analysis.  Thus it is usually beneficial to edit ones adjudication if new ideas become known.  There is a modest impartiality penalty paid when one changes ones own mind, but there is a much larger penalty paid when adjudications are finalized against your position, thus changing that position of one is wrong is generally better.

12. HOLDBACK ANALYSIS -- At a later time these sealed holdback adjudications are randomly sent to multiple participants with similar left-right bias for evaluation.  This is done to assess the assessors.  The goal is to perform a kind of auditing of the specific adjudication that was submitted.  Was it clear, did it accurately track the published rubrick for the conclusions it drew, etc.  The goal is to gather feedback on this adjudicator, as well as a test of the one the adjudication is sent to.  This test check if the hold back recipient is consistent with all other recipients of the same adjudication, and it is a test of the underlying participants original adjudication.

	The results of this holdback analysis are used to update the impartiality, accuracy, clarity, etc. scores for all participants involved.

13. TRIANGULATION -- Some fraction of these holdback adjudications are further analyzed thur triangulation.  During triangulation highly impartial (but still biased) judges are then called upon to perform a deeper analysis of this same adjudication.  This deeper analysis is used to further adjust the impartialty scores for these lesser judges.  Triangulation not only involves judges rated to be especially impartial, but it also involves multiple of these judges who must achieve consensus in order to the triangulation to proceed.  In those cases all lesser judges are updated based on their agreement with or disagreement with the senior judge consensus.

14. EDITS -- 


9. s of a
- 
- to derive a subset of consensus points.


## === CHEAP HACK FIRST SYSTEM ===
- GDOCS -- Use personal google doc accounts with public pages a basis for each person's note contributions.
- SERVER -- Synthesis web server.  Scans pages and presents tree (with comments etc.)
- ANNNOTATION COMMANDS -- Text-based annotation commands
- BOF -- Birds of a feather, users that mutually agree to share editing rights.  (social trainee)




Annotations:
- >Note 7496		>N7496	-- Header for note 7496	
- >Talk 2953		>C2953	-- Header for section discussing Indicates what follows is in reference to note 2953
- Version 113	V113	-- Provides version number for a note

C2953-Ver113 

U10-11	How do we know this is true?  Did this come from a study? some expert's opinion?
C43		Just because the rate of gun usage was higher it does not follow this is the cause of lower thefts.
		But you can say there was a correlation in this one case, you also cannot say it is a general trend w/o more evidence.

_
# ### INFO ###
## === PTRS ===
### --- COMPANIES ---
#### -- For pay fact checkers --
[Tech Crunch about Reuters](https://techcrunch.com/2020/02/12/reuters-facebook-fact-checker/) 
Reuters joins the list of US fact-checking partners that include The Associated Press, PolitiFact, Factcheck.org, and four others. Facebook offers fact-checking in over 60 countries, though often with just one partner like Agence France-Presse’s local branches.

_
# ### LOG ###
### 2022.02.16 -- Email based first system.

- Use scripted email or forms based system to select topics for analysis.
- Use script to select author pairs


### 2020.09.17 -- Police violence

An example:

Suppose a reader were reading a note/post about police violence against Blacks, and in the author’s opinion such actions ``almost always occurs when someone is being non-compliant with legitimate police request.''

Now the reader remembers reading an article where someone listed many cases where “the Black did nothing wrong”.  So they create a post for the news feed talking about this, and they highlight the sentence in the original posting and create a “-1” link to their new post.

~~~~

Later others plus +1 and -1 his original counter argument, each one adding evidence in one direction or another, and most people are creating a blog entry that they intend to read like a normal threaded conversation.

~~~~

After ten of these someone in the same user group as she is in edits her post/note to have the name:  "police abuse of compliant Blacks” 
And quickly cuts and pastes content from 10 messages into a nice list of evidence with phrase summaries for each and a link to original sources if those posts included them.  This person was a bit biased however and did not address the many counter evidence to this counter evidence.

~~~~

The original author sees this consensus stance against his original stance is ignoring all of these other posts (which he can easily see), so he summarizes them and creates the counter summary and ties them together as a different stance on the same note “police abuse ….”

~~~~

After after many edits and time original posting bubbles up as a key “pro-police” stances… so it gets more scrutiny … and cooler heads prevail over this little sub argument about "police abuse of compliant blacks”

This cooler head is able to do justice to both evidence in favor and against this assertion that this happens.  They even put a cohesive title on their piece:
      ``Police abuse of compliant citizens — it does happen, but not often’'

Their summary does a good job of weeding thru the best evidence in both directions at the top, and left the rest in a sub note for reference.
Their note did not get as much support as either of the other two stances, but the forum algorithms recognized it as less biased, as so put it at the top with is summary text fully expanded, and left all others posts with just their one-line link listed below it.

~~~~

I am trying to make it so that people can engage in tiny ways that move the final argument structure forward.

The HARD part is ensuring that folks with really competing views aren’t just deleting or otherwise screwing with opposing content.

—dan

_
### 2020.09.07 -- Shinster email

I will have failed if I get all the right “experts” to contribute but then they are not believed.
Instead I would hope to get leading anti-vaccers on the same site with immunologists, and let them each present their perspective, but they cannot avoid having the best critique of their perspective posted right along side their position.


The website would look a little like wikipedia, but instead of a single article on “vaccine issues” 
Multiple people might edit and fork a range of framing summaries, and we would present the middle least biased one (determined by math + reviewers)

Then under that would be a range of “stance” posts that would try to clarify a single narrow issue like   ``what does the data say about their safety?’'
This would get split into a range of “for” and “against” arguments, many people can edit and fork these arguments, so there are many.
But we algorithmically judge “logical precision”. And “biased presentation” etc.   we do this in the same ways that folks like https://mediabiasfactcheck.com/ do.

The hard part would be getting liberal and conservative, pro-science and science-skeptical folks who were open to rational debate….  I think one could essentially “judge the judges”
This is tricky, since I need to do that judging in a way that everyone agrees is unbiased.  I think it would be based on internal consistency against a set of agreed (written rubrics) for assessing “logical presentation” and “biased presentation”

Judges themselves grow to have greater scores in their ability to discern and in their neutrality.  So it becomes a self policing system where judges are judging each other and we measure consistency in blind comparisons.  (e.g.  do judges consistently agree)

Big picture:
— Use the energies that many have to “speak out” about issues they care about, energy to “be heard” as the driver for content.
— Use comparative analysis between users to see the more neutral better framings of pro and anti- stances
— Force each person to see the counter evidence for the things they believe.


p.s. a-la-carte idea that was missed in above:

The idea is to create a deeply annotated discussion of the issue, by allowing users to comment directly on the sentences in the leading stance that they oppose.
They can “simply wrong” then a short paragraph showing with references why that sentence are wrong.  The author of that stance probably does not want a highlight shown to everyone that says “simply wrong” in the margin along with references!  So they will probably edit or remove that sentence.  Which is fine, then the consensus stance become more justified.  

The idea is that as you read a stance and all the counter points you can click buttons to agree or slightly edit what you see, and the algorithm presents everyone’s edits as a consensus… showing the annotations with heaviest support most clearly and immediately.   Other annotations are still there, but you really need to read EVERYTHING on the issue to see them.


_
### UNSORTED ###

- [Change My View](https://www.reddit.com/r/changemyview/)   channel dedicated to persuasive arguments 

THE FORUM

Key ideas

TWO VIEWS



TWO ORGANIZING PRINCIPLES --  ‘by-time’   and   ‘by-topic’
Discussions (used for information input) are best organized by time (that a post was submitted).  Global organization (used for retrieval or output) are best organized by topic/sub-topic.

USE BEST ORGANAZATION FOR

MOSTLY AUTOMATIC TRANSLATION BETWEEN ORGANIZATION

 
_
## === PRINCIPLES ===

COMPETITION&EGO DRIVEN

CONSCISE&POLISHED -- Goal is to have people compete on providing conscise and polished 

MECHANISTIC&UNBIASED
  Mechanistic (and thus position neutral) processing.
  Universal objective procedures applied uniformly to all positions
  on all issues.
COMPLETE
- Support (or opposition) on a submission only affects its promanence
  not its publication, all submissions are published.




