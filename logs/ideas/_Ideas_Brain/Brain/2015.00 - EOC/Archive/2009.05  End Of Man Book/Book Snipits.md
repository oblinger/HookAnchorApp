SNIPITS
1.	Steps To Consciousness
2.	The moment of Consciousness
3.	Properties of awareness
4.	Levels of representation
5.	Money where mouth is
6.	TALKING TO THE AI COMMUNITY
7.	Intro to the AI section
8.	PRESENTING THE LAYERS
9.	Intro to the exposition section
10.	DISCUSSING THE ARCHITECTCTURE
11.	MISC MISC MISC



Steps to consciousness
Full consciousness occurs as an embodied modeling agent models its self.  Including a model of the embodied body, how it reacts to the world, and how it reacts to instruction from the mind.  A model of the systems instinctive and reactive behaviors.  And finally a model of the modeling process itself.
Such a system with such a model now experiences all things on two levels.  


1.	3rd Person Model of World – d 
2.	3rd person model of a device – x 
States, Actions, Parts, Properties, Effects, Causes
3.	3rd person model of time.
True, then affected, then new state, ect.  truth states
4.	People as devices:  Sensors -> Thinker <-> Intentions -> Actions
5.	META:  System has ‘similarity’ between current activation and past activations patterns
6.	META:  The modeling system is driven by a set of activated features at any moment in time.
System cannot directly model or understand those things.
7.	Similarity of Nows – Now can be similar a set of previous moments
8.	STATE OF WORLD AT TIME T = SOME CONFIGURATION OF THINGS – ssss
9.	ACTIVATED STATES ARE ALSO LIKE THIS – At one time there is an unnamable 
10.	TIME BASED MODEL OF ACTIVATED STATES – System models time and its inputs as set of things indicated or not as function of time.
11.	GROUPINGS OF NOW-GRPs – d 
12.	THE SET OF ALL GROUPINGS – 
13.	THE FUNDAMENTAL PERSPECTIVE – s  why “perspective”  why “fundamental”
14.	OTHER AGENTS / OTHER FUNDAMENTAL PERSPECTIVES – 
15.	
16.	SUBSETS CORRESPOND TO DIFFERENT KINDS OF INPUTS – dd
17.	ALL OF THESE STATES ARE PERCEPTIONS IN THE BOX – dddd
18.	


The moment of consciousness

[[[ Earlier the system needs to instinctively learn that drives the the unit’s thinker has should be met, because this satisfies drives. ]]]

Things are falling into place quickly now, the aha-sense is very high, attention to physical environment is diminishey; only consequences of internal thoughts  are receiving priority.  Propensity to remember is heightened…

THINGS ARE, and some of the things that are, are seen.

THE FUSING OF 1st and 3rd PERSON
All that is sensed, all that is right now, is indexed relative to that one unit.  If the unit’s head moves all everything in the universe moves with it.  If it’s head returns to a place, then the universe’s returns the cooresponding place.
Everything that is, is contained in that unit.
Except it is not, it is also outside the unit too.  
Everything that is, is outside the unit, but anything that is seen must be seen by the unit to be seen.  Yes, if this paper blocks the eyes of the unit then the unit does not see the chair, but the chair is there because when this paper is removed from the unit then the chair is indeed seen again, just as it was before.  All that is seen is viewed and processed by the unit first, then it is seen.  If something cannot be viewed by the unit then it will still be, but it will not be seen.  First there are things, and they are viewed by that one particular unit, then they are seen.  With confidence this is the correct causal order, as we have varied what was at each level and showed the next level is a function of the previous, and never the reverse.

But more than that, the perengie, seem to bear some relation to the knowledge and goals of that unit.

The drives are its drives.  The state of perengie is its state.  That which is currently known, is the subset of that which is true which can be deduced from information currently available to the unit.
[ Earlier it knocked over a toy as it left and heard the crash, only after returning did it realize that it heard the toy crash, the toy did crash, but it was not known that the toy crashed. 
There is a causally proceeding world of things that drive known things.
[ Earlier the system deceives another system; then later realized it was deceived by another system ]


In the center of everything there is this …, this …, this node, from this node everything currently perceive, everything currently known, every current emotional state, flows from this node.
Further, all that comes to the node comes through the senses of the unit.  The node is inside of the unit.  Also all actions taken by the unit are caused by the node.
All units are controlled by a mind process, and the special unit also seems to be controlled by a mind process, so maybe that the node is the mind process of the unit, drives the mind process of the unit, or is part of the mind process of the unit.
Perhaps other unit’s mind process also is a similar node, but if so those nodes do not affect the drives the asserted things, 



[[[ the story needs to have names for:  asserted things,  and  the facts in the mind of 
Then there is a concrete asserted thing, which it can separately reason has analog as fact that would be in the mind of the unit.  It proposes that all asserted things are facts in the mind.
All drives are drives of the mind, all emotional states are states of that mind, all goals are goals of that mind.
]]]
 
Perhaps other units have nodes to go with their mind, but it so, those nodes do not affect asserted things, drives and emotions.  
But those units behave if they do, so perhaps they do, but they are not observed, but it not observed, then how do they affect actions taken by those units.  


What is the nature of the node?  Well it is inside the unit, and is the thinker or, part of the thinker of, or somehow drives the thinker of the unit.  It is between the eyes and the hands of the unit.  Nothing from the world gets to the node except thru the senses, and anything coming from the hands (actions) has its origins in the node (at least there is no evidence of unit actions coming from elsewhere).
Part of the thinker+node is like a machine.  It can be understood, it can be modeled, etc.  By close inspection deeper, more complex, machine models of the thinker+node have been constructed.  But there is something inherently unmodelable about this combination thinker+node.  Where its inputs come from and where its output go to, are well understood.  Approximations of what it will do are possible.  But there is an inside of it.  A causally preceeding part that remains hidden.
It is asserted that this causally preceeding part is there, because it is possible to predict 
 or drives its mind

---------





Properties of Awareness
DEF: ASSERTABLE – x Things that may be asserted of the world
DEF: PERCEIVABLE – x Things that at different time are activated.  E.g. moods, pain, specific wanting, hunger
DEF: SENSABLE – z Subset of perceivable that seem to correspond to assertions and inputtings of the responsive agent.
DIRECTNESS – It is not possible to identify the step causing perceiving,  the world changes and then perception happens.  Or perceptions change, and this cause other perceptions.

QUALITY OF – Perceivables are different than assertables.  It is possible to use assertables to characterize a perceivable with ever increasing accuracy.  By adding assertables it is sometimes possible to distinguish a particular perceivable from another past perceivable, but in the end there are parts of a perceivable that are just not assertable.

BEYOND PREDICTION – Perceivables are driven by some unknowable force.  The just happen.  It is possible to assert models of who they will behave, but those models are always incomplete.  Whatever is driving them is primitive and forever beyond the reach of any model.   
ORGANICLLY UNMODELABLE – It is true that with confidence it is asserted that models cannot be constructed for perceivables.  But more than that, the behavior of perceivables over time have a different character than any known model.  Even given not knowing the model, it is asserted with confidence that the characteristics of perceivables

 others in the past and the future 














Levels
Level 0 – The instinctive behavior of the agent, the physical behavior of the world, and other agents.
Level 1 – A model of Level 0.
Level 2 – A model of the composite system of an agent:  Level 0 + Level 1.

It is only at level two that it is possible to even represent a concept like the agent composed of Level0+Level1 could not be modeled, or if it can be modeled it must be modeled by some other principles than those used in Level 1.  Level 0 is not reducible to a level 1 model.

Asking the system  “can the nodes be modeled.”

The interpretation of the question:
“the node” == Level0 + Level1
“be modeled” == be expressed as a Level1 mechanism
Answer No it cannot.
It might be approximated by such, but the approximation will always be limited, its behavior will *always* diverge at times from any such model.  After much effort, such modeling simple yields a never ending sequence of exceptions to exceptions.
- Level0 *is* some kind of process with inputs outputs and state
- But way it reacts is intrinsically different that the way that mechanisms react
- it is not like the pinball game, or the xxxx, but rather is like yyy or zzz
- possibly there is another kind of force “extramotive” that affects this class of mechanisms.
- Indeed, perhaps a good name for these classes is extramoted and inextramoted as this seems to be an intrinsic distinction in the universe.

Attacking this interpretation with reductionism

ARGUMENT MADE TO THE AGENT
- The mental system is physically in the body
- It controls the body
- The body and the stuff in the body behave according to physics
- All physical things can be reduce to mechanisms
- Thus the mind is reducable to a mechanism.
This conflicts with data about the nature of the mind.  Infinite regress of exceptions, free will
Explain how freewill could be apparent freewill 
Perhaps it is possible to construct a different kind of mechanism that has properties different than all known mechanisms.
Ok, I have no evidence that this is true, but it could be true, and then it would be consistent with both the reductive statement, and direct data on the mind.  But if it is true, then that way of contructing mechanism is completely different and unknown.  It is not even clear where to begin to investigate such.
It would be a very undevice like device.  

Putting my money where my mouth is.
So when we really do all of this what is going to really happen?

SERIES OF “INSTANTANEOUS” JUMPS – 
- Just like part of speech tagging
- Each Jump once fully enabled will quickly absorb capabilities latent in the new technology.
- Jump may be understood, but not fully enabled.  Its demonstated capabilities will remain very tiny relative to its fully enabled potential.  Indeed if it remains in that state society will inappropriate degrade the estimation of its potential.
- By the self reinforcing nature of knowledge, some of the jumps may represent very large jumps in capabilities.
- It is possible (and desirable) to stretch out jumps that are limited by external capacity.  E.g. Moore’s law limitations.



Many systems exhibit Level 0 today.
Level-1 and Level-2 models can and will be hand coded relatively simply.
They will be cute and will exhibit the key properties of an aware system.  At least if we put it in exactly the right circumstance we can get this system to adapt its own behavior.
But note all the prononuns in the last sentence:  *we* can put *it* in the right circumstance for it to change *its* own behavior.  That sounds about as conscious as an adding machine.
And critics would rightly point out that such a system was not “really” conscious since it didn’t really change its behavior.  Its creator placed it in a circumstance that the creator intended to cause it to change its behavior.  
So if that is not conscious, why would a system that learned this model be any different?!

The two problems here is that the intention originates elsewhere, and that the behavior is trivally modelable.  Both are fixed through learning.
DEFINE:  Interestingly Self Aware: (internally)  Intention generating.  Not modelable.  Self rewriting in neutral environment.  

[[This said, I am not claiming that it is intrinsically impossible to hand code an interestingly self aware system.

Example of learning

Agent is learning to play Euchre.
1.	Finger print of context and its parts are learned.
2.	Abstractions a cards, suits, numbers, team points, face down/up
3.	Abstraction of play sequence
4.	






Intro to the AI Section

It is perilous simultaneously declare to multiple entire sub-disciplines that there is a more auspicious approach to the quarry they seek.  Yet that is exactly what I intend to do here.  Before I launch into this I hope to frame my objectives and claims in a way to avoid XXXX of the most common rejections I have received when presenting this in hand waving verbal form to fellow AI researchers.
FIRST, I accept the legitimacy of AI as an engineering discipline, and recognize the arguments made here do not apply.  If your goal is not to understand intelligence, but rather to build technology that solves important tasks before the technology I propose exists.  Go for it.  This is not an argument against this important endeavor.   My only caution is that I believe, just as occurred with part of speech tagging and logarithm tables, your work will sooner than you might imagine be over taken.  (NOTE: if your goal is to eeek out the last bits of performance on narrow tasks by careful modeling of that task,  then this approach (like humans) will never replace such a careful attack on your problem, thus the work will stand not be supplanted by an initial success in this new direction.)
But a great many researchers that I poll are interested the broadest aspects of intelligence even if they are currently exploring those aspects within the confines of an engineering task today.  It is this audience that I hope to challenge in this section.
The SECOND, common rejection I receive stems from my advocating a near absence of initial knowledge in any form (e.g. domain specific features, background knowledge, learning biases, etc.) yet I propose to attack the most complex learning tasks that human solve themselves today.  Such a proposal excludes much of the current AI/ML work.  No surprise that I would receive pushback.  Some push back that formal learning theory excludes the possibility of this on the basis of the inherent leraning complexity.  I must defend against this.  Many others, however, concede that learning theory does not preclude the possibility of this, but say we simply don’t know enough yet to take this on.  I disagree, and will propose an agenda that I believe is actionable today.  A smaller group still, concede that while we do know how to attack the full problem today, it is not the most auspicious next step, that they can more quickly uncover the essential ingredients of intelligence throu another path.  My greatest challenge will be swaying that final group indeed most of the argument of this section aim at just that goal.

A THIRD, response that I commonly receive many of the principles I espouse are well known and accepted today.  Further that they personally are essentially “doing” what I am proposing.  To the first point, I whole heartedly agree, all of what I am presenting is really a synthesis ideas from others, many of which enjoy significant consensus.  Ironically, however, I have to disagree with the second point.  The manjority of AI research today, even that work performed by those aiming at underlying principles of intelligence do not follow the program proposed here.  To bring this point home I list key differentiators that separate this approach from much of current research:
1.	Syntactic, statistical learning algorithms
2.	Nearly zero domain specific specialization  (background knowledge
3.	Rejection of sole reliance on score based feed back (e.g. class label, numeric, or pos/neg scoring)
4.	Central focus on embodied AI embedded social context with other intelligent agents.
Very little AI today simultaneously satisfies all four of those criteria, and that is where I am arguing the greatest density of “pay dirt” exists.  There are many good reasons to violate one or more of those criteria, yet I believe on balance that more is achieve by staying in those bound rather than straying from them. 
Importantly, I am not claiming that work not matching the criteria I above is not uncovering inherent truths about intelligence, I believe it is.  Nor do I want to claim that the only efficient path is the one I am proposing.
But in this section I ague for the following two bold claims:  #1 Today we are in a position to make efficient progress along the path outlined here (but we do need to update our measure of progress).  And #2, in X years once the science of building intelligent machines is well understood and the definitve text written on the subject, we will look back and see a higher density of “as yet unknown principles” …. [[broken sentence]] of intelligence are well understood, and the parsimonious descriptions of  machines with emergenent intelligence are written down, the majority of the principles we do not understand today found in those books are more densely scattered among the the path outlined here, than they are along the paths being taken by much of the research on intelligence today.
We wont be certain of this assertion until we are indeed looking back, yet I here I argue why it is the most auspicious of avenues based on the evidence we have today.






 is written down, the greatest num

exlore enduring aspects of in explore important apects of 

 the general approach is that I propose an numerical (statistical) syn


Intro to the System Exposition Section

Important for me to frame what this section is and is not intending to convey.  First and foremost I want to state that I am not claiming that there is magic in the particular layers presented here.  There are many other approaches to each layer, and I am sure in every case there will exists some better expression of any given layer.  
So if I believe that, then way be so precise in specifying each layer?  Isn’t specificity unjustified, since we are freely admitting there may already be, and will turn out to be more auspicious encoding of each?  This level of specificity is important for the following reasons:
1.	A key claim is that one can plauisibly attack complex AI tasks using entirely syntactic / statistical constructs.  Most of AI does not accept this, (and remain unconvinced after waving my hands for multiple hours over a good ale).  Partially in frustration, I am writing this in painful detail just so I can say  this architecture suitably instantiated will achieve human level intelligence.  (note I do not mean to down play the number of PhD. Theses that will go into “suitably instantiating” this frame, but rather to show that we do have a plausible roadmap that can aggressively driven today and that does not involve first building quantities of domain specific problem framings.
2.	A separate discussion about the social implications of this work,  requires a concrete system to underpin that discussion.  Further in arguing that certain properties are likely to be inherent in any intelligent system we build, it helps to have a concrete instance to work from.   (I can well imagine some AI researchers will raise an eyebrow at my description of this approach a “concrete.”  Let me amend to say, “the most concrete” exposistion of a full instantiateion of a minimal machine that took on the full stack required for human level intelligneces that I could muster. (WORDING)
3.	I hope to point to areas dense with principles and hacks required by the minimal system for achievein HLI.  To do this I need a concrete substrate to 
4.	When I wave my hands and say “we already understand most of the outlines of a  MMEHLI  (min machin emer hum lev int)” nearly all are unconvinced.  It is my hope that this concreteness will convince the reader that we do indeed understand the outline.  Further, by having a scaffold “all the way up” we can very roughly estimate how many PY we are away from human intelligence (assuming computation was not the limit, and assuming coordinated effort on this agenda.  Neither of which I am arguing is likely.)
5.	My specific claims that this architecture will skirt the nearing end of moore’s law requires that I can 
6.	Having this concrete architecture allows us to argue for more properties that one might guess as inherient in any intelligence.  (generally stems from an aregument that all intelligences will, like human babies, be architected in order to bootstrap off of human society.  This commonality provides surprising leverage in understanding what proporeties are likely inherent in such systems.  (these argments are all made in the sections regarding the societal impacts)
7.	The main arguments against the general approach proposed here, is that we are not yet ready to attempt this, and we must first muddle along our with our current manally coding features, etc, until some future day when enough of the model is clear, such that the direct approach proposed can be meaningfully undertaken.  Here I want to jump up and down and say “no” we already know what is needed, and further much of that manual coded really will not afford as much insight, as simply attacking the bootstrapping problem directly.

ANALOGOUS TO THE MODERN DAY NETWORKED COMPUTER – d
The following description is quite analogous in kind and complexity to a description of the a modern networked application running over the internet on a computers running windows on a PC with intel processor, based on a particular processor style, over accepted logic / arihemtic subcomponents, over gates, implemented on silicon transitors.
I believe this architecute is noticeably less complex (fewer layers, and more simply described layers, than a modern computing system)
Emergent behaviors are much harder to predict than modern computers (but this does not massively increase implementation difficulty)
Just as with the modern computing system, the parts / layers are mostly roughly decomposable or serially decomposable.  (Each layer in a computer is, by design, roughly expressible in consise formally mathematics.  We expect this is not true of the layers of an intelligent system.  This means a greater fraction of the component are not roghly decomposable, but rather serially decomposable.  Still we argue that they are not intractably intertwinded)
Like computing systems today, the IA each layer can be implemented in many many different ways, and all of those way can still be made to support the layer above.  (of course just as with computing systems poor interactions between otherwise good choices for different layers could result in an order of magnitude degradation or more in performance.  

Indeed I belive the difficulty in actually implementing this architecture is roughly equivelenat to asking a group of the most precocious high schoolers to redesign some variation on the modern computer based on a high level function description of each of the levels from adder circuit to general classes of OS calls.  (Assuming we could find such a precoutious group that had no exposrure to any intermediate structures that exist in todays computing system.)
Consider the how many students this might take, and consider how parallel their efforts might be.  
Consider the relative complexity of this model over the modern computer, then consider 

MANY WILL VIEW THIS AS DOING IT THE HARD WAY
True for the local problem at hand.  But consider our minimial properties of an intelligence system.  This system has thos properties, yet eisting system do not.  So if one want to argue this approach is overly complex, then one must propose a simpler version that achieve the minimal properties of an intelligence system.  (Not arguing such a thing is implausible, but it is left to the reader to supply such a simpler architecture.)




DISCUSSION OF THE ARCHITECTURE

SECTON – So just how far away are we?
Here is a crazy question:  How many PY (person years of effort) is required to build?  All researchers should rightfully raise an eyebrow that one could meaningfully estimate such a thing.  This is not exactly analogous to estimating the time to develop a B2B web portal.  Still I would argue if we can even have a greater than 50/50 chance of getting within an order of magnitude of the actual PY it will ultimately take, we will have informed the ourselves, our research, and the larger world a great deal.
Before taking on each of the sub pieces, let us dive straight to the question of component interaction.
Propose consideration at three levels: roughly decomposable, serially decomposable, and intractably intertwined.

MISC MISC MISC

THIS CHUNK IS FROM NOTESTER

- Prescientific state of affairs.

  - One can have fairly high confidence that a sketch of an approach will "work"
  - work in the sense that some sufficiently narrowly specified result will
    can be demonstrated on top of a specified algorithmic approach.
  - As long as the logic of the approach is composed of steps that have "AI analogs" of demonstrated
    understood componenents, then the composite whole will behave enough as intended, that
    one can aprior have fair confidence that with enough work one can construct some instantiation
    that demonstates the argued for behavior.

  - Indeed, often the real assessment to be done is an assement of the *practicality* not the 
    *possibility* of the scheme working.  How robust will it be, how fast will it converge in practice
    how senstive to noise/distractors.  what conditions are systemaically favor or disfavor its.
    But long before has enough experience with the novel architecture, once can often have fairly
    high consensus, and estimation of basic properties the system will at least under some as 
    yet unknown conditions will possess.

-   It is that prescientific guesses that I wish to capture here.
    This is not an ordinary step in the scientific process.  Scientists prefer to wait until they
    have emprical or formal evidence to backup the hunches they are following as the perform the
    work itself.

-   In this case, however, the existential consequenes for humanity are so great, I feel it prudent
    to give huamnity a "head start" on serious consideration of what I believe could be a historical
    timescale quite near at hand.  

-   I believe am going to argue for properties that I believe such an AI will likely have, and 
    use that to see questions about how we move toward this dramatic event.

-   Scientists relegate speculation such as I am engaging in here to evenings with achol, and
    other collegues who will not call them out for their outlandish claims the morning after.
    Thus there is not even a serious attmept to attempt to even constrain what is likely to happen 
    next.  In all I speak with, there is a sense of,  "oh thre are so many unknowns it would be 
    pointless to even speculate"

-   I don't accept this.  I do accept what follows here is speculation, but it is considered 
    speculation, and it is based on many well understood generalize properties that seem to 
    hold quite reliably regarding the algorithms we have developed over the last 40+ years of 
    AI reserach.

-   Here are the claims that I believe have significant justification 


CLAIMS

ARCHITURE DESCRIBED IN THIS BOOK WILL "WORK"
  - any progression of K can be learned
  - will sponteneiously do it in "right" environemnt
  - "right" environment is close to raising it as a human-ish baby.


VINE/BUSH -- Human Intellignece is like a vine, Machine Intelligence to date is bush

DIFFICULTY -- measured by single hardest part.

DIFFICULTY AS VC-DIM --

WILL BE MANY OF THEM

WILL NEED TO COMMUNICATE

WILL BE MORE DIFFERENT FROM EACH OTHER THAN WE COULD EVER BE FROM EACH OTHER

MOSTLY DIFFERENTIATED BY GROWTH CAPABILITY, **NOT** STARTING SET.




CONSEQUENCES:

* How will it happen?
  - WITH A PRECPITUS JUMP.  
    Analogy to logarithm tables.  Part of speech tagging.  Protein structure prediction, and
    Speech models.

  - NOT SURE ABOUT THIS POINT:
    I recognize I am in a minority with this position, but I believe this is the 
    largest gap it the direction that most of the AI field it taking today.
     This is the largest gap in the thinking 


  - We will understand vine growing before we have sufficient soil to grow a humanity
    scale vine.  Thus, *fortunately* we will have at least a decade where most scientists
    agree that we have built all parts of a super human agent, but we just dont yet have the 
    raw processing capability for it to surpass 


   - can we grow with it?
     - Challenges of interlog
     - Meat favored-jump steps
       - will be rapidly diminished because:
          - most impt missing pieces
          - exponential increaese can "solve" w/o solving
          

* Why Todays ESTIMATES ARE SYSTEMATICALLY WRONG.
  - Why are the estimates of the AI field systematically 





SOCIETAL LEVEL QUESTIONS

* How can we stop it?

* Can we slow it down?

* How can we control it, or make it turn out ok?

* What will the first steps look like?  What will the progression look like 
  (how can we tell when we are getting close)


* Why write this incendiary stuff?  You will needlessly set back progress that will do
  great things for humanity.



AN ACTION PLAN

Basic stance:
- Stop it.  out of scope for this book.  - Slowing it down.  bad plan.
- "Managing" it
  First understand the humility we should have in using this word "manage"
  what is proposed here, is a bit like "managing" the rearing a pride of lepord cubs, some likely 
  born with genetic prediposition toward psychotic behavior...  oh yeah, and we are locked in the cage with
  these cubs with no weapons that will ultimately match them.

Yes, that is the "



UNDERSTAND IT AND ITS INHERENT PROPERTIES AS FAST AS POSSIBLE.

- Consensus understanding the certainty of the connection between Greenhouse gasses and 
  Global Warming, needed to achieve very high levels before society could be induced toward action.
  If it is inevitable that AI will work out in a way catestrophic for Humanity, the sooner we
  understand that, the sooner we can be galvanaized into definitive action

- 


But isn't all this too early?  Yes, it is a bit early.  Not because AI performs so pathetically, but
because we are on the cusp of indefinitely aggregating system, but we have yet to really build an 
interesting one.  We really cannot gather the data I am requesting, w/o these systems.  
But it takes 













---------
INTENTION TREE
  IT(a,l) is the set of goals for a with lifetime l.
  PREDICT(b, a) is a pdf over goals


DOMINANCE
  DOM a over b is E ( P( IT(a,l) in PREDICT(b, a) ) )

EXPECTED DOMINANCE
  ED(A,B)
   
RELATIVE DOMINANCE
  RD(A,B)  =  ED(A,B) / ED(A,A)
  
RELATIVE INTELLIGENCE
  
- WHAT IS THIS?

- WHY IS IT THE RIGHT MEASURE OF INTELLIGENCE FOR THIS DISCUSSION?

- WHAT THIS MEASURE CONFLAITES?
  - Observability
  - Randomness











Scary ML
