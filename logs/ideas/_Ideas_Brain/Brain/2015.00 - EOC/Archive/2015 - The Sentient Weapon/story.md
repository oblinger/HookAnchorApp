
THE FIRST

Well it wasn't really the first, indeed the levels of self understanding happened many times in fits and starts.

Further exploration is really focused on a pretty primitive example, and one whose physical embodiment is limited in the same ways that human are limited, so that the closest analogy to human consciousness could be made.

Why?


JUST WATCHING
1.	System executes animal behaviors blindly
2.	System perceives world, abstracts world, acts on world (all by instinct)
3.	System’s abstractions are adapted 
a.	Subsymbolic percepts are abstracted into relevant symbolic categories
b.	Abstractions are recursively abstracted
c.	System opportunistically instantiates complex web of abstractions and rereps of percepts
4.	Deep, complex, causal, predictive models of observed world are learned.
System can predict for example chances that bot will slip on a steep hill.
In comm chatter it hears the world ‘SLIP’ associated with the event, so that is the way it labels the event.
5.	Tieing this event to induced or built in causal models of the world it can even tie this slip event to a causal model involving coefficient of friction and gravity.
At this point system is a brilliant zombie.  It can correctly predict, plan for, and reason about the world it sees.  But it has no idea that it exists, or that any agent exists.
6.	This predictive capability is always applied to those aspects of the world itself that are a bit predictable, but not completely so… this leads to a simple model of an agent, as having percepts, and actuators, and some control program.
xxx

It is still a zombie, It cannot reason about its own reasoning, it cannot choose to rally against its instincts, indeed it does know it has instincts to be rallied against.
CLOSING THE LOOP
7.	Animal nature is commanded to execute some tasks.
Systems generalizes model of universe to include point of view,
but still has no idea that this is anything other than 6-additional parameters specifying a vantage point (it does not understand in any way that this is *its* vantage point)
8.	System correlates vantage point with GPS location for object X779.
This is the first indication of a ‘SPECIAL’ object, but still no sense of why X779 should be tied to those six parameters, rather than, say, those parameters being tied to 6 different objects.
Induced abstractions of Visual and GPS systems given oblio a symbolic model of the world, that X779 is now part of that world, but is not interestingly different from others.  Until it mounts the hill it has been watching these last weeks as luck would have it, it starts to slide down the hill.
9.	Lowlevel abstraction units recognize that a new percept pattern is occurring,
the visual field’s rate of change is higher than the wheel’s counters, but it is more than that, it is also a bit like a slow turn, left and then right, and some other patterns never seen before.  This new perceptual unit is labeled Evt1099845.
10.	These anomalous perceptual events occur from time to time, and the systems induces two patterns:  (a)  The visual field indicates a faster (and weirder) pattern of movement than the wheels indicate, and (b) it occurs at the same time that ‘SLIP’ occurs in object X779.  This is the second indication that X779 is somehow different.
11.	System builds a very nuanced model of this anomalous perceptual input, long before its symbolic model of X779 as the ‘SLIPPING’ attribute associated with it, it detects subtle patterns of misalignment of visual and GPS/Telemetric input, so it knows that a slip is imminent for X779.
12.	To the causal modeling system the temporal sequencing suggests this perceptual pattern is keying off a causal precursor to the slip event.  So oblio’s dependency network modeling the world places these particular perceptual abstractions closer to the causal source from which its agent models derive their values

At this point oblio’s causal dependency graph of the world now includes these perceptual abstractions of its sensory inputs while slipping.  These new grouping of perceptual abstractions has the following conclusions asserted about it:
a.	These abstactions are assertions available, but that it cannot communicate outwardly using any language, nor can these assertions be communicated to it by language.  These concepts are intimately tied with the gory details about exactly how its modeling subsystems abstracted the millions of perceptual inputs over the thousands movements in daily life.
b.	These abstractions could not be computed from its model of the world, its model of ‘SLIP’ or any models that it has to compute with.  They are not derivative from other knowledge it has.
c.	Indeed, for many reasons the causal network puts these abstractions closer to the causal source for properties of X779 than other knowledge it has about that agent.  Why?  Because sometime they temporally precede ‘SLIP’ assertions for X779.  Further, nuances about this abstraction stream can make much more details predictions about the specific physical result of slipping (e.g. how far will X770 slip, will ‘FUNCTIONAL’ of X779 be true at the end of ‘SLIP’)  Indeed most of its theory that applies to all agents seems to be deriving from some deeper theory connected to these abstractions, at least when it comes to agent X779.
It doesn’t know the words for these yet, but the system now has two very different ways of perceiving a slip of X779, the first is as an assertion about that state of that agent, and the second is as abstractions of the perceptual stream during a slip—the qualia of slipping.


13.	Over time the system becomes quite facile it mapping between these two levels of understanding about the world.  It is quite useful use both sources of assertions in predictive tasks, so its theory of X779 becomes quite specialized with many ways of predicting assertions in the upper theory from those in the direct theory, as well as the reverse.
14.	SEEING THAT THOUGHTS PRECEEDS ACTION.
15.	MODULATING THOUGHTS TO CONTROL ACTIONS
16.	
17.	OTHERS HAVE QUALIA TOO.  One bit of strategy it stumbled upon (pun entirely intended) is the following:
Slip sub-type 7 is associated with these kinds of visual fields, and often results in X779 being non-functional.  In cases when its gripper arm is in position to grasp an object that is as far away as an object can be and still be grasped.
reasons that X779 cannot roll like ball when arm is out
18.	The system perceives agent X4534 is slipping and has its gripper arm out.  X4534 does not roll like a ball and remains functional.  This collection of assertions is very consistent with the stumble strategy, except that ….



-  Embodied, adaptive agent with a closed loop action stragy

- finds new rep like crumbs left by society

- operating at a high level but as an idiot savant, 
- but slowly it also acreted the things it creators knew, but were so basic they forgot to tell it.


- more times than can be counted a system knew its own name, where it was, what it was for, etc.
  but these facts were not treated any differently than any others

- next level is the barest of a meta model
  It could reason about itself and even use these crude shapes to predict effective values for
  internal parameters, still done as blindly as it might optomize the hundreds of parameters on
  a pulp processing facility

