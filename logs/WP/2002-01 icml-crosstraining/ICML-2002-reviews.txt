Dear Dan Oblinger,

We regret to inform you that your paper, titled

		 "Cross Training: A Framework For Addressing Discrepancies 
Between Training And
Testing Distributions (and its application to email-based skill mining)"

has not been accepted for publication in the proceedings of ICML-2002. 
Out of the 261 submissions, only 87 were accepted or conditionally
accepted.  The reviews of your submission are appended to this message.

We and the program committee recognize that you have put a great deal
of effort into your manuscript.  We hope the suggestions and other
comments in the attached reviews prove useful to you in your subsequent
research.  In many cases the reviewers are optimistic that with
additional effort you'll have a publishable article for a subsequent
machine learning or related conference.

In case you're curious about the inner workings of ICML-2002, each
submission was assigned to three members of the program committee and
to one area chair. After your area chair received all of the
independently completed reviews, electronic discussions took place
among the reviewers.  Final decisions were not made simply based on a
weighted sum of the reviewers' ratings.  Instead, they were consensus
recommendations that emerged out of these discussions. Note that in a
few cases only two reviews are provided; in this case the area chair
has provided a comprehensive review.

We hope to see you at ICML-2002.  We have an exciting tutorial and
workshop program in addition to the conference program.

Limited scholarships will be available for Students.  Please see

		 http://www.cse.unsw.edu.au/~icml2002/scholarships.html

for instructions on how to submit an application (when the link will
become active). You do not have to have an accepted paper in the
conference to qualify for a scholarship, although first preferences
will be gven to those who do have papers in the conference or one of
the workshops.

Registration information will be available soon at the conference
website

		 http://www.cse.unsw.edu.au/~icml2002

Our annual meeting has become the primary forum for presentation of
leading-edge research on machine learning.  We are looking forward to
an exciting and productive conference.

Sincerely,

Claude Sammut
Chair, ICML-2002
icml2002@cse.unsw.edu.au

*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=
*=

First reviewer's review:

          >>> Summary of the paper <<<

This paper presents an application, mining information about employee
skills from email, and shows how imbalance between available labeled data
(from newsgroups) and the test data can be addressed systematically.



          >>> Comments <<<

Goals and Tasks. The goals of the research and the learning tasks are
clearly presented.

System Description. The description of the system seems sufficiently clear.

Claims and Evidence. The claims made are clear. The evidence presented is
limited. Given that the focus of this paper is an application and related
algorithm, testing on more than a single categorization task seems
important.

Context and Limits. The authors do a good job of placing the research in
context. They also point out some of the limitations of the work and
suggest directions for future research.

Communication. The paper is well-written and clearly organized.



          >>> Points in favour or against <<<

This paper presents an application with an interesting issue in learning.
It is well-written and seems to solve the problem well. My one major
concern is that the experimental evaluation is very limited. If this is a
possible practical application, a more extensive evaluation should be
feasible and would be highly appropriate.



 
=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--
=*=

Second reviewer's review:
THIS IS THE AREA CHAIR'S SUMMARY

          >>> Summary of the paper <<<

Although the paper is well written, I feel that the paper has some problems
as pointed by one of the revieweres. Further, running additional experiment
for a different domain does not seem feasible within the three weeks
timeframe. The paper is rated "marginal" but if we have to choose one of
"acept", "conditional accept" and "reject", we have to conclude "reject".



 
=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--
=*=

Third reviewer's review:

          >>> Summary of the paper <<<

Motivated by a problem arising with a skill mining application, the authors
formulate the more general problem of "cross training": how to deal with
discrepancies in the character of training and testing distributions. They
discuss two types of discrepancy that are relevant to their application:
(a) dilution with irrelevant cases and (b) differences in grain size; and
they suggest a way of dealing with each type. They describe the application
of the general principles to their skill mining application. Their
experimental tests involve virtual data sets that enable them to examine
the impact of dilution rate and grain size. The results generally support
the theoretical arguments presented earlier in the paper, and the authors
add a further analysis of some of the reasons for the success of their
method. Finally, they point to other types of learning problem that can be
seen as involving cross training, and they mention some directions of
future research.



          >>> Comments <<<

Goals and tasks: Well specified.

 System description: Shorter than it
would be if the paper focused solely on the application, but adequate to
get the key points across.

 Claims and Evidence: Fine.

 Context and
Limits: Very well explained.

 Communication: On the whole very well
written.

 Other Comments: A few minor points:

 a. The first paragraph of
Section 2 is somewhat redundant with parts of Section 1.

 b. Figure 1:
Missing words at end.

 c. "Dsource" and "Dtarget": Please use larger
letters for the subscripts, perhaps reducing them to "s" and "t" in the
process. The symbols are now unnecessarily hard to distinguish while
reading.

 d. 3.1.1 "This yields good results when the dilution rate is
relatively small.": Is this a generally known fact?

 e. 3.1.2 Example in
second paragraph: It would be clearer to speak of "a classifier trained to
recognize the presence of any one of a set of objects in a single frame
from a continuous video stream"

 f. 4.3, "A virtual dataset is regarded as
negative if it is composed entirely of skill-irrelevant messages ..." You
seem to be assuming that a person lacking a skill will never write a
skill-relevant message. Perhaps a comment on the basis for this assumption
would be helpful.

 g. Experimental Results: It is not made explicit why
the chance rate of predictive accuracy is 50%. Were positive and negative
cases equally numerous in the test data sets? Might it not have been better
to make the positive data sets relatively rare, as they presumably are in
the real world?



          >>> Points in favour or against <<<

In favor: Good treatments of both an important general problem and an
interesting application. All of the main criteria for a good paper are well
fulfilled.

 Against: I can't think of anything, but perhaps reviewers more
expert than me can find some flaws. In particular, I cannot judge just how
original
 and valid the solutions proposed are, relative to previous work
in machine
 learning.(I'm only on the PC because I'm familiar with this
type of application.)



 
=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--
=*=

Fourth reviewer's review:

          >>> Summary of the paper <<<

The paper proposes a framework for addressing the problem that arises when
training and test data come from different distributions, and applies it to
a knowledge-management problem.



          >>> Comments <<<

Changes in distribution between training and test data are certainly very
frequent in practice, and the problem deserves more attention than it's
received in the past. The framework the paper proposes is reasonable, but
it's also fairly obvious and doesn't solve any problems by itself - it all
depends on what goes in the "Calibration" box (Figure 2). Further, the
problem the paper focuses on - training on messages, predicting for people
- isn't really a problem of training vs. test distributions. Both levels of
granularity are in general present in both the training and test data.
Rather, inferring user skills from the skill categories of the messages
they posted is a standard learning problem. The paper denies this by saying
that merging all messages into a single bag of words would lose too much
information, but (a) that's not the only possible approach, and (b) as the
paper itself admits, the problem is not in the merging, but in that the
learning needs to be cost-sensitive, assigning a higher weight to the
"skill" messages. There are known methods for doing this, and the paper
should compare its method with such a solution. Again, "cross-training" and
imbalanced classes are different problems, but the paper confuses the two.
Now the training and test data used do come from different distributions
(postings vs. emails), but the paper only mentions this as a detail in the
implementation section, and in any case it's not clear why the kinds of
"non-skill" messages that appear in the test set can't be included in the
training set also. The paper's use of binary features instead of numeric
ones for cross-distribution robustness may be the germ of a good idea, but
again this is only presented late in the day as an ad-hoc implementation
solution.

The solution proposed for "dilution cross-training" is the use of a
"reject" option, which has been around for a long time (see Duda & Hart,
for example). The solution to the "grain size cross-training" problem is
essentially voting, which again is a standard solution for aggregation
problems.

The paper mentions concept drift as a related area, but ignores almost all
work in it (e.g., see the special issue of Machine Learning edited by
Widmer and Kubat, their work, etc.).

Figure 1 is incomplete (see last line of figure).

The knowledge-management application is an interesting one, but why not
just ask the people to provide a list of their skills, and reward them for
doing so, directly and/or when their skills get called on?

"... the list of terms ordered by information gain.": You mean the top 50? 



          >>> Points in favour or against <<<

In favour: Addresses important under-researched problem, interesting
application.

Against: Confusion in framing the problem, experimental flaws. 



 
=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--=*=--
=*=



