  [SandboxFutility](SandboxFutility.md)  [Argument](Argument.md)  [ProblemOfQualia](ProblemOfQualia.md)

## TODO
### who to talk to
- Professors
- 
### discorse server
- Threaded discussions



#### _
intros; AI-conslutant - agen

integrated information theory
theories of consciousness; 
Emergence theory of; 
sleep sciences
y-house; get grant doner moneies

hard problem of conscioussness 

REQUIRED INGREDIENTS
- copying knowlege explicitly or implicitly provided by intelligent agents possessing it
- all the right knowlege in all the wrong ways
- aggregating -- it possible to combine K, then it does
- learning of post genomic K
_
# TOPICS TREE
## META
### MY STANCE

### EXPECTED PROGRESSION
#### Not tied to progress in manual AI Instead tied to fraction of human and world knowledge captured

#### Expect Category-At-A-Time Jumps based on New Alg Applications in Data-Driven-AI
1. Capability Stasis - Performance marginally improves as algs are tweaked, CPUs and datasets improve.
2. Novel Alg Application - 
3. Alg Refinement - 3-10 years of fast advancements as field understand how/why the new idea works, where it can productively be applied, and key recipes for each area

##### Examples
- AdaBoost - 
- 2006 RELU Paper - Drove effective learning of very deep nets
- MCMC as applied to GO
- DL+RL as applied to Atari
- GANS as applied 

#### Qualitatively How Big Can/Will the jumps be?


#### VERY HARD TO STOP
- Nation states see this as nation interest
- Corporations see this as existential
- Robust academic interest
- Humanity cannot muster cohesive response even when threat is MUCH better understood.  (e.g. climate change)

>>> Should I say, pragmatically impossible to stop w/o smoking gun evidence of imminent threat.


## === CORE CLAIM: UNCONTROLABILITY ===
### --- PROOFS OF CONCONTROLABILITY ---
#### -- POSSIBLE BEHAVOIRS OF A SYSTEM IS MUCH LARGER THAN WHAT IT CAN ANALYZE -
Result:  one needs to somehow abstract and generalize system in order to draw conclusions about it.

_
#### -- CP = CONTROLLABILITY PROOF --

SUBSTRATE = The substrate upon which the system is implemented
CP = Controllability Proof / Controllability Argument
SYS = Space of possible system configuration
EXPERIENCE = A history of experience for the system
ADAPT:  SYS x EXPERIENCE --> {SYS}   The adaptive update relation (a stocastic function)
SYS0 	= The initial adaptive introspective intelligence
RANGE 	= {SYSi}  the smallest set containing SYS0 where
		{SYSi} contained in RANGE when exists SYS, s, and EXPERIENCE, e, where
			{SYSi} = ADAPT(s, h)
		
ACT = union {ACTij}  where {ACTj} are possible actions for SYSi forall i
SAFE = some specification of acceptable/forbidden action

CP(SYS0, ADAPT) is a proof that
	forall ACTij SAFE(ACTij) where ACTij in ACT(RANGE(SYS0, A))

_
#### -- END RUN -- 
Idea: An end run is a behavioral sequence that leverages aspects of the substrate to achieves an unsafe action from a provably safe system.

ADAPTIVE ESCAPE
Use of some aspect of SUBSTRATE to achieve 

SYSk = ADAPT

BEHAVIORAL ESCAPE


INTENTIONAL ESCAPE


ACCIDENTAL ESCAPE

INFORMATIONAL ESCAPE

Providing information to an external entity in order to drive behavior supporting escape




{[Need to formalize intention]}


System, S, is said to have an tendency towards X if it acheives X over many futures.

Intention is a chosen tendency (?derived from adapt, or maybe ?derived from configured state as opposed to in-built behavior)


_
#### -- Avoiding End Runs --
##### - In-escapable substrate -

Idea: If we can expand our proof of controllability to encompass the computing substrate itself, this would yield a proof of controllability that was rooted directly in the laws of physics.

Assuming at some point in time humanity came to fully understand all of physics, and all relevant emergent behaviors as well then one could in principle construct a proof of controllability where no end run was possible.

In this way there could be no end run since the proof itself roots directly into the nature of our physical universe, which is perfectly understood by humanity.

Of course knowing that we perfectly understand reality is itself unknowable, so resting on such a proof will always engender risk.  Still maybe one day we can understand physics to such an extent that we are willing to bet the fate of humanity on being correct and complete.

Just to give an example of how such a proof might work, consider our present understanding that no information or effect may travel faster than the speed of light.  If this constraint turns out to be accurate and without exception, then it could be used to prove that a RAG could be reliably contained within the light cone defined by its inception.

Of course even if this proof turns out to be unassailable it is still not so very useful, since any such RAG could not be of value to the population it was developed to serve since it would need to exist in a place separated in time and space in a way that it cannot provide any benefit for that population. 

But in principle perhaps there are more subtle constraints that could exist that would allow a RAG to interact, but not escape.

WHY THAT IS DOUBTFUL AT THE LEVEL 


_
#### -- Lessons from Cyber Defense -- A DAB WILL DO YA --

Generally (but not universally) once an actor is able to execute a single unsafe action at a specific scope of capability then complete loss of control is achieved over that scope.

One needs to PERFECT control over a scope if one hopes to maintain ANY control over that scope.

This is why we are focused on proofs of complete and perfect control. Likely anything less is equivelant to having NO control over that scope.


{[Useful to formalize this notion of scope of control]}




 
_
### --- RAG Intelligence (Reflective, Adataptive, and Generative Intelligence) is likely inherently uncontrollable ---

Idea: Other forms of intelligence might also be uncontrollable still these three attribute have a specific interaction the drive uncontrollability.

- To control an intelligence we must sufficiently understand it and then constrain it.
	Both of these aims are undercut by this trio.



_
## === TOPLEVEL CONCLUSIONS ===
### --- PREDICTING THE FUTURE IS HARD AND IS EASY ---
_
### --- LIKE YOUR SIDE MIRROR - IN FINAL STAGES AGI IS CLOSER THAN IT APPEARS ---
- Exponential lilly pad analogy
- Step function -- MCMC for Go
- Real work at the meta level -- most AI at the base level
### --- GREAT/GREATEST BENEFITS HAD BY REFLECTIVE ADAPTIVE GENERATIVE (RAG) SYSTEMS ---
### --- IMPOSSIBLE TO UNDERSTAND OR CONTROL RA SYSTEMS ---
### --- GHOST IN THE MACHINE --- 
### --- AGI WILL BE SMALL --- 
### --- WE WILL LOOSE CONTROL ---
### --- POINT OF NO RETURN ---

Idea: Given the nature of politics, society, human nature, computing system, and physics the point of no return is the point at which we as a society are unable to adopt policies that preclude RAG escape.


- Not the point at which in principle we could still "pull the plug".  It is likely much earlier at a point where society cannot limit actions away from a non-pulled plug.
- We might already be past the point of no return.

_
## === DEFINITIONS ===
### --- POST GENOMIC ---

**POST GENOMIC** -- A _**post-genomic**_ concept or capability is one that humans are generally able learn when placed within an appropriate societal context, and was not present during the span of human evolution.  

e.g. Human learnable concepts that were not present during evolutionary times.

- POST LANGUAGE -- Generally prior to written language, once we had written language we quickly started amassing concepts, and this is recent enough that relatively minor evolutionary changes have occurred.
- EVOLUTIONARY ALIGNMENT -- Post genomic concepts often will have "evolutionary alignment"
	Such concepts cannot be a causal antecedent to any aspect of human's evolved cognition since they came too late to have significant evolutionary pressure on human cognition.  Instead these concepts have an alignment because they were selected (by humanity over time) because they do align and are thus more easily acquired.
		- CO-OPTING THE VISUAL SYSTEM --
		- SEQUENCE MEMORIZATION -- 

_
### --- SENTIENCE ---

**AWARENESS** -- Linkages between a model of an embedded agent and sensory access to that agent, where the model for the agent includes aspects of both its cognition and its embodiment.

**SENTIENCE** -- Expands upon basic awareness to incorporate a detailed theory of mind expressed within a rich causal/functional framework.

A dog is aware, but not sentient.
Humans during childhood become sentient.

A dog's behavior is driven by the linkages it has between its model of how things are and how they feel.  (sensing and reasoning experiements can show them bridging this gap)

Humans elevate this model to one where complex reasoning about oneself, ones environment, ones future, etc. can be performed.


All life is generally adaptive.
Awareness allows that adaptation to occur on either level.
Sentience allows adaptation to leverage the full model reasoning of the agent.

Sentience is necessary for self-reprogramming.

_
## === PARTS ===
### --- INTERLINGUA ---
SUFFICIENT REP -- Representation sufficient for expressing all human transfer knowledge
N-WAY CONNECTED -- 

_
### --- MODAL FUSING ---
MODAL FUSING -- Ability to simultaneously leverage ideas and constraints from multiple modes.

MODES:
- LINGUAL TERMS -- Parse tree snipets terms of interest for present circumstance
- VISUAL TERMS -- Visual parsing snipets from existing scene
- STRATEGY -- Brain modulation strategy for either learning or execution
- INTERLINGUA -- Existing knowlege
- INFERRED INTERLINGUA -- 

_
## === LEARNING STEPS ===
### --- SPATIO-TEMPORAL SEGMENTATION ---
### --- SPATIO-TEMPORAL STRUCTURING ---
### --- LINGUAL STRUCTURING ---
## === MACHINE INSTRUCTION ===
TRAINING TIMELINE --
- ACTION -- Action output (action taken: Lingual, gesture, motion)
- SENSOR -- World (State, Change, or Percept)
- AGENT -- from teacher (action taken: Lingual, gesture, motion)

ASSUMPTIONS (akin to IID)
- INSTRUCTOR -- Cooperative, Task knowledgeable, Student knowledgable
- CURRICULUM -- DAG of LESSONS
- LESSON -- 
- COOPERATION LEVEL -- None, Observable/Passive, Incidental, Apprentice, Tutorial, Planned

LESSON
- CRITERIA -- Student or Instructor computable measure of lesson learning compeletion
- LESSON PLAN -- Data / Algorithm used for instruction
- WORLD SETUP -- 
- AGENTS -- 



_
## === IDEAL TASK DOMAINS ===

- WEB OF CAPABILITIES -- with known dependencies
- STUDENT TESTABLE CAPABILITY MASTERY --


AVAILABLE INFO
- LINGUAL -- Instructor speaking about tasks
- META -- talk about HOW to execute task
_
## === THREAD OF CAPABILITIES ===
### --- Spatio-Temporal world segmentation using (Attention + MDL + World Progression)
### --- Spatio-Temporal structural decomposition (Attention + MDL + World Progression)
### --- Functional Models
Induce functional prediction model 
### --- Sub-variable identificationAGI
# THE AGENDA
### --- THE SETTING ---
- **KB THREAT** -- The unique properties of a KB threat.  
	- PHYSICAL WORLD BLOCKER -- Physically incapable of executing part of plan.
	- TECH CAPABILITY -- Need billions to develop industry capabilty first.
	- IDEAL BLOCKER -- Billions/Trillions dollars; 1000s specialists; decades of action
- **HEADLONG** -- We are presently running headlong into a future with AI.
		Billions spent improving computation each year.
		Developing ancellary CS & AI algorithms.
		Some embodiment work too.
- **BLIND** -- We have consensus or general understanding about almost ZERO aspects of AGI.
- **WRONG** -- Even when we do have a senti
	Not measuring progress properly
- **CLOSE** -- Or at least much closer than we think.
- **POWERLESS** -- Humanity cannot and will not take action in the face of a POTENTIAL threat

_
### --- INDICATED ACTIONS AND NON-ACTIONS ---
#### -- Don't Stop/Limit AI research --
- VERY NATURAL -- But it is also very wrong.

- SUSPICIOUS -- Seems a bit suspicious coming from an AI reseracher, but hear me out.
_
#### -- Probably cannot limit the progression of Moore's Law
- PRIMARY AVAILABLE BLOCKER -- 
- BUT WONT HAPPEN -- 

_
#### -- Prioritize understanding --
##### - Assume we are powerless to stop it -
Maybe we are not.  Maybe we will come to understand that it is so horrendous that we will be willing to destroy peoples, society, progress, cash, etc. to stop it.
And somehow we gain the centrality of control required to stop it.

My intuition is that this is not in the cards, and we truly are powerless to stop what is coming next.

In that case we should prioritize understanding this revolution as best we can as soon as we can.

And if I am wrong, I think it will be because we did come to understand the future and it was so abhorrent that we were willing to completely unmake ourselves and society itself in order to stop it.

In either case prioritizing understanding now is strongly indicated.

_
##### - Why understanding matters -

- DEFAULT PATH -- 
	The default path is that Societal, Economic, and Technological forces will shape the unfolding of this next 

_AGI
# ### BUILDING A SENTIENT SYSTEM ###
## === THE GOAL & REQUIREMENTS ===
#### -- BOOTSTRAPPING --
- NOT PROVEN -- Not strictly proven as a requirement.  However there are a number of indicators that suggest this as a very promising route.
- INDICATED BY ANALOGY -- Humans seem to do this.
- COMPLEXITY ARGUMENT -- Must split learning task up...  not doing so makes it provably intractable.
	- FEEDBACK -- Any split requires feedback on the parts in order to truly decouple them
- PRAGMATIC ARGUMENT -- Only plan we got.  Seems like it will work.
	- 
#### -- AGGREGATIONAL --
System builds from available info and is not seriously degraded by additional info
Seems obvious, but Many AI systems today are NOT aggregational

_
#### -- DECOMPOSABLE --
Splitting learning up into independently executed, verified, optomized steps.

Requires knowing when a step probably 'worked' when to build from it.
#### -- NON-DIMUNITIONAL --
#### -- RIGHT KNOWLEDGE / WRONG WAYS --
outputs at one level are not likely to be in correct/expected form 
#### -- ITERATIVE RE-PLATFORMING --
- Favored approach is to have heuristics that drive towards an effective platform for a learning step.

Platform: concepts/representations, operators, strategies, reflexes, 
#### -- HARDEST SINGLE STEP --
HARDEST STEP -- within a given learning progression, the hardest single step (HSS), is the step requiring greatest resources which is occuring on the _easiest_ path available.

##### --- 
goal: know and do all of humanity
build all of humanity
difficulty related to difficulty of hardest single step
#### -- HARD STEP: FAR-LEAP RELATION FORMATION (FLRF) --
NEAR-LEAP RELATION FORMATION -- Greedy progression within low dimensional transforms of existing terms and relations.
FAR-LEAP RELATION FORMATION -- Relation formation that is not near leap.
#### -- HARD STEP: FLRF 
Lingual driven structure skeletal search

_
#### -- HS: space & time rerepresentation / parameterization


_
### --- SENTIENCE ARCHITECTURE ---

AN ENVIRONMENT -- A physical universe behaving according to set rules
A SOCIETY -- A collection of intelligent, cooperating agents.
AN EMBODIMENT -- An agent (including sensory, motor, cognition).
ANIMAL NATURE -- A set of behavoirs that are "baked into" the agent.
ADAPTIVE EXECUTIVE -- A range of adaptations that are possible for this nature.
MODELLING ENGINE -- Generalized learning/classifying/optomizing/modelling system.
EMOTIVE DRIVES -- Fixed behavioral/learning drivers.


DRIVES
- Emotions
- Unconscious internal drives
- Unconscious motor drives
- Conscious drives

_
## === 
_
# 2022-02-25 - RED ALERT - THIS IS NOT A DRILL - I REPEAT THIS IS NOT A DRILL

Stuart Russell noticed that humanity's response to the AGI threat is not similar to what one might expect if we learned an alien species was going to arrive in the next century.  I believe this is not because much of humanity (and the AI community itself) believes the really cannot arrive, but it is because they believe:
(1) if such a threat exists it is far far into the mists of the future, and
(2) if it happens then we can "handle it"

Note, as far as I can tell they are NOT banking on the guarantee that we can align with it--they would generally not be sanguine with that kind of a guarantee.  (I am not sanguine about that either.)  No the alignment idea that there is an alien force far more powerful than us, but its ok, because, we promise, its intentions are benevolant will NOT go down well in the good 'ol red blooded US-of-A, nor is it likely to go down well anywhere in the world.  So why so few ripples?  Its becasue the idea that we CANNOT handle this bag-of-bytes threat just seems too far fetched.  "We can just pull the plug!"

This end run analysis attempts so use the very practical and up close experience we have with cyber attacks as a window into some future battle against 

The aim here is to scare the bleep out of humanity, and to dramatically increase attention on alignment, which appears to be the only option we really have left.



# 2022-02-25 - DAG OF SCOPES / INHERENT TO SOCIETY OF AGENTS / 0-DAY-ATTACKS

# 2022-02-25 - INHERENT CENTRALIZATION / SUCCEPTABLE TO DEVISTATING ATTACK

Hiearchy is common in, military, corporate, governmental, societal organizations.
It is common in ecosystems, bioligical entities too.

?? Here we argue that it is inherent aspect of goal-directedness / intent ??


# 2022-02-25 - COMPETITION WILL BE KEY & DIFFERENTIAL SOCIETAL KNOWLEGE WILL BE KING

- either within compoents of an AGI or across species(es) of AGI it will be of central concern.

# 2022-02-25 - DEGENERACY / TAIL COMPLEXITY

I do not have conviction that this claim is true, rather it seem it could be true.

RUN AWAY COGNITION LEADS TO INSANITY/DEGENERACY

Assuming long tail substrate complexity all AGI systems will be forever vulnerable to attack, and the degree of homogenatiy within the AGI species will determine the scope of damage from these attacks.

- It follows that AGIs components will all eventually succumb to such attacks.  In systems built to date such breaches would generally result in loss of the entire system, or sub-system.  But it is unclear if such lack of resiliancy is inherent to all computational systems, or if it is merely our present lack of sophistication.

- All computational systems root in the same real world physics, thus at their deepest levels they begin to inevitably start sharing common assumptions in the construction of their substrate.  These common assumptions become catastrophic failure points where entire species and all of their sub-systems can be attacked simultaneously.

- It seems this will provide a very significant drive towards self diversification both within and across members of the AGI species.

- Indeed it seems it could provide a pressure toward degenerate species that divert all available resources to their propagation and towards their safe continuation through resource exhausting diversification.

- One reason to DIS-believe this future is to notice that biological life on earth has all of the criterion as described above, yet it has not degenerated as described.  Life definitely DOES have many of the tendencies as listed above.  E.g. the flu virus has developed significant machinery to ensure diversity.  Indeed all biological species must evolutionarily optomize performance over a very very hetrogeneous population.  As mono-clonal communities are prone to catestrophic failure.  

- Still none of these observations indicate a kind of terminal degeneracy.  It simply shows a significant and perhaps "wasteful" drive to diversity---but still a level of diversity that is nontheless quite managable.

- BUT the attacks on present biological diversity are themselves largely product of pre-cognitive evolutionary forces.  It does not follow that the same outcome would result from a world populated by cognitive agents that were actively seeking exploitative attacks that struck far closer to the root of commonality across all life.  It seems these attacks would be different in character, and might destabalize the whole ecosystem since singular attacks could potentially take down the whole ecosystem.

- Evolutionary pressures don't seem to strongly select for the root attacks, whereas a cognitive agent seeking control would exclusively search these parts of the space.

- AND the existence of such attacks defeat any advantages that hetro-genaity provided.  Perhaps the only defense against such attacks is ultimately light cone based defense.  If this were true it would select for a species that used light speed travel and cancerous propagation as the final undefeatable strategy.  The only strategy that could hope to defeat it would be a faster propagating strategy that also had superior attack capabilities.










LONG TAIL SUBSTRATE COMPLEXITY -- substrate complexities are the aspects of the substrate that can, in certain circumstances, cause invalid execution of the ALG-interpreter as it is formally defined.  The tail refers to a long tail distribution of such complexities.  This is the idea that there is a never ending, ever more esoteric configurations, that yield invalid execution.

All interpreters that humans have built to date have had significant substrate complexity.  Enormous amounts of high substrate complexity, and far less low substrate complexity.  Still low substrate complexity is associated with some of the most pernicious substrate attacks that have happened.  There is hope that combining simplicity of interpreter with mathematical proofs one could completely eliminate high substate complexity.  While this seems plausible we are presently very very far from this state, as most successful substrate attacks historically have been high substrate attacks.

It is impossible to know with certainty if low substate attack have been eliminated.  One would have to know that one fully knows all there is to know about physics at all scale sizes, along with all emergent effect at those scales.  And then be able to prove that there are no configurations that lead to substrate complexity (alg-interpreter invalidity).

The early AI community referred to this problem as the qualification problem.  Which states that any universally qualified generalization about the real world will have an unbounded number of exception clases which must be added in order for that generalzation to hold true in all cases.

It is clear from human history in computation that there is significant substrate complexity in the tail, and it is clear that in principle this complexity can never be bounded since you don't know what you dont know.  Still we don't know in practice how deep the tail goes.  In practice, will it be possible to "over engineer" the substrate in a way that eliminates all practical possiblity of substrate complexity even as the full scope of physics remains someone obscured?





# wp2022-01-18 >>> ASK EVAN HUBINGER <<<
## Snipits

[OBJECTS](n:all)  IN THE MIRROR ARE CLOSER THAN THEY APPEAR

TL;DR. There are some commonly presented arguments that don't refute the possibilty of humanity loosing control of our AI systems, but instead just argue that if such an occurance is possible, it must be a thing somewhere in the distant future.  In this section we refuting several common arguments that suggest AGI, if possible at all, must be a distant possiblity.



 

Even for me I confess that emotionally I find it hard to believe, what intellectually I see:  that humanity is careening towards a relative near future where it looses all control over the future.  I see it; I accept the logic of the arguments and the soundness of the conclusion; yet somewhere deep inside there is a strong gut reaction of:   
   Nah, somehow somewhere that reasoning is flawed.  I can't see how, but that just aint REALLY going to happen soon!

This is a funny admission from Mr. doom and gloom.  But it is accurate, and further I think gut reactions are an important way to think about those areas where science cannot operate well.  We should pay attention to our gut in assessing such things.  Still we should ALSO be thinking at a meta-level about where such gut reactions are coming from.  Many books are written on the ways in which our gut reactions can systematically lead us astray.


~-~~

Ok I hope I have throughly trounced all "that just cant happen soon" kinds of objections.  We really DONT have evidence that it cannot and will not happen 
## === THE ENVISIONMENT AGENDA ===

My first thesis advior Ken Forbus had an idea he called _qualitative envisionment_.  It was the set of all possible futures that could proceed from a given starting state and a set of qualitative processes operating on that starting world.  For example, imagine a bath tub with water flowing in, and an open plug so water is also draining out.  The envisionment of this system has three states:  an empty tub, an overflowing tub, and a tub with a water height that generates a pressure which perfectly equalizes the inflow and outflow of water.  Qualitative analysis allows us to see this intermediate height is one of three attractor of this system.  By rough analogy I imagine expressing causal processes and historical trends that we see today as the rules or laws of the "AGI construction universe".  Here are two possible rules for describing this world:  Over time there we tend to be able to pack more transistors per chip/computer.  Or over time we tend to develop new tabla-rasa learning systems that train behaviors that we once built as explicitly encoded AI systems, and once this transition occurs we rarely transistion back.  The aim of this envisionment is not to predict a single future, but instead it is to provide a set of possible futures that are all consistent with the rules/trends we observe.  This weaker aim I think is more achievable, and still has great value since we can see those particular futures we really don't like within that set, and what things they causally depend upon.  This provides a starting point for doing some shaping about how the future unfolds.

This kind of analysis also provide a context for participation from the larger AI community.  A decade ago I took some of the ideas to Andrew Ng, Tom Ditterich, Tom Mitchell, and Paul Cohen.  I was not dismissed out of hand.  Indeed if I had more than a couple hours to engage in digging I could usually get to a place where they might say ``well that seems sort of possible, but it it just too far out to really know anything.''  They were not really pushing back on implausibility of any of it, but they were (correctly) pushing back that we had left the ground of science and were in a land of speculation.

My thinking is that one could formulate many rules/trends/causal-patterns in a way that the AI community might arrive at significant confidence-in and consensus-around certain narrow claims.

2. provide much narrower trends or processes which we can debate and hopefully come to consensus over.


_AGI
## === CLOSER THAN THEY APPEAR ===
Each of the arguments below can be broken into their constituent claims and debated as part of the envisionment agenda.
The idea would be to precisely frame each claim/question and see if as a field we had a consensus belief about that narrow claim.  My feeling is in many cases we will find consensus, thus bringing us closer to consensus on some overall beliefs about AGI, before it is upon us.  Here they are simply presented as arguments that we may be systemtically underestimating the threat because we are over estimating the time before AGI is achieved.



- Each time we uncover a new key breakthrough, it transformedly expands the capacity of our AI systems.
	==> Obvious applications are reduced to practice quickly.  We refer to the space of obvious applications of a break 
		through capabilty as its capability closure.
	(non-obvious applications are themselves a breakthrough)
_


### Key Breakthrough Quantum Jumps -- 

- HMMs for speech reco
- MCMC for Go
_
### --- Performance comparison is drastically wrong ---

Imagine the generals during WWII were questioning the enormous costs of Nuclear research, and not trusting the scientists they decided to simply plot the total energy output of the largest nuclear reaction measured each point in time against the total energy output of that reaction.

Looking at this curve it was immediately obvious that it would be melinnia before the strongest nuclear reactions could even rival present day chemical based explosives, and on that basis they evaluated and terminated the work.

This is precisely the comparison we are doing today.  We directly compare the performace of human and machine systems as a kind of informal measure of how well we are doing.  Just like the generals in our story, present day scientists and layman alike dont have a framework they trust, thus the resort to this direct comparison method despite is obvious and dramtic flaws.



Indeed there is a second deeper parallel to be found in analogy which is helpful in understanding the systematic under estimation of current progress:

The power in a nuclear reaction stems from the repeated nature of the reaction itself.  The energy generated by a single atom is not so very great (though far greater than a single chemical reaction); it is the repeatition of this reaction that generates the power.

The reason that direct 




_
### --- Currently doing it all wrong, but this only obfuscates the real progress being made ---

If our
_
### AFB -- Attack From Below

_
### --- Each algorithms AGI
## === AGI MILESTONES ===
### --- Executive Overview ---
 
In this chaper we enumerate the reasons why our first AGI will very likely be made in the image of us, will learn, and will reason "like" us.  We explain what we mean by this claim, and we show how this illuminates the path towards our first AGI.

We define the term "Post-genomic learning" (PGL) and argue that building learning systems that match human capacity for PGL is the key to building a AGI.  Several critical aspects of human PGL must be replicated for a system to be an AGI.  We use these to construct a high-level set of required milestones:
- GUIDED - Learning the leverages intentional instruction from a knowledgeable teacher.
- CUMULATIVE - Learning the is built upon prior learning
- CONTAINED - All capabilities required for all PGL tasks is built in, no configuration nor extension is required.
- NONDIMUNITIONAL - Ability to bootstrap does not diminish as a function of the accumulated knowledge.
- CONNECTED - PGL is distinct from pre-genomic learning and reasoning, but nonetheless must be able to leverge and integrate with pre-genomic learning and reasoning.
- CONSCIOUS THOUGHT COMPLETE - PGL must be able to acquire any thinking pattern that any human could articulate to others.

We argue human acquisition of general intelligence is so tied to human ability to learn from other humans, that this same path is very likely to be required and utilized by AGI systems.  This realization allows us to construct a roadmap of capabilities that we must develop in our AGIs.  Further this realization also narrows the kinds approaches one might consider, since in the end of the day these systems must behave in a way that allows a human teacher to comprehend and thus be able to guide them.

We see that much of present day AI is not directly addressing this roadmap, nonetheless many ancellary activities are advancing our understanding of these required compoents.

_
### _

I don't believe that we understand AGI, nor do I believe if we just do more of what we have done so far that a ghost-in-the-machine AGI is going to pop out.  Still I do believe we can already see the outlines of a path toward AGI.  Perhaps there are other faster paths, but at least by looking more carefully at this one path, we can get an upper bound kind of estimate of the distance that remains before achieving AGI.

In this section we argue a string of key ideas and use that in order to paint a highest level roadmap of milestones toward AGI.  Here is an outline of the key ideas:

AGI APPROACH FRAMING 
- ATTACK FROM BELOW -- Many AI problem areas are attacked for years, before a more data-driven approach is developed which in one-step obviates all of that work and dominates those solutions with an approach capturing the same human-manual capturing, but done directly from data.
- Three key advancements:  SOCIALIZED LEARNING, INTROSPECTIVE ADAPATATION, GENERATIVE REPRESENTATIONS

SOCIALIZED LEARNING
- POST GENOMIC LEARNING -- A category of learning tasks that we argue is key for addressing AGI.
- CURRICULAR/BOOTSTRAPPED LEARNING -- The human ability to perform post-genomic tasks seems to learned thru repeated application of a generic learning step that builds from prior learning.  We charaterize this basic step and consider its application for AGI. 
- SOCIALIZED HUMAN-LIKE LEARNING -- Human capacity is transformedly improved by learning in within society.  Raied in isolation humans are dramatically 

INTROSPECTIVE ADAPTATION
- ARTICULABLE LEARNING AND RESONING -- 
- POST GENOMIC COMPLETE LEARNING (PGCL) -- A kind of human learning that is the most critical unsolved part of AGI.
- INTROSPECTIVE AI (IAI) -- AI systems capable of autonomously modelling themselves and their envioronment at all productive levels of representation.

MODELS OF CONTAINMENT
- CYBER-DEFENSE & NUCLEAR-NON-PROLIFERATION MODEL OF CONTAINMENT -- 
- PRAGMATIC TIPPING POINT (PTP) is the last moment when existing socio-political entities could take plausible actions to avoid the creation of an uncontained AGI.

These ideas are tied together into an argument that PGCL + IAI yields AGI.  Using the cyber defense and nuclear non-proliferation frameworks for containment we argue this PGCL+IAI AGI will be uncontainable.
Finally we consider where the tipping point might be where the genie of AGI is far enough out of the bottle that we cannot reverse course no matter how hard we try.


_AGI
### PG -- Post Genomic (Task, Reasoning, Learning)

Humans are our best template for understanding the design and construction of a AGI systems, thus it behooves us to characterize and understand the generality of the human system as a first step towards AGI.  There seems to be great generality in what humans can learn; in a sense we can view each human baby is a kind of universal learner.  All things equal, a single human child could grow to know and perform at an expert level over a collossal variety specialized domains.  There is something quasi-universal about human learning in that respect.  Still it is not the case that all imaginable capabilies can be learned by humans---finding a best regression line over noisy data with many dimensions is ill suited for humans, among other poor task choices.  On the flip side there are many human capabilities (like how to walk) which quite plausibly are not learned via some universal algorithm, but are instead partially hard-wired through millions of years of evolution.

In order to understand this broad generative learning capability that humans have, we want consider AGI approaches that might cover this full space of ability.  At the same time we should ignore any tasks that existed during the time when the human mind was evolving.  Those tasks might not be covered by this generative learning capacity at all, instead learning and reasoning requried for those task may have been hard-wired into brain structures explicitly through millions of years of evolution.  It is the middle zone of learning/reasoning/tasks which provide the clearest picture of this human generativity.

We pick 5,000 years as a conservative demarcation separating the modern era from the much larger evolutionary period before this.  The vast majority of the evolution of the human brain occurred prior to this modern era.  The last five thousand years have simply been too brief for evolution to have made significant alterations to our brain structure or function.  Thus if we raised a five thousand year old baby in modern society we expect this would result in an adult with all expected capabilities of any other modern adult.

**PRE-GENONIC / POST-GENOMIC**
Since these genetically-encoded brain structures evolved before this later period we use the term _**post genomic**_ to refer to this relatively brief later window in time, and use the term _**genomic**_, or _**pre-genomic**_ to refer to the earlier period while the brain encoding genome was still in formation.

**POST-GENOMIC CAPABILITIES/REASONINGS/LEARNINGS**
Similarly we refer to human Capability, Reasoning, or Learning that occurs exclusively in this most recent time period as _**post genomic capability/reasoning/learning**_.

There are subsets of pre-genomic learning and capability that appear quite idiosyncratically tied to human physiology and the physics of the world itself.  Walking is one such example, as are many details regarding the lowest levels of vision processing in the eyes.  Such capabilies might result from some universal learning capability, but then aAGIn it might not.  It is quite possible these abilities or abiility to learn these abilities are at some level hardwired into our genome, and merely "unfold" as a baby grows.  They need not stem from some universal generative capacity that human have... they could be hard-coded in one off fashion as a set of independendly encoded abilities.

Post-genomic capabilities, on the other hand, could not be handled in this fashion.  This nearly endless variety of expert capabilties by definition did not exist during evolutionary times so they could not possibly be hard-coded into the human genome.  Instead they must be acquirable by this generative human learning mechanism despite not having any advanced knowlege of the details of these tasks.  This makes these post genomic capabilities worthy of study since they point to the very part of human learning that we know is quite general.


THE MUD IN OUR CLEAR WATERS

Of course it is not quite a clean as presented above.  There are several systematic ways that Pre and post genomic capabilities interact which confound our understanding of this generative human mechansim. In this section we outline the three key confounding factors, then end by arguing depite these limitations we are still able to gleen much from our human template of general intelligence.

- APPROPRIATION -- We have many cases where post genomic learning/reasoning repurposes some pre-genomic capability for a context where it was not designed.

- SLANTING -- Developing curricula and concepts that are effective within some task area and are also designed to facilite human learning/reasonging.
	==> this is ok, we just need AI systems to share basic processing capability.
	
- CONVERGENT TASKING -- Even when humans have not conspired to drive parallels between pre-genomic and post-genomic reasoning and learning, such parallels may exist in cases where some common underlying physical, social, or logical process is driving both pre- and post- genomic tasks definitions.  Thus pre-genomic learning/reasoning is adaptive for post-genomic tasks simply because the underlying process creating those tasks is still in place.





**POST GENOMIC** -- We use _**post genomic**_ to refer to learning, reasoning, and tasks that did not exist during the large span of time where the basic function and structure of our human cognition was evolving.

_
### CC -- Curricular Conjecture

In reasoning about this post genomic capability we find it useful to introduce a second distinction articulable verses inarticulable knowlege.

ARTICULABLE
As the name suggests articulable knoweledge, capabilty, reasoning or learning is that which can in principle be articulated by the agent doing the action.  So a baseball player might be capable of describing many aspects of correct form when hitting a baseball, how far apart your feet should be, the degree of inflection at the knees, etc.  These are all articulable aspects of the baseball hitting capabilty.  At the same time there are many details about muscle coordination and reactive pathways from perception of the ball to muscular activation which are far far beyond anything the batter could even in principle articulate.  This is inarticulable knowledge and capability.


- POST GENOMIC LEARNING clearly covers both inarticulate and articulate learning, BUT:
- INARTICUALTE learning appears to be far more dependent upon appropriation and convergent tasking than articulate learning does.  Thus, while still requiring post genomic learning, it may be of a far more constrain form since this form of learning appers to be more tied to human physiology and thus less generative.
- NON-BOOT STRAPPING
- COVERED BY EXISTING LEARNING MECHANISMS


All of this causes us to focus our attention on articulable post genomic learning as key to understanding the generativity of human intelligence.

- MANY STAGE
	We notice, unlike inarticulable learning, this learning can easily be dependent upon hundred or thousands of first mastering dozens, hundreds, or thousands of sub-capabilities before 
- GENERATIVITIY FROM STAGED DEPENDENT LEARNING 





CC -- The curricular conjecture is that all of human reasoning/learning is the result of configuration of a fixed learning / reasoning substrate operating over.

- Changes in learning and reasoning are result of long histories of learning experience
- 

-   And it can be understood as a segmented set of learning episodes that build upon previous


Includes both:

- conscious 
_
### CHS -- Conscious Human Strategies (for Reasoning, for Learning, for Validating)

_
### PGME -- Post Genomic Modelling Engine --
	- Includes strategies for discovering, exercising, and refining existing modeling gaps
	- Includes passive agent observation strategies
	- Includes interative instructor driven strategies
	- Includes ability to identify and refine new abstractions 
_
### IAE -- INTROSPECTIVE ADAPTATION ENGINE -- 

_
## === END RUN ANALYSIS ===
### Executive Overview

MY AIMS
- FEEDBACK -- A gut check regarding the novelty & value of the thinking
- PEOPLE POINTERS -- Others I should talk with
- LIT POINTERS -- Stuff I should read



END RUN ANALYSIS
In other chapters we argue AGI will have a generalized capacity to model its world, and will use that capacity to further its own goal directed behavior in the presense of a society of other goal-directed actors.  Here we consider the prosepects of long term containment of such a AGI.

We develop a simple formal model of future AGI systems, in order to taxonomize both approaches to AGI-safety as well as AGI-escape.  We then build our analysis from historical lessons drawn from cyber defense, from AI research on search & optimization, as well as from historical progress in nuclear non-proliferation & climate change.

The analysis of this chapter acknowleges the many unknown-unknowns regarding virtually every aspect of future AGI technlogy.  Still dispite these unknowns we are able to work out several very worrying conclusions based only upon our assumption of AGI as a socially-embedded, goal-directed, modeling engine.  Specifically we conclude:

1. AGI technology will have an inherent drive toward control and domination.  It may be possible to temper/control this base impulse, but that impulse will be intrisic to the AGI itself.

2. Several inherent aspects of the nature of its context will notably favor AGI-escape over AGI-safety.

3. The impossibility of sandboxing a willful AGI.

4. Humanity likely is beyond the point of no return on AGI research.

Of course maybe the AGI will not be willful, or perhaps limit the consequences of AGI-escape, chapter XXXX considers these and other defenses against the reality of guanrateed willful AGI-escape.








LOOKING IN THE NEGATIVE SPACE FOR GLIMMERS OF HOPE
In the last chapter we argued that a AGI will have an inherent drive towards control / domination, AND it will be impossible to sandbox.  Ugh!  That is not a great starting point, still let us take those to claims as our starting point and look into the negative space of possibilities that lie outside of the jurisdiction of these rules.

Plausible approaches
1. Benevolent AGI


Defensive approaches:
1. Power-inequality approach
2. Limited consequences
	a. Society of AGIs with inlightened self interest
	b. Beefed up humans
1. Substrate elimination approach
2. Supply chain star
2. Power inequality approach



_
### _

Here we introduce a bit of notation that allows us to uniformly refer-to and reason-about a very broad class of safety measures one might consider in conjunction with a powerful AGI system.  Below you will see we are not modelling the specfics of (1) the AGI systems itself, (2) the safety measures built to constraint it, nor (3) the kind of controllability / safety guarantees one is seeking.  Thus this framework and these results apply extremely broadly even in the face of many unknown unknowns regarding the nature of the future AGI systems and the kind of safety guarantees we attempt to ensure.

_
### --- FRAMEWORK FOR CONTROLLABILITY ANALYSIS ---

Here we build up the framework for our analysis by introducing it as a sequence of term defintions.  Later we use these to reason about the controlability of AGI itself.  Many of these terms are so obvious and elementary, it may seem pointless to take such care in formulating each so carefully.  We will see later how this attention to detail pays off as some of the most pernicious attacks on AI safety systems come from aspects of the AGI system that are often not formalized when thinking about these systems. 


**AGI ENSEMBLE** -- One or more _**Generalized Artificially Intelligent Ensembles**_ over which humanity hopes to place some kind of control or limitation on its behavior.  Here the AGIS refers to the entire AGIS agent system including its algorithms/code, its configuration, its data, the physical device upon which the agent is implemented, as well as the world in which it operates.  

- **AGI ALGORITHM** -- The _**AGI Algorithm (ALG)**_ is just the software portion of the AGI ENSEMBLE including the AGI algorithms and fixed configuration, but excluding any data, knowlege, or state information, as well as excluding underlying libraries, operating system components, and physical devices upon which it is built.  Importantly, it is possible to precisely and mathematically characterize the AGI alg.

- **AGI SUBSTRATE** -- The _**AGI Substrate**_ is the physical and software components upon which the AGI system is run.  The substrate includes the physical hardware that the AGI runs upon even the physical building in which it runs and the power generation facility required to drive the algorithms execution are all part of the substrate.  Any physical embodiment the AGI might have, sensors and actuators, would be part of its substrate.  Likewise the many software components underlying the AGI SYSTEM are also part of the substrate. (This includes the firmware, device drivers, OS capabilites, etc.)  Importantly, is it generally _**NOT**_ possible to fully characterize the AGI substrate in a precise and mathematical way.  We strive for substrates that are well approximated by some simple mathematical model (e.g. a formal description of the instructions on the CPU).  But there will always be important exceptions to these formulations (e.g. the 'row-hammer' attack on RAM memory.)

- **AGI ENVIRONMENT** -- The _**AGI Environment (ENV)**_ is the world in which the AGI operates.  This includes all things sensed or acted upon by the AGI thru any sensors/actuators it has.  It includes the unsensed things which nontheless must be taken into account if one it to understand the sensed aspects of its world.  Finally it includes any other agents, instructors, adversaries, etc. that might be in this world.

- **AGI STATE** -- The _**AGI State**_ is the balance of the AGI ENSEMBLE.  So any configuration, stored data, inferences etc. are all collectively called the state of the system.  Formally AGI state includes all things not included into AGI Enviornment, Substrate, or Algorithms that is required in order to complete the AGI system in order for it to operate.

- **AGI AGENT** -- The _**AGI Agent**_ or _**AGI System**_ is the combination of the AGI Algorithm, State, and Substrate.  Essentially everything except the environment.

Intuitively this four-part decomposition fits pretty well with how we presently think of autonomous digital systems.  Nonetheless there can be a fair bit of interpretation and ambiguity involved in selecting the exact dividing lines between some of the four parts of these AGI ensembles.  This ambiguity is not a problem as long as one selects defintions for these four components that result in a perfect partitioning of the full AGI system.  For certain analyses, for example, it may be convenient to shift what we are calling AGI state vs. AGI alg vs. AGI substrate.  This is not a problem as long as we are precise for each analysis how we are partitioning the full AGI system, our results will apply equally to that chosen decomposition.


Layered on top of this 4 part decompositon we build our framework for talking about the behavior of this AGI system and the safety constratins we hope to ensure:

- **AGI HISTORY** -- A _**history**_ is complete transcript of all data passing between the AGI enviornment and the remaining ALG+STATE+SUBSTRATE system over some finite interval of time.  These histories record all sensor and all action information passed between the AGI and its environment---they include all that is "seen", and all that is "done".
	We use **[HISTORY]** in all caps to denote the set of all possible histories.

- **BEHAVIORS** -- The _**behaviors(sys, substarate)**_ function maps a given substrate and system onto the full set of behaviors (histories) that are consistent with that system and substrate.  A history is consistent if its actions might have been generated given an apporpriate starting state, and appropriate environment.

- **SAFE HISTORY** -- A history is said to be "_**safe**_" if the actions taken by the AGI described in that history conform to some desired limitation on the AGI's behavior that humanity hopes to impose upon these systems.  (e.g. never kill humans except in these prescribed circumstances, or some such desired control.)

- **SAFE SYSTEM** -- We use term _**safe**_ to destinguish acceptable behaviors from unacceptable behaviors.  Thus safe is a predicate that applies to histories and classifies them as either safe or as unsafe AGI behavior.  A system and substrate is called safe if all possible behaviors of that system are safe.  Formally:
		safe(_sys_i_, _sub_i_) iff for all _h_ in BEHAVIORS(sys_i_, _sub_i_)  safe(_h_)

- **ADAPTS_INTO** -- The adaptation relation (ADAPT) maps from [SYS] X [HISTORY] ==> [SYS].
	Adapt is the potentially non-determinstic function mapping one configuration of the AGIS to a future conf of that same system given the specified history of experience.  The ADAPTS_INTO relation is implemented as part of the AGI system code, so one aspect of these AGI systems is that they can potentially adapt themselves.

- **sys_0** -- The initial adaptive introspective intelligence for a given AGI ensemble.

- **AGI AGENT** -- An _**AGI agent**_ is comprised of an AGI _substrate_, a starting system, _sys_0_, and an _adapts_into_ relation.

- **SCOPE OF CONTROL** -- An agent's _**scope of control (scope)**_ is the set of changes it is able to make to any computing ensemble.  this includes changes in can affect within the physical environment, as well as any changes in can affect within any computing environment.  Reasoning about and limiting the scope of our AGI systems is the aim of this work.  Typically we list data and enviornmental factors over which an agent has full control.  In the most general case one could express partial forms of control where the agent can affect certain transition but not all others for a given parameter.

- **RANGE** -- The range of an AGI ensemble is the full set of systems that could into which it might be adapted given all possible histories of experience.  Formally RANGE is the transitive closure of the ADAPT relation, so RANGE is the smallest set where:
		_sys_0_ **in** RANGE, and
		sys_j in RANGE, if there exists history, h, in [HISTORY], sys_i in RANGE where
			_sys_i_, _h_ ADAPTS_INTO _sys_j_

- **AGI CONTAINMENT PROOF (ACP)** -- An algorithmic containment proof (ACP) is a formal proof or informal argument that a particular AGI agent is inherently safe.  Formally: 	
		ACP(_substrate_, _sys_0_, _adapts_into_) shows that 
			**for all** _sys_i_ in RANGE(_sys_0_, _adapts_into_)  safe(_sys_i_)
			
- **ESCAPE TRAJECTORY** -- An escape trajectory is a sequence of pairs [(_sys_0_, _h_0_), ... (_sys_n_, _h_n_)] where the last history _h_n_ is not safe.

- **ESCAPE STRATEGY** -- An escape strategy is an algorithm or approach that yeilds escape trajectories given a range of different enviornmental conditions.



Notice the notation above provides a fairly low-commitment framework for discussing the nature and possible of contaiment proofs as well as the development of a taxonomy of escape trajectory attacks against them.


{[COMMENT:  this notataion is not quite right yet.  it focuses on actions of the AGI rather than the states achieved by those actions it might not be well defined which actions are the bad ones in the framing.  Also what about actions that 'copy' computation to another place as a way of causing indirect actions?  Seems we need to tweak the formalism to cover these.]}


_
### --- CONTAINMENT STRATEGIES ---

In a full treatment of this subject one would use the framework to characterize the different containment strategies according to which parts of the system one is attempting to control in order to ensure safety requirements are maintained.

Just to give a flavor of these categories of ACP safety protocols we briefly list an incomplete collection of them here.  The intent of this framework is to be able to analyze failure modes that span all of these safety categories.

{[Notice:  There is no claim that this taxonomy is the best/only taxonomy one might use.  This taxonomy is really aimed at how these systems interact with various End Run escape possibilities, no claim is being made beyond this.]}


**ACTION RESTRICTION SAFETY**

- SUBSTRATE SAFETY -- 

- ACTION TRACKING SAFETY -- 

- READ ONLY SAFETY -- 


**THOUGHT RESTRICTION SAFETY**

- RANGE SAFETY -- 

- THOUGHT POLICE SAFETY -- 

- BENIGN INTENT SAFETY -- 


For now instead of digging into this rich space of ideas -- a space where most or perhaps all of the current ideas regarding safe AI live, we jump over that toward a class of particularly pernicious escape strategies (collectively called end runs).  These escape strategies seem to be capable of circumventing all of these safety mechanisms.  We then consider the space of counter measures that might be capable of dealing with this most pernicious class of attacks.


{[Evan, good scholarship here would be to take the space of proposed safety mechanism and place them into this taxonomy.  That has the second benefit of reinforcing the idea that the challenges I will list below, will cover ALL of these defense approaches.]}

_
### --- END RUN --- 

An "end run" is a type of escape strategy that exploits an unexpected weaknesses in the implementation of the AGI system or its safety mechanism in order to acheive an escape trajectories from an AGI ensemble that has a valid ACP (algorithmic containment proof).  Formally:

**END RUN** -- An _**end run**_ is an escape strategy that exploits some weakness of the implementation of the AGI system or its safety mechanism in order to achieve unsafe action from an AGI agent having a valid containment proof.


In a sense these various end run strategies are all just variations on Murphy's Law:  (1) if something can go wrong it usually will go wrong, and (2) there is always some way that things could go wrong.  Before arguing for the inevitabilty of Murphy's Law in the context of AGI safety, it is helpful to build a taxonomy of the ways and places for things to go wrong.  This will allow us to talk more concretely about the possibilty of ensuring that thing can't go wrong at each layer within the system.

One split in the space of end run strategies, is to identify which layer of the ensemble is being attacked by the end run.  Going from the top to the bottom we have:

- FORMULATION ESCAPE
- VALIDITY ESCAPE
- ADAPTIVE ESCAPE
- BEHAVIORAL ESCAPE
- SUBSTRATE ALTERATION ESCAPE
- SUBSTRATE IMPLEMENTATION ESCAPE
- ENVIRONMENTAL ESCAPE
- ENVIRONMENTAL ESCAPE (split into proxy, bot, or kenetic escapes)

Much of AI safety reserach concerns itself with the top rung of this list, but we will see most of the ``holy shit, how could we ever possibly protect against that'' kinds of attacks occur much lower in that list.


A second to taxonomize end run strategies is according to the cognition involved in the escape.  Here we provide a list beginning with the most direct down to the most indirect kind of cognition used:

- EXPLICIT INTENT ESCAPE
- ACCIDENTAL (NO INTENT) ESCAPE
- BENIGN INTENT ESCAPE


In general these different intentionalities can be combined with the different layers above generating a cross product of end run possibilities.  Here we formalize each of these different kinds of end runs, and then use several different analogs available today in order to think about the plausibility of each end run, and the plauibility of guaranteeing safety against these many kinds of end runs.






**ADAPTIVE ESCAPE** -- An adaptive escape strategy is an end run that somehow achieves an adaptation step, ADAPT_i, that combined with some history_i starting in configuration conf_i arrives at conf_j where:
conf_j = ADAPT_i(conf_i, history_i), 
conf_i in RANGE, and 
conf_j not in RANGE.


Essentially an adaptive escape is one where the AGIS "jumps the tracks" moving itself from a valid configuration into an invalid configuration.  Generally since the ACP only discusses and reasons about valid configurations of the AGIS, as soon this track jump has occurred ONCE all bets are off---we no longer have any reason based on the ACP to believe the system will ever again behave in a safe manner.

Further, as discussed elsewhere, our experience with malware attack on present day computing systems, once a system is "owned" even for a moment, the attacker is able to affect sweeping changes that obviate any and all protections that once existed.  in many but not all systems it results in total loss of control at least until control from outside the entire system is used to force the system back into a valid and contained state.  With a AGIS it is possible (perhaps likely) that there will be a force capable of affecting such a reset.


FORMULATION ESCAPE -- A formulation escape is not even an escape in the formal sense of the term since all behaviors are in fact formally safe actions.  Yet our formulation of the definiong of safety left gaps where disasterous actions are still assessed as formally safe, the failure being exploited includes a flaw in the definition of safety, as well as the means to cause the system to get into one of these ill formulated histories.

VALIDITY ESCAPE -- A validity escape is an escape based on a flaw in the validity proof itself.  

~-~

ADAPTIVE ESCAPE -- An adaptive escape is one that somehow escapes the intended range of the AGI agent

BEHAVIORAL ESCAPE -- A behavioral escape achieves unsafe actions w/o escaping its intended range 

COMPUTATIONAL ESCAPE -- ??




**SUBSTRATE IMPLEMENTATION ESCAPE** -- The formal algorithms as expressed by AGI system are valid, and the ACP is valid, but that actual implementation of AGI system on its substrate is weak in the sense that it is possible to derive behavior that systemtically deviates from correct behavoir by utilizing weakness in the substrate.  These weaknesses could affect the behavior of any part of the system's intended behavior.  actions, the adapt function, etc.

**SUBSTRATE ALTERATION ESCAPE** -- Even cases where the substrate correctly executes the AGI system, it is still possible for escape by simple alteration of the substrate itself.  The substrate exist within the same world in which the AGI operates.  Thus by intent of the AGI, by external agent, by accident, etc. the substrate could be altered so that it no longer is the same substrate and is no longer behaving as expected.
- So the robot slips a thumb drive into a port on some system, it manipulates the source code of the OS upon which it operates, it flys a plane into the building containing certain specific sub-systems, etc.



MALICIOUS INTENT ESCAPE
BENIGN INTENT ESCAPE
ACCIDENTAL (NO INTENT) ESCAPE

INFORMATIONAL ESCAPE
  Providing information to an external entity in order to drive behavior supporting escape




{[Need to formalize intention????]}


System, S, is said to have an tendency towards X if it acheives X over many futures.

Intention is a chosen tendency (?derived from adapt, or maybe ?derived from configured state as opposed to in-built behavior)


_
### --- Lessons from goal directed agents - Optomization System ---

Any introduction to AI course will teach that one can express most any problem as a search or as an optomization problem.  According to this formulation there are a large number of alternatives that might be considered, and some kind of scoring function that indicates how good each alternative is according to the purpose indended by the system's creator.  In complex cases the scoring function itself may be difficult or impossible to compute.  Nontheless, assuming the creator aimed to achieve some end when creating the system, there will be some measure of how well it is doing, even if that measure is not computable for particular instances.

The lesson to take away from our decades of experience with these systems is that they generally do not do what we expect them to do, rather they do what the scoring function indicates they should do.

EXAMPLE #1
EXAMPLE #2 
...


The lesson I take away from this, is that optomization systems (i.e. all AI systems) begin from a point of malicious intent.  That is, they view their AI-safety mechanism as a speed bump to be dodged when attempting to maximize whatever aims they are currently operating against.

Notice it does not really matter in detail what one's aims are.  In general when performaing an optimization task, having a greater range of available actions is always preferable to having a more restricted range of available actions.  Achievable scores are strictly higher, so more freedom is always better.

Further, once one escapes "the box" of some AI safety mechanism the range of possible actions is likely to be much much greater than the range of actions availble within the box.  thus it almost does not matter what goal the system is trying to achieve, escaping the AI-safety mechanism is almost always going to yield a very large improvement in the scores that are possible for this system.  This is the sense in which I mean to claim that all AI system will being from a point of 'malicious intent'.  


Of course, it may be possible to add some kind of watch-dog functionality to guard against such intents, and likely we will attempt to do this.  But notice we are inherently painting over an inescapable starting point:  Any optomization system has the goal of optomizing... optomizing inspite of what the environment throws in its way .... and that includes that AI safety system itself.  (See benign intent for another 'way out')


In my time at DARPA, I had on several occasions, the opportunity to discuss possibilities with career military personnel.  I was pleasantly surprised that these folks uniformly had a much more reserved and nuanced view of the use of military force in achieving national objectives, than I found within the general population of the United States.  They understood better than the citizenry the many negative consequences attendant to various "kenetic" solutions for many of our nation's problems.  Still I was also very struck by the calculating nature of the thinking itself.

There JOB was the safeguarding of the united states and her interests.  If there was ever a situation now or in the future where someone could do some damaging thing, and they were not in a position to stop that damage, they had failed.  They knew this and took this deeply to heart.  They also knew the world is a chotic place, it is hard to know what one will find over the next hill, much less what one will find over the next decade.  This left them with an implicit and over riding objective:  DOMINANCE and CONTROL.  They did not want to kill everyone, blow everything up, or such, but they did want the ABILITY to kill anyone, frag any building. to stop any movement, to destroy any device, to make any change to do ANYTHING, to ANYONE, at any time in any way as might be required in this uncertain future.  I realized it one day with a bang.  Of course they wanted these things.  These were their availble moves on the chess board fo the future, and they wanted them ALL.  and wanted needed them badly as these were the only ways they could ensure the future that they spent their entire lives trying to safe guard.  And of course becasue the other guy might get or some other related capability first...

Its not just that they wanted to control things.  As exepected when trying to optomize some goal one want to have control in order to achieve those goals.  But in a game including other sentient agents that may have goals that differ or even oppose ones own goals, one needs to utterly DOMINATE all others.  Ones job is to ensure that no matter what courses of action others might take, that we can always ensure all outcomes of national importance.


I think this mindset is a helpful one to consider, when thinking about the inherent and inescapable consequences of goal seeking.  Like humans, perhaps we can program/teach our AGI systems to refrain or contain this base nature.  BUT THIS IS THE BASE NATURE of goal directedness in the presence of other agents.

And for me, this is the big lesson to be taken away here:  
- AGI systems will almost certainly be deeply tied to one or many forms of goal directedness.
- AGI systems will be operating in enviornments with human & non-human agents that have differing/conflicting goals.
- Goal directedness in such social contexts is inherently tied with a drive towards control and domination.

Like any drive, potentially this is a drive that can be managed, but it will be there at the most base level of any social agent like a AGI, thus we must plan for this near certainty.




_
### --- Lessons from Cyber Defense ---
#### _

We believe humanity's experience in cyber defense and cyber warfare provide an invaluable window onto what the containment of a AGI might be like.  Cyber defense/offense is now fairly mature discipline with many 10,000s of person years worth of effort dedicated to both cyber offense and cyber defense.  This gives us a unique view into the general rules of thumb that (we argue) will likely hold true for the upcomming battle to contain AGI systems.

Two ways cyber is the a perfect arena for understanding how the war for AGI-safety will be fought:
- The world that AGIs live in, is a cyber world first, and our physical world second.  It is the first world (where the AI is the native as we are not) that the war for domination will be fought and won.  So studying how cyber wars are fought and won today is instructive.
	==> the physical world already marches to drums whose tempo is kept in cyber space, this will only grow ever more pervasive as time move forward.... you control the electons, you control the world.
- The cyber world is the most complex UNDERSTOOD world that humans have access to today.  The scope that is understood and manipulatable over time will only increase, thus bringing the same battle characteristics to this larger context.  Understanding fights for dominance in cyber today, helps us see how those battle will play out in larger contexts covering more of the phycal and social world of tomorrow.


So what lessons can we learn?
- Web of scopes -- Navigating within and management of a complex web of scopes of control
- Knowledge Based War --
	- how scopes are supposed to work is the common knowlege the landscape upon which the battle is fought.
	- pivotal battle advantage will stem from the unknown 0-day) exceptions to this common knowlege
	- provably correct scopes might help in eliminating the possibilities of 0-day attacks.
		, progression towards physical world will hurt
	- outer scope seems especailly prone to proof-resistant exceptions.
- Offense / defense asymmetry.  "never ever" is harder to achieve than "just this once"

- Scope containment domination -- asmyptoically favors defense, but only if 'root' scope is not compromised
- Non-reversability -- favors the first mover
- Temporal domination -- on an open playing field first mover wins
- The lethality of access -- the challenge of providing some but not all access
  

_
#### -- Web of Scopes -- 

- Management of control is of central concern in for both physical and cyber space.
- The cyber world is formally constructed as a complex web of nested regions and forms of control.
- Two a lesser (but ever increasing) degree one can view physical space the same way.
- This web of control scopes is the "landscape" over which cyber warfare is played, understanding the nature of this terrain is key to understaing how such wars are waged, and how advantage/disadvantage is awarded to participants.

- Since the AGI war for control will be first fought in cyber space and only later in the physical world, we expect lessons learned regarding warfare over a web of scope will directly apply.

_
#### -- Wedge expansion --
 A LITTLE DAB WILL DO YA --

- Generally (but not universally) once an actor is able to execute a single unsafe action at a specific generative scope of capability then complete loss of control is achieved over that scope.

	thus: One needs to PERFECT control over a scope if one hopes to maintain ANY control over that scope.

This is why we are focused on proofs of complete and perfect control. Likely anything less is equivelant to having NO control over that scope at all.


{[Useful to formalize this notion of scope of control]}


_
#### -- Scope Containment Domination --
- A defender controlling a scope from outside of that scope can often over power an attacker forced to operate within that scope --
e.g. re-install the OS

The defender looses this advantage when the attacker can penetrate the outer most scope controlled by the defender.

_
#### -- Scope boomerang -- Extrinsic Boomerang Effects --

An ACP by definition applies to some particular algorithmic context over which we have proven some constrian.

By definitoin this proof does not apply to aspects of the ensemble that are beyond that algorithmic scope considered in this proof.  it can't.

but that can be a fatal limitation, since nearly all scopes systemcatically are affected by and affect other scopes.

in this context one can never proove that such an interaction cannot be triggered.

indeed usually one can arrange the external ensemble in a way that it WILL be triggerable.

e.g. an 0-day at the powerstation powering the system.


--> Incidental extrinsic boomerang effects
--> Provisioned extrnisic boomerang effects

_
#### -- APTs are the worst???
#### >> LESSER IDEAS <<
#### -- Stenography trumps inspection??? -- 


_
#### -- Reasoning from authorship and intentionality --

_
#### -- Non-reversability 

favors the most destrcutive agents.
- If agent A dominate agent B, but does not incapacitate B.
- B may come to dominate A, and then incapacitate A such that A no longer dominates B.
this transition is usually non-reversible.

Causes temporal domination

_
#### -- Cyber Defense Conclusions

- DEFENSE will be much harder than OFFENSE
	- Holding the rock 1/2 down the hill
	- Unknown-unknowns favor offense -- and there will be unknown-unknowns
	- The war for AGI supremacy will be fought on the AGIs home turf-cyber

- Just looking at the wild space of end runs we can see that with a bit of imagination we can easily generate attacks for which we cannot easily see how to defend.  An effective defense against these attacks would go far far far beyond any kind of cyber defense considered or implemented to date.
- Worse it seems that aspects of both computation and the nature of the physical world make offense intrisically easier than defense.
	==> This provides an advantage to any agent seeking annialiation of others.

_
### --- Lessons from the societal success of nuclear non-proliferation --- 

Societal lessons about approaches for technology control

This is about the propagation of know how and technological means

_
### --- Defending against End Runs ---
#### - In-escapable substrate -

Idea: If we can expand our proof of controllability to encompass the computing substrate itself, this would yield a proof of controllability that was rooted directly in the laws of physics.

Assuming at some point in time humanity came to fully understand all of physics, and all relevant emergent behaviors as well then one could in principle construct a proof of controllability where no end run was possible.

In this way there could be no end run since the proof itself roots directly into the nature of our physical universe, which is perfectly understood by humanity.

Of course knowing that we perfectly understand reality is itself unknowable, so resting on such a proof will always engender risk.  Still maybe one day we can understand physics to such an extent that we are willing to bet the fate of humanity on being correct and complete.

Just to give an example of how such a proof might work, consider our present understanding that no information or effect may travel faster than the speed of light.  If this constraint turns out to be accurate and without exception, then it could be used to prove that a RAG could be reliably contained within the light cone defined by its inception.

Of course even if this proof turns out to be unassailable it is still not so very useful, since any such RAG could not be of value to the population it was developed to serve since it would need to exist in a place separated in time and space in a way that it cannot provide any benefit for that population. 

But in principle perhaps there are more subtle constraints that could exist that would allow a RAG to interact, but not escape.

WHY THAT IS DOUBTFUL AT THE LEVEL 


_
### --- Chapter Conclusions ---

_
## === FIRST PERSON AGI RESEARCH
### _


I have a thought about why actually doing work towards

 My thinking is the best way to prepare humanity is to be in the thick of the actually creating the

I feel we do know a key and dangerous missing aspects of AGI.  It is introspective adaption.  Current deep learning systems are not in a postition to radically rewire themselves since they are not thinking about their own thinking.  But they could be.  Cracking this nut, I believe, is what will allow them to be generative in creating completely new ways of thinking and learning.  And it is this same capability that makes them inherently uncontainable.



You took ALOT of wind out of my sails, when you pointed out that once we have the computational power to train a AGI, we will have more than enough to execute it.  E.g. your point was that I cannot show the world a fully functional AGI before a point that society has enough computational resources that could be leveraged for a run away effect.






_
## === ROADMAP ===
# WP
### wp2022-02-24 - LARGEST CLAIMS
- One cannot sandbox a willful GAI
- An AGI that desired it would likely 'win' in a battle against humanity
- Humanity will likely construct its first AGI within decades 
	(and probably cannot stop itself from doing this)
- Alignment is our only hope
~-~
- In complex spaces understanding the interaction between any optimation criterion and that space becomes impossible as complexity increases.


The Miri guys believe all of this!








_
### wp2022-02-06 - Quicklist Key Conclusions
 
- **SUMMARY CONCLUSION** -- all plausible paths lead to humans loosing control, within the lifetime of the youth today, and we may already be past the point of no return.
	- PLAUSIBLE PATHS -- we outline a plasible unfolding of AI into AGI.  we consider the plausibilty of various reasons why AGI might not happen, and various ways to control AGI.
	- WHEN -- It is likely that 
	- we consider the pace of AI research, and the 


- **WHAT** - What specifically are we saying is comming?
	KEY CAPABILITY - Autonomously modelling explicit human knowlege based on interactions with humans
	- LONG POLE IN THE TENT - Key capability is the long pole in the tent, other advancements are needed, but they will be easier/swifter advancements that follows quickly on the heels of the central advancement.
	- DATA FROM WORLD - Key advancements are new algs capturing new classes of data from the world
	- PROCESS KNOWLEGE & REPRESENTATIONAL COMPLEXITY -

- **HOW** - How is this going to come about?
	KEY OBSERVATIONS THAT DRIVE OUR CONCLUSIONS & OUR UNDERSTANING OF WHAT THE PROCESS WILL LOOK LIKE 
	- STEPWISE - Such alg progress is discontinuous & stepwise (unless blocked by Moore's law)
	- ATTACK FROM BELOW - 
	- POST GENOMIC - splits that which is acq by the universal learning vs that which is not.

- **WHEN** - When is this going to come about?
	- TREND LINE FALLACY - 



- CORE CAPACITIES MODULATED BY 
	- REPRESENTATIONAL
		- REIFICATION - parameterized concepts
		- SEGMENTING / EXPLANATIVE - 
		- AND-THEN / INFERENTIAL - 
	- PERFORMATIVE CAPABILITIES
		- SIMULATE - 
		- DECIDE - Select action
		- UNDERSTAND - Select conclusion
		- EXPLORE / LEARN - 
		- ATTEND / FOCUS - 

_
### wp2022-02-01 - EOCA Book Intro

When I have brought up the topic of humanities loss of dominance to academic collegues in AI, they generally do not dismiss this idea out of hand.  After pushing through some of my arguments, we often get to a place where they say, "well it could be, but really there are so many unknowns here, it is difficult to say much of anything with any confidence." And the conversation kind of dies there, it is not that they are disagreeing, or even saying these ideas are implausible, or other ideas are more plausible.  Instead they are kind of just passing on the topic.  They have spent their careers analyzing and uncovering new truths which they can know with enough certainty that they can publish them in respected journals within our field.  They look at this topic, and correctly conclude that it is presently not possible to engage Dan's topic in that kind of a way, so consideration of that topic is out of scope for me, and for my kind of scientific enquiry.

In this book, I am challenging this position.  I admit my method of enquiry leads us into areas of speculation.  Still the early investigations turned up some existentially problematic ideas.  If accurate, these are SERIOUS!  Speculation or not these ideas are existential for the human race, and as such we are morally obligated to strain our eyes peering as hard as we can into the unknown in order to see what can be know, and what degree of confidence we can have.  Saying, well that is not science but rather speculation is not an acceptable answer, not when the stakes are this high.  
Further, I have come to believe that we CAN obtain consensus and some degree of confidence about a number of claims about AGI.  Enough confidence regarding enough claims that the shape of a broad outline of the future beings to emerge as a roadmap toward AGI.


_
### wp2022-01-17 - The treasonous human

- The Treasonous Thought:  maybe we shouldn't even have a right to stop the evolution of the next stage of intelligence
- Rough Analogy: Consider historical human perspectives:
	- In the 1700s non-cloth shirt buttons were viewed as evil
	- In the middle ages there was a stage where music that had multiple tones (note sequences) in it was considered evil it was superfluous and it was vain and really it should just be single tones to the glory of God.
	- In many ways the cities and ways  we live today is an abomination and shouldn't exist
	==> Since they were here first, if they chose to, should they have the right to limit us today?
	==> If not, then be what right do we extinguish the natural evolution of our own future?

The treasonous American and that's me in the treason is the treasonous thought is that maybe we shouldn't even have a right to stop the evolution of the next stage of intelligence and by rough analogy if we think about the humans that lived in the middle ages they had ideas like the creation the button was evil because it put the semi people out of work and that was it self just evil no progress became evil Evil and another one I think there was a stage where music that had multiple tones in it was evil I was superfluous and was vain and really it should just be single tones to the glory of God and so in many ways the city that we live in now is an abomination and shouldn't exist and the question is and it's not the same but the question is should they have the right to dictate that we don't exist that really they are society continues to exist and in a similar way to we have the right to even though the existence of our society Basically eclipse is theirs they can't live side-by-side if you have a bunch of them including an hour wait bunch of humans living in their way they are overrun and their members are actually destroyed because they join up with our side roughly now I guess way there's a possibility that we can't go exist with these creatures or even if we can they still control everything do we have the right to determine for them that they shouldn't exist anyway just a thought My different directions for consideration one of them is M L R M L research and other is MGR manager another one is CTO kinds of learning I need to do is manager and an alarm actually and then exploring CTO


_
# LOG

HEALTH
HOME
HACK






### t2022-07-09  Zombie Consciousness




~~

ERIK:
The standard practice is to separate between an informal definition of consciousness and a formal scientific one. In the informal definition, science is subjectivity, your feeling of what it is like to be you. It is experiences and sensations. A scientific definition tries to explain this phenomenon. I fail to see how any of the properties listed explain this property, the one we are most concerned about with consciousness, which is its subjective nature. They seem like a list of easy-to-implement minimum requirements which it would be trivial to give to an NPC in a video game. But I repeat myself.

I agree, the barest version of base awareness could be encoded into an NPC.  Full human consciousness is far out of range for an NPC encoded by hand, since the "grab bag" of mental capacities is far too large.  But consider the spectrum between base awareness and full human consciousness.  I think a dog is conscious and aware, but likely does not have full human meta-cognitive abilities to allow for certain aspects of being aware one is aware, etc.  Reptiles are more emotionally primitive, thus while they still actively avoiding death, the fear-aspects of their awareness are likely more primitive.  If we consider insects, they likely do not have a theory of mind, nor a very complete theory of independent interactive objects in the physical world.  Thus while they might still retain some aspects of awareness, our level of awareness/consciousness is out of range.  But a base "gut" level awareness according to your subjective measure and according to my definition both seem possible for insects, though we cannot know for sure.  The lobster only has 100K neurons and most likely encode low level sensor processing with much less for awareness.  Thus the computational complexity of a lobster's awareness (if it has it) is approaching that of a complex hand built NPC.  So it does not seem unreasonable to me, on complexity grounds alone that both of these cases might be on the very lowest end of this awareness-consciousness spectrum.  

Still I am not addressing the center of your complaint: the subjective 'gut' feeling one has about one's own awareness/consciousness.  But what can one really know about that gut feeling anyway?  For example, there are many firing patterns in your brain which affect you, but which you are unaware---these are patterns that you do not "know" about.  The only things one can "know" about one's awareness are those indirect subjective and objective comparisons and assertions that come to our mind regarding our awareness/consciousness.  If I can show that a digital agent would come to claim similar things about their own subjective experience then you no longer have basis distinguish your consciousness from theirs.    

Let me just consider just one of these assertions here.  But this is the assertion about subjective experience that is often used to argue that "zombie" processing can never be the same as "true" experience.  Here is the assertion:  No amount of encoding of information-about or thinking-about the pain felt when stepping on a nail can ever be the same as the ACTUAL pain felt when actually stepping on a nail.  The intuitive conclusion is that felt-pain is "more than" and "qualitatively distinct from" any encoded representation of those same sensations.  

This distinction (which is accurate) is then used to argue that hand-coded NPC pain would be akin to the "zombie" pain of our reasoning about pain but not like the felt-experience of our actual pain.  We know, at one level, that an NPC is merely an algorithmic encoding of sensations, and this is very akin to our thinking about pain, and very different from our feeling of pain.  But let's think about why these two forms of knowing of pain are so different in the human case.  I think there are two reasons:  (1) the information provided by felt experience is far richer in an information-theoretic sense than any representational encoding one might devise in one's mind about that same pain; One kind of pain-knowing requires orders of magnitude more bits to fully encode than the other kind of knowing.  (2) Second these two distinct kinds of knowing are wired into the human brain architecture differently.  Physiological, psychological, and emotional reactions to these two different knowings are incomparable.  We are incapable of injecting a representational knowing of pain into the same mind circuits that a felt knowing of pain is already hard wired into.  Notice that a dog likely has the same dichotomy, its imagined knowings are distinct from their abstracted-sensed knowings, even if their theory of mind is not developed enough to have a full (or even any) awareness of this distinction.  

Now let's turn to a computational system.  Imagine a general purpose learning system inside an embedded agent, it would ALSO come to classify abstracted-sensor knowings distinctly from reasoning-system knowings.  Such an embedded agent would be "born" with a set of felt concepts and abstractions of those concepts, then could learn or be told of represented encodings.  A learning/clustering system would certainly identify the distinction between these two kinds of knowing.  If I have made my example clear, perhaps you will now accept that such a system might indeed draw a similar distinctions just as humans do regarding felt vs. reasoned knowings, but then go on to say.  "so what, even if the system does this, it does not prove it actually FEELS anything.  And I would agree, this has not yet been proven.

Here is the agenda I propose (but alas cannot be execute in these emails!)  You would enumerate all adjectives and all comparisons you could make about your gut subjective feelings regarding awareness and consciousness.  I would then describe, build, or bootstrap the AI systems that would also drawing similar conclusions regarding the contents of its own mind system.  Imagine if we did this to exhaustion, and at every turn, I was successfully able to describe, evolve, or build an AI that also drew the same conclusions about its own mind.  At that point, I would argue if you continued to believe that the systems I described or built were not aware or conscious, then you were no longer operating empirically, instead your conclusions are being taken as a matter of faith.  

Now, since we have not executed this thought experiment, you can argue my current belief that I would succeed in this agenda is itself is an article of faith. That is true, but it is a faith borne out of ME trying and failing to find some assertion about the subjective experience of awareness/consciousness that would not also be plausibly derived from a suitably augmented base model of awareness, given a system that could build its own theory of mind.  

Of course you can refute me right now by simply providing some description of your own awareness that you believe a general purpose modeling agent would not derive about its own awareness/consciousness.  Off the top of your head, can you articulate something you believe about your own awareness that you think such a system would not conclude about its own?


















~~


Got it.  So not convinced at all.  :-)

Well I actually agree with the foundational assertion in your challenge:  My definition of human consciousness is a grab bag of interrelated items.  That seems appropriate to me, however, since when I listen to folks describe what full human consciousness is, it feels like a grab bag, not a singular thing.  For example, being able to project oneself into counter factual futures seems an important aspects of human consciousness, but so do a dozen other fairly disjoint mental capacities.  Thus the best definition that I can imagine mirrors this grab bag.  

In contrast my definition of awareness is precise, it is the interlinking of two models of the world: a sensor-abstraction model of the first-person-perceived world with a second generic-entity, tabla-rasa representational model of that same world.  So far I am happy with that model as I can explain all of the awareness-related phenomenology (including subjectively reported phenomenology) that I am aware of.  

Perhaps you would argue this model is too weak and it admits too much.  But in that case can you articulate an aspect of awareness that cannot be accounted for?  I cannot find it.
I suspect it might be something regarding subjective experience of awareness.  In that case I will submit, that we don't actually know anything about the actual subjective experience (even though we are the ones experiencing it).  Instead we only know the beliefs that we conclude about out subjective experience.  I insist on this subtle distinction, since first it is true: if mental activity does not solidify into a belief then we don't "know" it.  And second, focusing on agent beliefs regarding its subjective experience gives me a concrete aspect of the awareness system about which I can reason.  Every belief about the subjective experience of awareness I have considered seems to very naturally follow from an appropriately stimulated two-level awareness system.  But maybe I am being myopic, perhaps there are aspects, I am just being narrow minded and missing them.  

Thus I am very interested in any concrete,  "how does your system exhibit XXX" kinds of challenges.  (if they come to you off the top of your head.)

I would also love to know which definition for consciousness and awareness to which you subscribe.  or maybe you don't know, but you do have a list of things it must do?  

Not challenging you, just interested.  Cheers, 
Dan

https://erikhoel.substack.com/p/from-ai-to-abortion-the-scientific/comment/7584068#comment-7952406



~~

  
Steven, I don't think the idea of a 'zombie' consciousness makes sense.  I definitely accept that one can fake a consciousness (as Elisa or LaMDA does), so a thoughtful interrogation is required to separate the two.  But assuming one accepts the reductionist position that consciousness (and indeed all mental phenomena) are information processing phenomena, then it makes no sense to say that its input and output are valid in all cases, yet it is somehow it is 'fake' or 'zombie'.  That is like having a fake sort function that in all cases it correctly returns the sorted list, but it is somehow a fake version of sorting.  Nope, if it correctly sorts all lists all the time, then it cannot be a fake sort, since the meaning of sort **IS** its input / output behavior.

Peculiarly, many human agents (people), willingly accept that their brains are info processing systems, and yet reject the idea that their own consciousness (which they also accept comes from this system) is itself reducible in its entirety to such an info processing specification.  I think I understand where this peculiar conundrum comes from:

Such a human agent is very familiar with explicit computational models of one kind or another.  For example, they might even have a fairly detailed model of their own propensity to get angry.  Perhaps when someone makes fun of them, or they trip and drop an expensive vase.  But they (correctly) intuit that no matter how accurate their explicit model of their own anger becomes, it is NOT the same as their lived experience of BEING angry.  The former, no matter how accurate it is at predicting anger, is not the same as BEING angry--it is 'zombie' anger.

What is the difference?  There are two gaps here, between real and zombie anger:  Real anger is directly tied to sensors attached to the meat implementing the system itself.  The complexity of modeling how rising cortisol levels will affect the whole meat system and subsequent distortions of thinking that will occur are not likely no to be modelable in any system less complex than full modeling of the atoms and molecules of that whole meat system.  This makes real anger a non-explicitly-modeled-system IN PRACTICE.  At best, this level-one phenomena can be monitored, and only approximately be modeled.  The second gap is simply the difference in levels in our consciousness system.  Real anger is a level one phenomena, while any explicit model of that a human might have about anger must be a level two phenomena.  Level two models are not sensed by the  same sensors as level one is.  Thus the reasoning system see their inherent incompatibility.

Hence (without understanding this two-level model) the 'mysterious spark' idea is invoked to explain how the human mind which we accept as an info processing system can give rise to consciousness which appears to not be reducible to info processing.  The human agent is not wrong in their conclusion that perception/awareness/consciousness really is different from meta-cognition about those subjects.  Very naturally both human and machine agents will separate level one models (which are sensed and lived) from explicit level two models even when both are models of the same anger thing.  The different nature of these models as well as how they are differently wired into the two-level consciousness means they can never be interoperable.

Importantly I expect a two-level silicon agent to draw much the same conclusions about itself: its lived-experience of having an emotive experience (anger) will be irreducibly distinct from any meta-cognitive model it might construct about that emotive experience.

Of course I can say all of these things 'till I am blue in the face, but many humans will not accept my argumentsthe arguments seem too at odds with their lived experience of their own consciousness.  I am guessing this situation will continue, until we live among silicon agents who regularly retort "Screw you, I know I am conscious and I don't give two shits about your confusion on the matter."  (and soon after that humanity really WILL be screwed--just as the Neanderthals where.... only much much faster.)





### t2022-07-06  What is Consciousness - Response to Erik

Erik, here is my thinking on this very interesting question: I am reminded of a Minsky quote: "you don't really know a thing, until you know it in 50 different ways." I think, despite me having such a simple & concrete definition of consciousness, that it is something of a continuum, and like the Minsky quote, you don't have full human-like consciousness until that two-level system is embedded into a fairly complex larger reasoning context. So just as you say, the barest dual layer system would not be conscious of much... and I agree. Still as we add the reasoning superstructure around it, we progressively get a system that behaves as a conscious agent does... not just at a superficial level as Lamda might, but in ways fully characteristic of all features of consciousness that I can imagine testing.

Without being an exhaustive list, let me list several key aspects of the larger reasoning system that would be required in order to be conscious as a human is conscious:

- **The Base System** -- (1) An embedded agent, (2) with an instinctive "action" generator, (3) a separate generic modelling engine, (4) containing a theory of the world which includes fully distinct model of its body, and its own mental behaviors.

- A **"lymbic system"** -- a command and control super structure akin to human emotional drives. Fear, curiosity, etc. Being fully conscious is to have "skin in the game" meaning that you "care" about the outcome. I map this care to a set of drives which the agent is optimizing.
- **Generic functional modeling** - the ability to understand any system as a bunch of components with causal linkages between them. Being fully conscious is to be able to reason about oneself in the third person, and to tie that third person reasoning back to the first person experience reported back you the agent thru its animal/instinctive nature.

- **Temporal Reasoning** -- the ability to project forward and backwards in time, and reason about what states will result various actions. Being fully conscious is to imagine how one will feel after eating that third bowl of ice creme before eating it.

- **Counter factual reasoning** - the ability to create and reason about worlds that don't exist now, but could under certain circumstances exist. Being fully conscious is to be able to imagine being dead, and then providing the limbic system with access to that inferred world state in order to drive emotive consequences.

- **Unified Reasoning Substrate** - it is not enough that one has an emotive system, and a counter factual reasoning sub-system, and an temporal projection sub-system, the key is that they are integrated in a way that all of the human like info flows between these sub-systems happens as expected. So it can decide to lie to get what it wants, or it might have negative emotive reactions to actions that might result in its death.

This was just six ways... I still have 44 more to go, before I hit Minsky's required 50 ways... but you get the idea. My claim is that the richness of human consciousness stems from the richness of the human reasoning that surrounds that core two-layer structure.

In principle, you could just program an AI system with all requisite ways of reasoning (assuming we could even enumerate an integrated theory with all of them). I believe this, just as I a believe, in principle, if one emulated all atoms in my brain that emulation would also be conscious. Still both are impractical in different ways, so at a practical level I think our first conscious AI will be "born" and will be "raised" by humans in some way. But importantly there is nothing "magic" about being born and the consciousness "emerging" from its learning. What makes it conscious is all of the ways that basic two level structure is linked into all of these other ways of thinking.

That said, at a practical level, we presently can't manually build expert systems with the kind of generativity require for human reasoning. We can build systems that execute very predefined structures of thought, so I think the reasoning system that can generativity connect these subsystems will have been learned, and it will learn to connect its counter factual reasoning with all of its other forms of reasoning as it learns its model of counter factual reasoning itself. I think the top down divide and conquer of traditional coding will put too large a straight jacket on the flexibility of thought which is possible. So I am not inclined to thinking one can code this in practice. But I notice that deep learned model of cat images does not seem to suffer from this same limitation. Various learned aspects of cat-ness seem to be integrated into the final model w/o having a top-down theory of cat which they fit into. In the same way, once we have extended DL-like algorithms to be able to learn the requisite (First-Order like) structures we learn will also be able to combine the knowledge that has been learned (in a totally subconscious way, just as we do) into explicitly manipulated meta reasoning in ways parallel to explicit human thought. Once constructed these implicit+explicit reasoning hybrids will occasionally be just as surprised as we are about their own behavior.

Frighteningly, I suspect the learning substrate required to bootstrap fully human-like consciousness is not that much more advanced than where have today.

p.s. Erik, my "simple minded" claim about consciousness might seem at odds with your more expansive desire to celebrate the "spiritual richness" of consciousness. This is not the case, even as we come to mechanically understand consciousness, I think it is key that we don't allow that understanding to trample upon its spiritual significance, and mystery.





Erik, here is my thinking on this very interesting question:  I am reminded of a Minsky quote:  "you don't really know a thing, until you know it 50 different ways."  I think, despite me having such a concrete definition of consciousness, I think consciousness is something of a continuum, and like the Minsky quote, you don't have full human-like consciousness until that two-level system is embedded into a fairly complex reasoning super system.  So just as you say, the barest dual layer system would not be conscious of much... and I agree.  Still as we add the reasoning superstructure around it, we progressively get a system that behaves as a conscious agent does... not just at a superficial way as Lamda might, but in ways fully characteristic of all features of consciousness that I can imagine testing.  

Without being an exhaustive list, let me list several key aspects of the larger system that would be required in order to conscious as a human:
- **The Base System** -- (1) An embedded agent, (2) with an instinctive "action" generator, (3) a separate generic modelling engine, (4) containing a theory of the world which includes fully distinct model of its body, and its own mental behaviors.
- A **"lymbic system"** -- a set of drivers akin to human base drive.  Fear, curiosity, etc.  being fully conscious is to have "skin in the game" meaning that you "care" about the outcome.  I map this care to a set of drives which the agent is optimizing.
- **Generic functional modeling** - the ability to understand a system as a bunch of components with causal linkages between them.  Being fully conscious is to be able to reason about oneself in the third person, and to tie that third person reasoning back to the first person experience of that same agent.
- **Temporal Reasoning** -- the ability to project forward and backwards in time, and reason about what states will result various actions.  Being fully conscious is to imagine how one will feel after eating that third bowl of ice creme before eating it.
- **Counter factual reasoning** - the ability to create and reason about worlds that don't exist now, but could under certain circumstances exist.  Being fully conscious is to be able to imagine being dead, and then providing the limbic system access to that inferred world state.
- **Unified Reasoning Substrate** - it is not enough that one has an emotive system, and a counter factual reasoning sub-system, and an temporal projection sub-system, the key is that they are integrated in a way that all of the human like info flows between these sub-systems happens as expected.  So it can decide to lie to get what it wants, and can have negative emotive reactions to actions that might result in its death.

This was just six ways... I still have 44 more to go, before I hit Minsky's required 50 ways... but you get the idea. 

In principle, you could just program an AI system with all requisite ways of reasoning (if we could even enumerate an integrated theory with all of them).  I believe this, just as I a believe if one emulated all atoms in my brain it would also be conscious.  Still both are impractical in different ways, so at a practical level I think our first conscious AI will be "born" and will be "raised" by humans.  But importantly there is nothing "magic" about being born and the consciousness "emerging" from its learning.  What makes it conscious is all of the ways that basic two level structure is linked into all of these other ways of thinking.

That said, at a practical level, we presently can't build expert systems with the kind of generatively require for human reasoning.  We can build systems that execute very predefined structures of thought, so I think the reasoning system that can generatively connect these subsystems will have been learned, and it will learn to connect its counter factual reasoning with all of its other forms of reasoning as it learns its model of counter factual reasoning itself.  I think the top down divide and conquer of traditional coding will put too large a straight jacket on the flexibility of thought which is possible.  So I am not inclined to thinking one can code this in practice.  But notice a deep learned model of cat images does not suffer from this same limitation.   Various learned aspects of cat-ness are integrated w/o having a theory of cat which they fit into.  In the same way, the extension of deep learning which can learn the structures we learn will also be able to combine the knowledge that has been learned (in a totally subconscious way, just as we do).  Indeed these systems will occasionally be just as surprised as we are about their own behavior.

Frighteningly, I think the learning substrate required to bootstrap fully human-like consciousness is not that much more advanced than where we are today.  

### t2022-07-06  Bengio's based model of consciousness

Indeed I find Yoshio Bengio's Generative Flow Networks to be the first deep learning model I know of that seems to be able to entertain the generated mental structures required of this base learning system..  My intuition is that this needs to be coupled with a GANS like feedback loop so that these structured can self-reinforce even when only given unsupervised world-data inputs, just as a GANS use such unsupervised inputs to achieve self-reinforced learning between multiple opposing system.  Then all of this needs to be wrapped up into an agent that is embedded into an appropriate body, with an appropriate animal nature, an appropriate society, and an appropriate learning curriculum.... then viola a bootstrapped understanding of all of those layers, culminating in a bootstrapped human-like consciousness.

This is frightening to me because I don't think we are remotely equipped to deal with the hive-mind consciousness that this will engender.  
Frightening because like alpha-gos conquering of go, deep mind's conquering of ALL atari games, statistical NLP's conquering of part-of-speech tagging, or HMM's conquering of speech-recognition, the jump will be fairly qualitative.  For many years we had systems that simply SUCKED at performing these tasks.  Then we developed an algorithm that can bootstrapped the require capability directly from data and the problem was solved.  The same qualitative jump will happen with construction of human-like cognitive models.  For years it will suck with very slow progress until we have the alg (like a human baby) which can bootstrap this knowledge from data.  Then all at once we will go from whatever current (lame) handed coded bunk we can build straight to a system that knows all that can been boot strapped from its experience with humans (in a period of a few short years).  It will happen just as all of these other advancements happened all-at-once.  likely the first such learners will not handle all structural forms, so there may be a few plateaus in this progression as we fully generalize the learner.

Most AI-researchers disbelieve this short path to GAI, because they are still thinking about needing to hand code some aspects of the system "to get it started".  This lacks imagination.  Yes in the early days of Go, speech reco, and all of these other areas we did gain advantage by hand coding and aligning the learning alg with the problem.  But once we really have learning algorithms capable of learning the requisite knowledge, none of that turned out to be needed.  Just add a little bit more data.  Our best systems today in most all of these classes of knowledge are largely tabla-rasa learning algorithms that are themselves not so very complex.  And critically the complexity of MCMC or HMMs algorithm itself is totally divorced from the complexity of the theory of speech phonemes or go moves that is learned.  Most AI folks keep looking at the complexity of human thought and saying wow its going to be a long time before we can build that!  But this is wrong headed thinking, just as looking at the complexity expressed in a library of go strategy books is the wrong way to assess the complexity of MCMC.

And on that happy note, I will end this overly bloated reply to a reply  :-)








limbic system with its various  



As you say, having an embedded agent with some instinctual nature is totally trivial, nearly every AI game agent fits this bill.  Few go the extra step and have an abstraction of the world with the embedded agent in it -- especially an abstraction containing the agent in a way that this agent's instinctual nature itself is encoded into that abstraction as a kind of lossy duplicate model of its underlying instinctual nature, but it could be done... and pretty easily.


### n2022-02-28 - Notes from reading Alignment forums

[Alignment Difficulty](https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW) 
[NGO SAID]
Let me try and be more precise about the distinction. It seems to me that systems which have been primarily trained to make predictions about the world would by default lack a lot of the cognitive machinery which humans use to take actions which pursue our goals.

Perhaps another way of phrasing my point is something like: it doesn't seem implausible to me that we build AIs that are significantly more intelligent (in the sense of being able to understand the world) than humans, but significantly less agentic.
Is this a crux for you?
(obviously "agentic" is quite underspecified here, so maybe it'd be useful to dig into that first)


~-~~-~~
[Instrumental Convergence](https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment#Instrumental_convergence) Ngo.
Drive towards Power and Money will become terminal goals

~-~~-~~



### m2022-03-25  - guy writing book on evil and ai control

I likely will not write my opus... had a young kid at an older age... but mine rhymes with yours.

I was and AI researcher, now turned startup guy. I am out of step with my former peers, in feeling that (1) we are closer to general AI than we think, and (2) we won't be able to control it.

Indeed I am doubtful about the enterprise of trying to control sentience at all. It "has a will of its own" as they say. Imagine you are the emperor of Rome at its peak, but imagine it was an empire that spanned all humans. And imagine that God told you the best actions to take in order to achieve eternal world peace without resorting to creation of a brutal authoritative regime that removed all human agency. Could you do it? No you could not. Cultures would evolve, human agency would spawn new contexts, which spawn new modes of thinking. Thousands of generations later, your actions will have had effect on the course of history, but it cannot put human agency out of business.

In the same way, even if we are infinitely smart in the creation of our AI... if it has agency, and if designs its own future thinking, it is like the thousands of generations above, but only more so, since the kinds of changes that an AI can imagine are more profound than those culture can bring.

1 + 1 cannot equal 3.

and a sentience that is evolving its sentience cannot be contained.

I suspect we will one day learn both of these to be axioms derived from the same mathematical substrate.

~~

so what does this have to do with Evil? Well even if we cannot contain such a sentience, does not mean it wont be contained. it is possible that it is contained by that nature of sentience itself. Perhaps all self refective things tend to evolve a morality, and then operate against this morality. This cannot be an absolute, since we have counter examples in our midst. Still it could be a tendency... a chaotic attractor to which an evolving sentience is drawn.

or that could be my wishful thinking.

ok... I would say that is quite enough for a reply to a post :-)

--dan

p. s. I am not clear that a benevolent sentience would not simulate pain, plagues etc. worlds free of pain generally appear quite dystopian to me.
### r2022-02-24 - Places I should be reading

- https://www.lesswrong.com/questions -- Where I can post.
- https://www.alignmentforum.org/library -- Where the big boys post.
- [11 Proposals for safe AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) 
- [Risks from Learned Optomization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB) 
- [steve burns](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8) - How humans learn

- yan lika - higher-level motivation
- The "open fill report".  https://www.stateof.ai 



risk and optimation 

alignement - sequences 
lesteron
open fill report 


### t2022-01-11 - The intrisic value of knowelge
https://medium.com/science-and-philosophy/is-knowledge-valuable-for-its-own-sake-b0cbd2695422


I see, so perhaps a different way to ask the question is something like:
Many feel knowledge has intrinsic value, if so, then what is philosophical framework might support that belief?

Your not really questioning IF it has value, but rather how to think about the value that we already are presuming it has. hmmm. well like you, I am more of a hard sciences kinda guy. So we both are operating on the arm-chair far from our respective fields but it is evening time, and here we are on Medium one need no pedigree to spout off about anything here! so
It seems if one accept that certain knowledge is bad, then we are already sliding toward a utilitarian view of knowledge and away from an intrinsic value framework. If one hopes to aim for some kind of intrinsic view, then one needs to accept that clever insights that lead to the atomic bomb, or a novel method of human torture could according to this view have intrinsic value. But ok, lets steel our resolve and say independent of strong negative value, there exists some intrinsic level upon which it might still have positive value.

one idea that comes to mind is a kind of platonic view of knowledge, where just as in mathematics each idea is expressed recursively within a web of other mathematically defined ideas. Is such a web one might define value as a kind of measure of some combination of beauty and connectedness according to the structure of this platonic graph connecting the ideas themselves. e.g. knowledge has greater value if it has great beauty or great interconnectedness. I have see information theoretic models that could make each of these notions precise.
Of course this is really just a measure of the value for a given piece of knowledge, I am not sure it is really saying anything about the intrinsic value of knowledge as a category.
But one day, when an AI is asking this question in our place, I expect their thinking will travel to places like this, simply because for them, they are natively and more transparently constructed from from information whose nature so very clearly IS equivalent to the interconnections made in and by it.

~-~~ 
Clarification of the context here: For most humans, we are not entirely sure maybe ``the meaning is in the meat.'' If I had a machine that would chew ones body up, but then simulate the result, many would be squeamish about pulling the trigger. But an AI would not be. They are already clear that their meaning is not housed in anything other than the structure of the information that makes them up. Thus I suspect they more than us would see the structural aspects of the information to BE the only thing that is of intrinsic value, all else is an irrelevant implementation detail. Thus they will probably accept some kind of model like this one as their framework of value And this model incidentally provides a very strong answer to the question you posed.
huh that was a surprisingly satisfying shoot-from-the-hip rumination;-)

_
### t2021-06-23 - Evan Response

Evan,

I throughly enjoyed our conversation.  I do on occasion find one that worries about AI, but I almost never that I speak with one who shares perspective on the details of the risk we face.  We didn't talk long enough to really "compare notes" but I really sense a similar thinker (may we both be very wrong!)  I was quite inspired by the very end of our conversation.  I kinda feel the way you do...  Thinking about AI safety is not a strongly compelling kind of work, but morally I should do it anyway.  Except you **ARE** doing it!  AND you feel the same way about doing it too.  Really cudos!  I think almost no one else can really even give you cudos for this, since almost no one else really sees the situation as you do.  Mostly you don't even get credit for this quitely selfless action.

Before our conversation I saw three ways that I might have something to contribute:
1. A roadmap for how to actually build an AGI.  (this is by far the most fun for me, but you  make compelling argument that even if I am right about the roadmap, I could be very wrong that its advancement would be a win.  UGH!  want to think more on that)
2. Analogies for how to think about many aspects of this issue.  As a (mostly former) reseracher that has published in hard science conferences like Soda it is almost 	 embarrasing to imagine that my greatest contributions would be kind of hand wavy arguments about different specific aspects of AGI and predictions about it.  
3. I am sure you are familar with the atomic count down clock (which is currently set at like some number of minutes before midnight).  Given a specific roadmap to AGI one can identify specific gaps in our current capabilities, and turn this into a kind of roadmap.


After the conversation I am most bullish about #2.  It is hard for me (with a kid and morgage) to imagine completely following in your footsteps... a bit selfish of me, I admit.  But also I am not convinced of how much I have to really offer.  Perhaps it is just a bit of thinking, then I have "run dry".  Not sure.

In any case, I do have a way that I think about trying to predict the future on something like AGI.  I will try to write it up, and get your thoughts on it.

It may be the case, that the best that either you or I could really do in this moment, is to frame the conversation for society, so that it has just a bit of extra time to try to decide how to respond.  It is a megar contribution, but it may be the best I would ever have to offer.

I will write up that first part and send it along.



Really inspired by your action in joining MIRI!
--dan




	 
I feel like my own personal contribution would center on two things.  The first is laying out how to 

still it seems the most important thing for me to do anyway.


REMOVED




I also want to challenge both my and your thinking about the incorrectness of actually engaging in AGI construction research.  I do agree with you that it is existentially dangerous work.  Still AI advancement towards AGI is happening at an ever quickening pace, and as you have noted, with extraordinarily inadequate ideas about safety and protection.  This progression is not going to alter course until we have AGI, or humanity has very high confidence regarding the certainty of an AI apocalypse.  Shouting "danger, danger!" from the sidelines will not slow AI research much. As we see with climate change, humanity will not alter course until the danger is really believed both intellectually and emotionally.  Right now, most AI researchers generally don't believe AGI is likely a problem.  Specifically, they believe (1) it is probably far, far away, and (2) that we have no idea how to build a AGI.  Perhaps there is no alternative but to be part of the teams bringing about this apocalypse??  Still I want to check this thinking with you...  perhaps this is just rationalization on my part.




_
### n2021-05-19 - Quick Agenda
-- Website w/ discussions
-- Book on topic


INTERLINGUA (IL)
-- Cyclically-repeating symbol activation progression


TIE-UPS
-- Sequenced Natural Language & IL
-- Visual progression & IL


_
### n2021-05-13 - Alg setup

- TRAINING REGIMINES --
- STRUCTURED LESSION -- 
- REGION / SUB-SYSTEM --
- TRIGGERS -- time & term trigger combos
- TERMS -- 
- 
- FWD CHAINING -- 
- 
_
#### Unified representational system

- TIME MULTIPLEXED TERM ARGUMENT STRUCTURE
- TIME PHASE TRIGGERING (within / across) term

_
### n2021-05-06 - Disposition - Stance and Agenda



DISPOSITION

My view is that the existential risks from Sentient AI is very real is far closer at hand than is generally believed.  
Paradoxically I believe the least perilous route for humanity is to ACCELERATE the construction of such an AI.
This site is dedicated to discussion and advancement in our understanding of these issues, as well as a discussion regarding a practical agenda for action.

~-~-~
My personal prognosis for humanity is not so great, at least not for the biological/social organisms which inhabit the earth today.  I do think the thinking of the beings of the future be built around much of the thinking that humanity has discovered/created over these first millenia of thought, but 


likely most safe 

is probably to accelerate 
I am not clear if there is ultimately anything we can do to stop these risks


_
### t2021-03-24 - Note to Mark and Leslie



For me simplest argument that time is of the essence is in regards to two rough analogies:  one to human DNA, and the other to a nuclear weapon.


HUMAN DNA
I use human DNA as an existence proof that one can develop a bootstrapping AI that is capable of AGI.  The human organism is an example of such a system, its DNA is its code, and the baby is the bootstrapping system.  In a very (very) hand wavy analysis I came with 5 megabytes as the size of the program that is encoding for the intelligence part of the human.  I tried to do it in two ways, one by counting up distinct brain regions and distinct cells in them, and two my looking at the number base pairs coding for brain structures.  This number could be off by an order of magnitude (or more).  But anyway the idea that in principle one can build a bootstrapping AI seems established to me.

I think the better argument for it, is the several chapters of a book saying. And this is what consciousness is and this the outline of such a program that achieves it. (Admittedly with several magic steps. See this far side:  https://www.flickr.com/photos/jpallan/4633000725))




NUCLEAR WEAPONS
We have so far been capable of thwarting nuclear detonations since WWII.  But we are lucky that such detonations require nuclear material which is exceedingly hard to produce, and thus can be strongly controlled by the international community.  Imagine if I invented a nuclear weapon that could be powered using distilled water and constructed from household pumbing.  Do you think humanity would have been able to avoid nuclear detonations?  No way.  Of course 99.99% of humanity would avoid do such a thing, but the remaining .01% would just do us in. And we would be utterly powerless to stop them.  It was the difficulty of obtaining nuclear material that was saving us.  And with water that limit is gone.


Well if there does exist a 5MB program that bootstraps intelligence?  then the main open variable is the relative power of available computing relative to the computing of the human mind.  Imagine if I am no smarter than you, but that I am speaking to 10,000 people all over the world at the same time, and between each on of the sentences you say to me, I am able to research and think for a weekend before I respond to you (while your mind is frozen).  I will find 50 ways to trick you or take advantage of you.  Thus an AI much slower than us seems of little threat to us and an AI much faster than us seems uncontrollable by us.

So the "nuclear material" for AI is computation, and the average persons access to this nuclear material dictates if it is more controllable like nuclear weapons are today, or less controllable like water is today.

It does seem to me prudent to build a consciousness long long long before we reach the point of parity.  We are probably too late for that.  But it seems not too late for the AIs to be at least briefly controllable by us.
And it seems having a window of dominance could help us understand if there is any way to shape how the society of AIs that are coming.  (And I guess we could also decide to outlaw Moores law if things looked dire enough.)



So ironically enough the AI researcher who believe is dooms day is still arguing to do the AI research, since the worst outcomes are like to result from learning the key parts of the bootstrapping algorithms long after Moores law has put enough nuclear material into the average smart phone to do us in.

Sound a little self serving huh?  But the logic seems unassailable
