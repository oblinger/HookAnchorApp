

**[Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262)** (Meng et al., 2022) Introduces causal tracing to identify which hidden states are decisive for factual predictions, revealing how information flows through multiple layers and localizes to specific MLP modules.

**[Mechanistic Interpretability for AI Safety: A Review](https://arxiv.org/html/2404.14082v1)** (2024) Comprehensive survey covering causal abstraction hierarchies, activation patching, and circuit discovery methods that recursively traverse network components to validate interpretability hypotheses.

**"[[2025-12-03 Open Problems in Mechanistic Interpretability]]"** 

**[Understanding Neural Networks Through Sparse Circuits](https://openai.com/index/understanding-neural-networks-through-sparse-circuits/)** (OpenAI, 2024) Demonstrates that sparse models contain small, disentangled circuits implementing recognizable algorithms, suggesting tractable paths toward understanding hierarchical computations.

**"[[2025-12-04 A Comprehensive Mechanistic Interpretability Explainer & Glossary]]"** 

**[Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://www.transformer-circuits.pub/2022/mech-interp-essay)** (Anthropic, 2022) Discusses the analogy between reverse-engineering neural networks and binary programs, emphasizing decomposition of activations into features and weights into connecting circuits.