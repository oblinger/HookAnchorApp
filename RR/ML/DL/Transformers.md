
- [[DL Notation]], 
- [[VAEs]], 
- [[Activation Functions]],   [[Auto Encoders]],  [[VAEs]],


- [[Positional Encoder]]
-   [[Attention]], [[Multi-headed Attention Block]], 



#### FAMILIES:
- GPT-like (also called _auto-regressive_ Transformer models)
- BERT-like (also called _auto-encoding_ Transformer models)
- [[Seq2Seq]] - BART/T5-like (also called _sequence-to-sequence_ Transformer models)

### Key Ideas
- Adjust parameters in parallel (unlike RNNs)
- Positional Embeddings
- 