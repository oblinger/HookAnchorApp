# AI Safety & Interpretability Researchers - Bay Area

## UC Berkeley - CHAI (Center for Human-Compatible AI)

### [Stuart Russell](https://people.eecs.berkeley.edu/~russell/) - AI alignment, value alignment, inverse reinforcement learning
Founder of CHAI. Co-author of "Artificial Intelligence: A Modern Approach." Focus on provably beneficial AI systems. Elected Fellow of the Royal Society 2025.

### [Pieter Abbeel](https://people.eecs.berkeley.edu/~pabbeel/) - Robotics, deep RL, AI safety
ACM Prize in Computing 2021. Co-developed cooperative inverse RL approaches with Russell.

### [Anca Dragan](https://people.eecs.berkeley.edu/~anca/) - Human-robot interaction, value alignment
Works on AI systems that learn human preferences and coordinate safely with humans.

### [Jacob Steinhardt](https://jsteinhardt.stat.berkeley.edu/) - Robustness, interpretability, AI safety
Probing techniques for monitoring latent world states. Understanding how LLMs bind entities.

## UC Berkeley - BAIR

### [Dawn Song](https://people.eecs.berkeley.edu/~dawnsong/) - Security, adversarial ML
AI security and robustness against adversarial attacks.

## Stanford - HAI / SAIL

### [Percy Liang](https://cs.stanford.edu/~pliang/) - Foundation models, interpretability, benchmarking
Director of CRFM. Work on influence functions for explaining predictions. HELM benchmarking.

### [Christopher Manning](https://nlp.stanford.edu/~manning/) - NLP foundations, interpretability
Co-founder of Stanford HAI. Pioneer of word vectors and attention mechanisms underlying modern LLMs.

### [Dan Jurafsky](https://web.stanford.edu/~jurafsky/) - NLP, linguistics, AI
Collaborates with Liang and Manning on interpretability and safety.

### [Dorsa Sadigh](https://dorsa.fyi/) - Human-robot interaction, safe autonomy
Algorithms for safe and adaptive human-AI interaction.

### [Chelsea Finn](https://ai.stanford.edu/~cbfinn/) - Meta-learning, robustness
Works on robust and adaptable AI systems.

## Industry Labs - Bay Area

### [Chris Olah](https://colah.github.io/) (Anthropic) - Mechanistic interpretability pioneer
Founded interpretability research direction. Feature visualization, circuits analysis. 100k+ citations.

### [Dario Amodei](https://www.anthropic.com/) (Anthropic CEO) - AI safety, alignment
Co-founded Anthropic focused on AI safety research.

### [Jan Leike](https://jan.leike.name/) (Anthropic) - Alignment, scalable oversight
Former head of alignment at OpenAI, now at Anthropic.

## Programs & Organizations

### [MATS Program](https://www.matsprogram.org/) - Mentorship for alignment research
12-week Berkeley-based program connecting scholars with top alignment mentors.

### [Berkeley AI Safety Initiative (BASIS)](https://berkeleyaisafety.com/) - Student group
Student-led AI safety research and education.

### [Redwood Research](https://www.redwoodresearch.org/) - Alignment research org
Bay Area nonprofit focused on alignment techniques.



# ASF Organizations


Alumni have been hired by leading organizations like [Anthropic](https://www.anthropic.com/), [Google DeepMind](https://www.deepmind.com/), [OpenAI](https://openai.com/), [Meta AI](https://www.meta.ai/), [UK AISI](https://www.aisi.gov.uk/), [Redwood Research](https://www.redwoodresearch.org/), [METR](https://metr.org/), [RAND TASP](https://www.rand.org/global-and-emerging-risks/centers/technology-and-security-policy/fellows.html), [Open Philanthropy](https://www.openphilanthropy.org/), [ARC](https://alignment.org/), [FAR.AI](https://far.ai/), [Apollo Research](https://www.apolloresearch.ai/), [Truthful AI](https://www.truthfulai.org/), [LawZero](https://lawzero.org/en), [MIRI](https://intelligence.org/), [CAIF](https://www.cooperativeai.com/foundation), [CLR](https://longtermrisk.org/), [Beneficial AI Foundation](https://www.beneficialaifoundation.org/), [SaferAI](https://www.safer-ai.org/), [Haize Labs](https://haizelabs.com/), [Eleuther AI](https://www.eleuther.ai/), [Harmony Intelligence](https://www.harmonyintelligence.com/), [Conjecture](https://www.conjecture.dev/), [Magic](https://magic.dev/), and the US government, and joined academic research groups like [UC Berkeley CHAI](https://humancompatible.ai/), [NYU ARG](https://wp.nyu.edu/arg/), [KASL](https://www.kasl.ai/), [MILA](https://mila.quebec/en), and [MIT Tegmark Group](https://space.mit.edu/home/tegmark/technical.html).

80% of our alumni are now working in AI alignment.
