.[[AI Safety]].
  [[ASF]]  
  [[ABIO Safety]] 
  [[AIS]] 
AGI Safety

- [[AI Sys Safety]] 




**THE RIGHT TO REALITY**
- MARKED SYNTHETIC - Any synthetic media that could plausibly be mistaken for real must contain tamper-resistant watermarks and metadata indicating it is not real.

- All media viewer must use method A, B, or C to indicate media is synthetic.
- - Intentional or negligent circumvention of these rules results in stiff penalties.





# LOG

## 2025-12-09  Verbal / Constitutional Safety

- safety assess at level of verbal tokens
- relative to some constitutional backdrop


 i’m thinking about a way of conceiving of AI safety in terms of conscious thought and unconscious thought so the idea here is that in an LLM an unconscious thought is really the unfolding of the LLM‘s output given some input. This is unconscious in the sense that the LLM doesn’t really have access to the thinking that it occurred or the processing that occurred during this. Just like human thought we can look at approximate thoughts they come out when inquiring about the reasons, etc. And just like with human thought one can gain some insight into the workings of the LOM are the workings of the unconscious mind, but in both cases, these indirect measures are known to be quite faulty and conclusions drawn can easily be incorrect. By contrast, conscious thought deals with explicit verbal outputs from the mind or from the LLM. In both cases, these outputs don’t have to actually be spoken by the agent merely fought of in an explicit way. The analogy here is that both LLM’s and humans operate in a mode where the unconscious outputs from the LLM or the mind are fed back in to the LLM and the mind to drive further thinking. So in both cases, the conscious thought is the string of those symbols which are explicitly articulated out of the LLM in the stream of thinking in the mind. I’ll ask you tomorrow to summarize this theory of conscious and unconscious thought. Can you write this up as a parsimonious marked down that lays out the argument and the definitions here? I’d like to have a specific document that I can refer back to or get back to later please do that now.




## 2025-10-07  [[AI Safety Action Taxonomy Construction]] 


- Automatic Taxonomy Building: Unsupervised or semi-supervised discovery of latent behavior clusters, used to classify actions or objectives instead of linguistic concepts.
    

## 2025-10-07  AI-SAFETY PAPERS - Intent modeling

---

### Core Papers

1. “Concrete Problems in AI Safety” (Amodei et al., 2016) – Foundational taxonomy of avoidable failure modes such as negative side effects, reward hacking, and scalable oversight.
    
2. “Reward is Enough” (Silver et al., 2021) – Argues that reinforcement learning objectives alone can yield intelligence, framing value alignment as a problem of reward specification.
    
3. “The Alignment Problem from a Deep Learning Perspective” (Christiano et al., 2018) – Introduces reward-modeling and human-feedback methods that evolved into RLHF.
    
4. “Interpretability and the Inner Alignment Problem” (Hubinger et al., 2019) – Distinguishes outer vs inner alignment and warns of mesa-optimizers inside large models.
    
5. “Anthropic’s Circuit Interpretability Work” (Olah et al., 2020–2024) – Empirical reverse-engineering of neuron and circuit semantics; moves interpretability toward mechanistic understanding.
    
6. “ELK: Eliciting Latent Knowledge” (ARC, 2021) – Proposes formal setups for extracting latent world-models from AI systems without relying on behavior proxies.
    
7. “Weak-to-Strong Generalization” (OpenAI & Anthropic, 2023) – Studies whether aligned weak models can supervise stronger successors safely.
    
8. “Scalable Oversight via Debate and Amplification” (Irving et al., 2018–2022) – Examines multi-agent setups (debate, recursive amplification) as scalable human oversight mechanisms.
    
9. “Goal Misgeneralization in Deep Reinforcement Learning” (Langosco et al., 2022) – Demonstrates that agents can internalize unintended goals even under perfect training rewards.
    

---

### Conceptual or Emerging Areas

- Interpretability → Intent Alignment: Transition from static circuit mapping toward dynamic modeling of intentions and causal control in model activations.
    
- Automatic Taxonomy Building: Unsupervised or semi-supervised discovery of latent behavior clusters, used to classify actions or objectives instead of linguistic concepts.
    
- Generalized Action Abstraction: Efforts to describe LLM behavior through an action ontology independent of surface tokens—bridging reinforcement learning and language-model agency.
    
- Causal Representation Learning for Safety: Extracting structured, manipulable causal graphs from model embeddings to verify influence chains and intervention safety.
    
- Evaluation Frameworks for Alignment: Design of benchmarks testing deception, corrigibility, and cooperation beyond traditional accuracy metrics.


