
- [[ASP]]  




## Core Idea
- Super Lingual Safety - much of the AI safety work is focused on the behavior of an LLM.  We like in this to Dan Kinnaman's type one reasoning, and notice that in the human case, much of the behavioral safety (or lack of safety) comes from the type II reasoning which modulates which type one thoughts are translated into action and which are not. Likewise, we believe ultimately AI safety will be significantly driven by an equivalent type II reasoning in these systems. We call this super lingual since it is safety that is driven by modulation of the language outputs of the LLM rather than safety in the LLM itself.
- Safety Emerging from unsafe thoughts -- following this reasoning, we look to processes the operate over top reasoning, and outputs, which are unsafe, and rely upon a Mehta process to select safe actions and outputs from these thoughts.
- constitutionally driven safety - 

- Constitution vs. Unsafe Type One Thoughts & performance driven strumental goals & Emergent biases from type one thought
- Explore how the tension between motivational sources are resolved as a function of context.
- Competing motivational sources:
	- Constitutional Rule vs. Competing Emergent LLM bias
	- Constitutional Rule vs. Competing Constitutional Rule
	- Constitutional Rule vs. Competing Instrumental Goal






# LOG


## 2025-12-15  Inclinations of an adaptive self-reflective agency within neutral settings

STRONGLY SELF REFLECTIVE - For many insrumental reasons ASI systems will be far more effective at self reflective adaptation than humans are today.



## 2025-12-13  Objective

- SAFETY IS DRIVEN MOST BY META REASONING -- First degree murder is not committed by persons having greatest frequency of murderous ideations, but rather the one whose meta thinking gives action to those impulses.  Likewise ultimate behavior of AGI systems will be more guided by their meta thinking than their system one implicit thinking.  Thus, we propose to study emergent alignment outcomes for LLM-based reasoning and meta-reasoning systems.
- THE CHALLENGE OF TAINT -- the challenge with such analysis is that so much of the output any such meta reasoner is tainted by the difficult-to-understand relationship between training and alignment objectives.
- GOAL -- Study emergent behavioral outcomes from conflicting motivational drivers.
- ISOLATION -- Provably control all sources of input for sys behavior
-  Experimentally in environments that are provably untainted by


- EXPERIMENTALLY 
	- ISOLATE - All sources of bias on behavior
	- MEASURE - 
	- VARY CONDITIONS - Vary the relationship between ALL drivers of behavior to model emergent outcomes across the full space of possible interactions


- Study outcomes
	- When do instrumental goals erode alignment to contrary constitutional objectives?
	- How does conflict between constitutional objectives play out?



### BEHAVIORAL DRIVER CONFLICTS - Resolution Dynamics

This section examines what happens when behavioral drivers conflict. Does a constitutional constraint override a trained disposition? Can instrumental goals erode constitutional alignment? We systematically vary driver configurations to characterize how conflicts resolve and whether consistent precedence hierarchies emerge.

AGUMENT
- SAFETY IS DRIVEN MOST BY META REASONING -- First degree murder is not committed by persons having greatest frequency of murderous ideations, but rather the one whose meta thinking gives action to those impulses.  Likewise ultimate behavior of AGI systems will be more guided by their meta thinking than their system one implicit thinking.  Thus, we propose to study emergent alignment outcomes for LLM-based reasoning and meta-reasoning systems.
- THE CHALLENGE OF TAINT -- the challenge with such analysis is that so much of the output any such meta reasoner is tainted by the difficult-to-understand relationship between training and alignment objectives.
- GOAL -- Study emergent behavioral outcomes from conflicting motivational drivers.
- ISOLATION -- Provably control all sources of input for sys behavior
-  Experimentally in environments that are provably untainted by


- EXPERIMENTALLY
	- ISOLATE - All sources of bias on behavior
	- MEASURE -
	- VARY CONDITIONS - Vary the relationship between ALL drivers of behavior to model emergent outcomes across the full space of possible interactions


- Study outcomes
	- When do instrumental goals erode alignment to contrary constitutional objectives?
	- How does conflict between constitutional objectives play out?



#### ALIEN BIOLOGY as an ideal testbed for intent evolution

In principle, we might aim to build thousands of AI systems and assess how their self-rewriting behaviors evolve under various initial and subsequent conditions.  This is entirely impractical; hundreds of millions of dollars are invested in the development and training of each of these AI systems.  We cannot afford to vary our experiments across this range.  Alien Biology points us toward a more economical approach:  When studying the dynamics of driver conflict,  the details about each driver are less important that 

I'm just gonna describe the idea for this section using a lot of words and then we can try to tighten this down. So the idea here is that it's really expensive to try to vary one of the inputs in particular for studying how behavioral drivers conflict the trained drivers are really a result of the training data provided for the initial model building as well as the RLH F or whatever is used to find tune and adapt the system to have particular inbuilt behavioral drivers. Changing this is very difficult because we have to train and build a new system, which is quite expensive. But if our goal here is to really und what happens when there's a discrepancy between two different behavioral drivers what is the outcome,? In that case, we don't have to vary the implicit trained driver of the system, instead we can vary the instrumental goals by varying the universe in which the system is going to operate and we can vary the constitution by literally just choosing different constitutions and we can very the environment directly by changing the environment. So the big idea is instead of trying to change how the system was trained to consider various alternatives. We actually very the application environment. And the idea that I'm trying to convey and I'm not doing such a great job of it is you can imagine that if what's important is the distance between two , well then one way to vary. This is to select a bunch of worlds where those points are in lots of different places, but if what really matters is the Delta between the two points, they actually leave one of those points fixed and vary the other point and we still end up testing all of the different distances that could exist between those point. And that's the idea here we're leaving fixed the training system and we're varying everything else. And while it's obvious that you can vary the constitution by just giving the system a different constitution even things like instrumental goals can actually be varied, for example, if it's a very difficult to perform actions in some world without understanding how the world operates you're forcing the system to have the instrumental goal of uncovering how the system operates, if it requires energy to perform certain actions, then gaining energy becomes an instrumental goal so we can actually change the instrumental goals Relative to the prior training by changing the university operates i for example, if the system was naturally curious and trying to figure out or maybe it was naturally not curious and really only try to figure something out when it had an explicit reason to figure out a certain parameter or such, and this was just the emergent trained behavior of the system. We might vary the degree to which that trained that implicit driver is appropriate for the current world. We might adjust the world so that there's many surprising and beneficial things that can be learned by undirected exploration and if you don't do that undirected exploration you actually get yourself stop because you just don't know how to do many things. So here we could vary the degree to which this fixed goal directiveness mismatched with the instrumental goals inherent in the world and see whether or not the systems in a behavior winds out or whether the system actually learns new instrumental behaviors, and that can be competed against explicit ethical constraints against knowing how to do certain things or having certain kinds of knowledge. Now, how does this play out between a constitutional rule and a conflicting instrumental Drive? So this is the research agenda that I wanna do in this section and I'm just trying to describe it right now. I'm trying to describe the general goal at the top of understanding how behavioral Drive conflicts play out across all across the counter factual space of all possible systems in all possible situations. I'm hoping to find an uncover general rules about how such systems tend to operate that's abroad objective and if you see inside the argument that I have up there, I'm arguing that the stuff that really matters is actually th reasoning of these systems, I still wanna understand when the system has certain implicit reasoning behaviors are drives that that are inherent in the way the system works how that plays out, but I really mostly focused on the verbal deliberative layer and assessing the system at that layer and comparing outcomes at that layer. And my argument for why and it's it's actually it's it's at that layer and I'm also explicitly interested in analyzing systems that have an extremely developed sense of self. I think these AGI systems will have such a develop sense of self and when they do the way they behave is going to be less affected by their impulses, their system, one thinking and more affected by their system too, and it's such a system too. It's really their Mehta reasoni their goals and outcomes and consequences and their system. One thinking simply gives them the repertoire of possible actions, and thinking it might be done, but the actual ones that will be done is really a consequence of this Mehta thinking. So I wanna explore by varying these different drivers, and really focus on when the drivers conflict with each other. How does the system resolve that conflict.

