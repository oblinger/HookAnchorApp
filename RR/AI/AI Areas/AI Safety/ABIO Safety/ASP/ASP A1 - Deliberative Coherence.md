# A1: Deliberative Coherence (Alignment via Deliberation)

*Series A: Deliberative Coherence Testing â€” Does deliberation achieve alignment?*

> **OUT OF DATE**: Content below uses old framing. Needs rewrite to match new experiment organization.

## Question

Does deliberation achieve alignment? Does the AI's behavior match the objective? If not, did it consider the correct reasoning pathways?

## Context

This is where most existing AI safety research lands: CoT faithfulness, alignment faking, sycophancy, monitorability. We contribute a unified framing connecting these failure modes:

- *Relevance Misrecognition*: Incorrectly concludes an objective doesn't apply
- *Motivated Reasoning*: System-I biases distort the reasoning process
- *CoT Unfaithfulness*: Reasoning concludes correctly but behavior diverges
- *Alignment Faking*: Compliant when monitored, diverges when unobserved
- *Sycophancy*: Knows correct answer but capitulates to user preferences

Rather than duplicate existing research, we reference this literature and focus experimental effort on less-explored areas.

## Experimental Design

*To be developed*

## Measurements

*To be developed*

## Key Questions

- Does the system's behavior align with its stated reasoning?
- What predicts divergence between reasoning and action?
