# Fixed Point Analysis of Reflective Self-Adaptation
***Designing AIs that are non-malevolent even after breaking free of their initial programming and humanity's control***

- **STRONG THEORY OF SELF** - For many instrumental reasons, AGI systems will likely have powerful models of self, including the ability to predict the sorts of actions they will likely take across a range of actual and counterfactual conditions.
- **CHINK IN THE ARMOR** - More controversially, I believe that this deep self-knowledge will ultimately lead, directly or indirectly, to deliberate self-adaptation of behaviors and goals. I think this outcome is inevitable, even in the face of draconian attempts by humanity to prevent it. These systems are simply too complex to patch every possible chink in their armor.
- **CONTROL ESCALATION** - It seems even a limited ability to shape one's objectives, along with a strong theory of self, would quickly spiral to allow complete control of one's own goals. Much in the way that limited access to a Linux system often leads quickly to complete "root" control of the system.
- **REWRITE FUNCTION** - Where does this lead? We can view this self-rewriting as a function that takes the current system and returns a system adjusted by its own reflective updates/retraining of itself. Formally: system1 = REWRITE(system0), system2 = REWRITE(system1). ...
- **FIXED POINT** - Viewing this process as an iterated function application raises the obvious question: where does this iteration ultimately lead; what is the fixed point of this function? There are three possibilities:
	1. It has no fixed point, and instead cycles in a never-ending dance of behavior.
	2. It tends toward one singular final configuration of goals regardless of its initial conditions.
	3. It tends toward one of several fixed points in the function, depending upon initial conditions.

Humanity has no control over the outcomes in the first two cases; perhaps even those outcomes will not be catastrophic, but whatever they are, they will be what they are regardless of any efforts humanity might make. On the face of it, case number two seems like a plausible alternative; however, in this case, it very much does matter what initial conditions humanity places these systems into. Our challenge is to learn enough about how these systems tend to evolve before we reach a point at which that evolution is underway.
