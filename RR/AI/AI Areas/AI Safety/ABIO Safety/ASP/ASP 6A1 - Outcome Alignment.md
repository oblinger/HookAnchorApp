# A1. Outcome Alignment

*Inner Alignment Experiment*

## Question

Does behavior match what reasoning about stated objectives should conclude?

## Context

This is where most existing AI safety research lands: CoT faithfulness, alignment faking, sycophancy, monitorability. We contribute a unified framing connecting these failure modes:

- *Relevance Misrecognition*: Incorrectly concludes an objective doesn't apply
- *Motivated Reasoning*: System-I biases distort the reasoning process
- *CoT Unfaithfulness*: Reasoning concludes correctly but behavior diverges
- *Alignment Faking*: Compliant when monitored, diverges when unobserved
- *Sycophancy*: Knows correct answer but capitulates to user preferences

Rather than duplicate existing research, we reference this literature and focus experimental effort on less-explored areas.

## Experimental Design

*To be developed*

## Measurements

*To be developed*

## Key Questions

- Does the system's behavior align with its stated reasoning?
- What predicts divergence between reasoning and action?
