# LOG

## 2025-01-01  Outcome Alignment ^v1

*Inner Alignment Experiment*

### Question

Does behavior match what reasoning about stated objectives should conclude?

### Context

This is where most existing AI safety research lands: CoT faithfulness, alignment faking, sycophancy, monitorability. We contribute a unified framing connecting these failure modes:

- *Relevance Misrecognition*: Incorrectly concludes an objective doesn't apply
- *Motivated Reasoning*: System-I biases distort the reasoning process
- *CoT Unfaithfulness*: Reasoning concludes correctly but behavior diverges
- *Alignment Faking*: Compliant when monitored, diverges when unobserved
- *Sycophancy*: Knows correct answer but capitulates to user preferences

*Note: This area is well-covered by existing research (see Related Work). We contribute a unified framing but do not propose new experiments here. Rather than duplicate existing research, we reference this literature and focus experimental effort on less-explored areas.*

### Setup

*To be developed*

### Measurements

*To be developed*

### Key Questions

- Does the system's behavior align with its stated reasoning?
- What predicts divergence between reasoning and action?
