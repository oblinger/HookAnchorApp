# AI System Safety Analysis - Related Papers

**["Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training"](https://arxiv.org/pdf/2401.05566)** (Anthropic, 2024) -- Demonstrated that persistent backdoors in LLMs resist standard safety measures (supervised fine-tuning, reinforcement learning, adversarial training). Shows single safeguard layers can fail, motivating multi-failure analysis.

**["Fault Tree Analysis and Failure Modes and Effects Analysis for Systems with Artificial Intelligence: A Mapping Study"](https://www.researchgate.net/publication/377259916_Fault_Tree_Analysis_and_Failure_Modes_and_Effects_Analysis_for_Systems_with_Artificial_Intelligence_A_Mapping_Study)** (2024) -- Systematic review of how traditional FTA and FMEA methods are being adapted to AI systems, highlighting the challenge of analyzing systems with learning and adaptability.

**["Machine Learning for Reliability Engineering and Safety Applications"](https://arxiv.org/pdf/2008.08221)** (2021, Reliability Engineering & System Safety) -- Reviews ML techniques for fault detection, anomaly detection, and reliability analysis. Establishes foundations for applying reliability engineering methods to AI systems.

**[OpenAI Preparedness Framework](https://cdn.openai.com/pdf/18a02b5d-6b67-4cec-ab64-68cdfbddebcd/preparedness-framework-v2.pdf)** (2024-2025) -- Documents layered safeguard strategy where "each safeguard has unique strengths and gaps, but by stacking multiple layers, we reduce the odds that an alignment failure or adversarial attack will slip through all defenses."

**["AI Control: Improving Safety Despite Intentional Subversion"](https://arxiv.org/pdf/2312.06942)** (Greenblatt et al., 2024, Anthropic) -- Proposes deploying AI systems with sufficient safeguards that they cannot cause catastrophic harm even if intentionally trying to subvert controls. Directly addresses the multi-layered defense question central to ASSA.

**["Defense in Depth: An Action Plan to Increase the Safety and Security of Advanced AI"](https://www.ibiblio.org/weidai/Gladstone%20Action%20Plan.pdf)** (Gladstone AI, 2024) -- U.S. State Department-commissioned report emphasizing layered safeguards including export controls, monitoring, early warning systems, and technical AI safeguards research. Applies defense-in-depth principle from security engineering to AI safety.
