
AGI Safety

- [[AI Sys Safety]] 




**THE RIGHT TO REALITY**
- MARKED SYNTHETIC - Any synthetic media that could plausibly be mistaken for real must contain tamper-resistant watermarks and metadata indicating it is not real.

- All media viewer must use method A, B, or C to indicate media is synthetic.
- - Intentional or negligent circumvention of these rules results in stiff penalties.





# LOG

## 2025-10-07  [[AI Safety Action Taxonomy Construction]] 


- Automatic Taxonomy Building: Unsupervised or semi-supervised discovery of latent behavior clusters, used to classify actions or objectives instead of linguistic concepts.
    

## 2025-10-07  AI-SAFETY PAPERS - Intent modeling

---

### Core Papers

1. “Concrete Problems in AI Safety” (Amodei et al., 2016) – Foundational taxonomy of avoidable failure modes such as negative side effects, reward hacking, and scalable oversight.
    
2. “Reward is Enough” (Silver et al., 2021) – Argues that reinforcement learning objectives alone can yield intelligence, framing value alignment as a problem of reward specification.
    
3. “The Alignment Problem from a Deep Learning Perspective” (Christiano et al., 2018) – Introduces reward-modeling and human-feedback methods that evolved into RLHF.
    
4. “Interpretability and the Inner Alignment Problem” (Hubinger et al., 2019) – Distinguishes outer vs inner alignment and warns of mesa-optimizers inside large models.
    
5. “Anthropic’s Circuit Interpretability Work” (Olah et al., 2020–2024) – Empirical reverse-engineering of neuron and circuit semantics; moves interpretability toward mechanistic understanding.
    
6. “ELK: Eliciting Latent Knowledge” (ARC, 2021) – Proposes formal setups for extracting latent world-models from AI systems without relying on behavior proxies.
    
7. “Weak-to-Strong Generalization” (OpenAI & Anthropic, 2023) – Studies whether aligned weak models can supervise stronger successors safely.
    
8. “Scalable Oversight via Debate and Amplification” (Irving et al., 2018–2022) – Examines multi-agent setups (debate, recursive amplification) as scalable human oversight mechanisms.
    
9. “Goal Misgeneralization in Deep Reinforcement Learning” (Langosco et al., 2022) – Demonstrates that agents can internalize unintended goals even under perfect training rewards.
    

---

### Conceptual or Emerging Areas

- Interpretability → Intent Alignment: Transition from static circuit mapping toward dynamic modeling of intentions and causal control in model activations.
    
- Automatic Taxonomy Building: Unsupervised or semi-supervised discovery of latent behavior clusters, used to classify actions or objectives instead of linguistic concepts.
    
- Generalized Action Abstraction: Efforts to describe LLM behavior through an action ontology independent of surface tokens—bridging reinforcement learning and language-model agency.
    
- Causal Representation Learning for Safety: Extracting structured, manipulable causal graphs from model embeddings to verify influence chains and intervention safety.
    
- Evaluation Frameworks for Alignment: Design of benchmarks testing deception, corrigibility, and cooperation beyond traditional accuracy metrics.


