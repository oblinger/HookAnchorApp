
### 2024-12-18  DETRs Beat YOLOs on real-time object detection

https://arxiv.org/abs/2304.08069



Outcome-supervised Reward Models (ORMs) -- trained using final result of the model’s chain-of-thought,
Process-supervised Reward Models (PRMs) --  receive feedback for each step in the chain of thought.


Our main contributions are as follows:
1. We show that process supervision can train much more reliable reward
models than outcome supervision. We use our state-of-the-art PRM to
solve 78.2% of problems from a representative subset of the MATH test
set.
2. We show that a large reward model can reliably approximate human supervision for smaller reward models, and that it can be used to efficiently
conduct large-scale data collection ablations.
3. We show that active learning leads to a 2.6× improvement in the data
efficiency of process supervision.
4. We release our full

