



## Tokenizer / Embedding Layer Training

- Train Word Embeddings - 


## Transformer Backbone Training (Core Model Training)

1. **Train Attention Layers** – Optimize self-attention mechanisms (e.g., scaled dot-product attention, multi-head attention) to learn contextual relationships.
**Train Feedforward Layers** – Train MLP layers in each transformer block to capture non-linear transformations and feature extraction.
Train Normalization & Regularization Layers – Optimize layer normalization, dropout, and weight decay to prevent overfitting and improve stability.