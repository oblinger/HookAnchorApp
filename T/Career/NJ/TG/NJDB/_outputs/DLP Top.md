| name | dlp | dlproj1 | dlproj2 | dlproj3 | dlproj4 | dlproj5 | dlproj6 | dlproj7 | dlproj8 | dlproj9 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| [[Anthropic]] |  | Claude – a next-generation AI assistant by Anthropic, trained to be helpful, honest, and harmless, capable of complex conversational and text-processing tasks ￼. It can even handle extremely long context (100K-token inputs, ~75,000 words) after a recent upgrade ￼.<br>Claude – a next-generation AI assistant trained to be helpful, honest, and harmless, capable of a wide range of conversational and text tasks<br>Claude is a family of large language models developed by Anthropic as a competitor to ChatGPT, first released in March 2023 [oai_citation_attribution:28‡en.wikipedia.org](https://en.wikipedia.org/wiki/Claude_(language_model)#:~:text=Claude%20is%20a%20family%20of,was%20released%20in%20March%202023) | Constitutional AI – Anthropic’s alignment technique that guides AI behavior by a set of principles in a “constitution.” Claude is tuned to follow high-level norms (inspired by documents like the U.N. Declaration of Human Rights) to produce helpful, harmless outputs ￼.<br>Constitutional AI – an alignment method where an AI model self-critiques and improves its responses using a set of principles as a 'constitution' to reduce harmful outputs [oai_citation_attribution:0‡anthropic.com](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback#:~:text=As%20AI%20systems%20become%20more,model%20to%20evaluate%20which%20of)<br>(Not applicable). | 100K Context – Anthropic expanded Claude’s context window from 9k to 100k tokens, allowing it to ingest and analyze hundreds of pages or hours of information in a single query ￼. This long-context version can, for example, read an entire book and answer detailed questions in seconds. ￼<br>Claude and Alexa+ – collaboration with Amazon integrating Anthropic's Claude models into Alexa, bringing advanced AI capabilities to the voice assistant with Anthropic’s safety measures [oai_citation_attribution:1‡anthropic.com](https://www.anthropic.com/news/claude-and-alexa-plus#:~:text=Today%2C%20we%27re%20announcing%20that%20Claude,and%20consumers%20around%20the%20world)<br>(Not applicable) |  |  |  |  |  |  |
| [[Apple]] |  | AXLearn – Apple’s open-source JAX-based training framework for large-scale AI, enabling efficient training of foundation models on distributed accelerators ￼ | Neural TTS (Siri Voice) – on-device text-to-speech system using deep learning (Tacotron2 + WaveRNN) with optimizations to run 5× faster than real time on server and 3× on an iPhone, producing natural 24 kHz speech for Siri ￼ | Personal Voice – new iOS 17 accessibility feature that fine-tunes a speech model on 15 minutes of a user’s voice on-device, then generates a custom synthetic voice matching the user for use in FaceTime or Assistive Communication ￼ | Face ID Neural Engine – Apple’s on-device deep learning system (with a dedicated neural chip) that generates a depth map and recognizes users’ faces for authentication, using dual neural networks to adapt to appearance changes (applied DL in security)【n/a】 | Diffusion in Pixelmator – Apple collaborated to integrate Core ML–accelerated Stable Diffusion for image generation on Mac, enabling creators to generate art from text prompts locally using Apple Silicon optimizations【n/a】 | ** - ** | ** - ** | ** - ** |  |
| [[Google]] |  | Transformer (2017) – novel neural network architecture that became the basis of most modern generative AI models ￼ | LaMDA – family of Transformer-based dialogue models (up to 137B parameters) specialized for open-ended conversation ￼ | PaLM – 540B-parameter Pathways Language Model that significantly advanced state-of-the-art across many NLP and coding tasks via scaling ￼ | Imagen – text-to-image diffusion model with unprecedented photorealism and deep language understanding ￼ | MusicLM – text-to-music generation model producing high-fidelity songs from natural language descriptions ￼ | Magenta – open-source research project exploring machine learning in the creative process (generating art and music) ￼ | Google Duplex – AI system for natural phone conversations that carries out real-world tasks (e.g. making reservations) with human-like speech ￼ | TensorFlow – end-to-end open-source deep learning platform that lets researchers and developers build and deploy models at scale ￼ |  |
| [[Inflection AI]] |  | Pi (Personal AI) – an AI chatbot that serves as a friendly personal assistant. Inflection launched Pi in 2023 as a ChatGPT alternative, powered by its in-house large language model. The first version used the Inflection-1 model (trained on ~4% of the compute of GPT-4), and a later Inflection-2.5 model (trained with 10× more compute) nearly matched GPT-4 on many tasks ￼ ￼.<br>Pi (Personal AI) – a conversational AI companion designed to be supportive and readily available, providing advice and information in a friendly, flowing dialogue style [oai_citation_attribution:2‡businesswire.com](https://www.businesswire.com/news/home/20230502006113/en/Inflection-AI-Introduces-Pi-Your-Personal-AI#:~:text=PALO%20ALTO%2C%20Calif.,in%20a%20natural%2C%20flowing%20style) | Inflection-1 LLM – Inflection’s proprietary large language model series. The 1st-gen model (Inflection-1) was trained on 1.5 trillion tokens and the latest Inflection-3.0 was deployed for enterprise. Inflection open-sourced 3B & 7B versions for research ￼. These models power Pi and can generate text or code, enabling a range of applications.<br>Massive AI Cluster – Inflection (with CoreWeave and NVIDIA) is building the world’s largest AI training cluster (22,000 H100 GPUs) to power its advanced large language models, supporting Pi and other products | Intel Collaboration – Inflection AI partnered with Intel to deliver its models to enterprises. They announced an Inflection 3.0-based appliance using Intel’s Gaudi3 AI accelerators ￼. This Inflection for Enterprise system runs Inflection’s latest model on-premises with high efficiency, offering tailored chatbot and instruction-following AI services for business use ￼.<br>Inflection for Enterprise – an enterprise-grade AI system (Inflection 3.0 model) launched with Intel’s support, allowing companies to own and fine-tune a dedicated LLM appliance on Intel Gaudi hardware for secure, high-performance generative AI [oai_citation_attribution:3‡businesswire.com](https://www.businesswire.com/news/home/20241007441972/en/Introducing-Inflection-for-Enterprise#:~:text=PALO%20ALTO%2C%20Calif.,for%20the%20world%E2%80%99s%20largest%20enterprises) |  |  |  |  |  |  |
| [[Meta (Facebook)]] |  | PyTorch – open-source deep learning framework that accelerates the path from research prototyping to production deployment ￼ | LLaMA – family of foundation LLMs (7B–65B params) released for research, enabling state-of-the-art open models in the community ￼ | Segment Anything (SAM) – image segmentation model + 1 B mask dataset that can segment any object from a prompt, democratizing segmentation research ￼ | CICERO – first AI agent to achieve human-level performance in the strategy game Diplomacy by combining strategic reasoning with natural language negotiation ￼ | NLLB-200 – “No Language Left Behind” 200-language translation model that achieves far higher quality on low-resource languages than prior technology ￼ | AudioCraft – open-source suite (MusicGen, AudioGen, EnCodec) for text-to-audio generation, enabling high-quality music and sound effect creation from text prompts ￼ | OPT-175B – Open Pre-trained Transformer (175B param) released by Meta, matching GPT-3-level performance with a model open to academic use ￼ | BlenderBot 3 – a 175B-parameter open-domain chatbot (built on OPT) that learns from conversations “in the wild” and internet access to improve its skills and safety ￼ |  |
| [[Microsoft]] |  | MT-NLG 530B – Megatron-Turing NLG, a 530-billion–parameter Transformer LM (co-developed with NVIDIA) that set a new standard for large-scale natural language generation ￼ | Kosmos-1 – multimodal large language model that can perceive images and text, follow instructions, and perform visual question answering and reasoning ￼ | VALL-E – neural codec text-to-speech model that can synthesize a person’s voice from a 3-second sample, enabling zero-shot personalized speech generation with high fidelity ￼ | NUWA-Infinity – multimodal generative model that produces arbitrarily high-resolution images or long-duration videos from text or image inputs (via a two-stage autoregressive approach) ￼ | ResNet (Deep Residual Nets) – ultra-deep CNN (152 layers) by MSR that won ImageNet 2015, using “residual” connections to train networks 5× deeper than previously possible ￼ | DeepSpeed – easy-to-use deep learning optimization library (open-sourced by Microsoft) that enables unprecedented scale (trillion+ parameter training) and speed for model training/inference ￼ | BioGPT – domain-specific generative Transformer (by Microsoft Research) trained on large biomedical corpora for text generation and mining in the biomedical domain ￼ |  |  |
| [[NVIDIA]] |  | StyleGAN – GAN architecture (2018) from NVIDIA that generates high-resolution photorealistic images (e.g. human faces) with controllable style attributes, advancing the state of generative adversarial networks ￼ ￼ | GauGAN – deep learning model (named after artist Gauguin) that turns simple doodles or segmentation maps into stunning, lifelike landscape images, using GAN-based image synthesis with breathtaking ease ￼ | eDiff-I – “Ensemble of Diffusion Models” by NVIDIA Research, a next-gen text-to-image diffusion model that offers unprecedented fidelity and alignment between text and image, with capabilities like instant style transfer and intuitive edits ￼ | Instant NeRF – Neural Radiance Fields implementation that uses a fast multiresolution hash encoding to reconstruct 3D scenes from 2D images extremely quickly, allowing photorealistic novel view synthesis in seconds (Time’s Best Invention 2022) ￼ | Megatron LM – NVIDIA’s framework for training very large transformer models (basis of MT-NLG), which introduced efficient model-parallel and pipeline-parallel techniques to scale generative LMs to hundreds of billions of parameters ￼ | ** - ** | ** - ** | ** - ** | ** - ** |
