
### tiny intro

The Uniform Agenda aims at a particular way to break-down and build-up software.  We argue systematic use of this approach would yield transformative improvements in software development efficacy.  We start describing a small slice of this agenda, and use that that slice to explain why we believe JSON gained so much ground against XML even at a time when the XML tool chain was already well established, the standard and technology were very mature, and nearly universally supported across most languages and frameworks.  Even today, when people compare these alternatives they often focus on their surface form, parsing efficiency, etc. as the driving difference.  Here we describe what we believe is the difference that drove adoption, and it is also one of the key ideas underlying the Uniform Agenda.  Here we define, language, generality, embedability, essence of, and use these to understand JSON vs XML in a new way:


_
### LANGUAGE FRAMING

Here we define a few familiar terms that underlie our framework:

**PROBLEM** -- A _**behavior**_ (also called a "_**problem**_") is a specific function to be computed, data to be represented, or desired order-dependent input/output pattern that is systematically defined over wide range of in-out sequences.


**PROBLEM CLASS** -- A _**problem class**_ or _**capability**_ is a set of behaviors that represent a nameable, describable grouping of related problems/behaviors.

For example, "sorting" or "reactive GUI scripting" might be two such classes.


**LANG/CODE/VALID** -- A software _**language (LANG)**_ is anything that generates some systematic behavior given _**source code (CODE)**_ that is _**valid**_ for that language.  Formally:
	
	_behavior_ = BEHAVIOR(_code_, _lang_)   **given**   _code_ in VALID(_lang_)


**RANGE** -- The range for a language is the set of behaviors it that can be generated by that languages given all valid source code inputs for that language.  Formally:

	_behavior_ in RANGE(_lang_) **iff** 
		**there exists** _code_ in VALID(lang) **where**
			_behavior_ = BEHAVIOR(_code_, _lang_)
	
_
### GENERALITY

**GENERALITY** -- We say that one language L1 is _**more general**_ than L2 iff RANGE(L1) is a superset of RANGE(L2)

All other goodness measures being equal one language being more general than another makes it "better" than the other language.  

_
### NATURAL

At one level we can evaluate code purely based upon functionally what behavior results from that code.  That is an important consideration, but it is not the only consideration when choosing between different languages that might be used to produce some common behaviors.  After all both python and Turing machines are are Turing complete, yet when we look on GitHub we see few solutions developed on top of Turing machines.  Part of that is a performance issue, but another huge consideration is how "natural" those solutions feel for the software engineers who must deal with them.  Here we define this notion of naturalness:

One approach is to use the same one the supreme court used in defining pornography:  We dont need to define it, one just knows it when one sees it.  Yes, at least is the extreme cases engineers generally can agree on what is an is not a natural solution for generating some particular behavior.  Still we offer the following thought experiment as a slightly more concrete specification of this notion of naturalness:  In a nutshell some code, c, that is part of some larger software system is "natural" if software engineers are not inclined to refactor it when given the opportunity to do so.  We frame it this way, in order to avoid looking for 'perfect' solutions to problems, and rather we call a thing "natural" in any case that it is not so painful to deal with that engineers would take the step of refactoring it.  This defaulting towards calling things natural unless painful to deal with is the bias that we are aiming for in our definition.


**NATURAL** -- We say some code, c, is natural if software engineers are NOT inclined to refactor c:
- given ownership of the software system, S, containing c
- given the time and money resources to perform the refactoring, and 
- given a mandate to maintain and extend, S, over a variety of future behaviors.


Notice in this framing, c, is explicitly part of some larger system. Thus refactoring might include throwing out the language that c is written in and choosing a different language for that part of S.

All other things being equal one (obviously) prefers "natural" code over code that is not natural.

_
### EMBEDABILITY 

Lang A embeds into Lang B iff
	There exists a deterministic mapping function, M, such that:
		Forall a in VALID(A), There exists b in VALID(B) where b = M(a) and b "corresponds" to a

Different interpretations of this notion of "corresponds" results in different levels of embedability:
- **FUNCTIONAL EMBEDDING** -- the weakest form of embedding merely considers if there is any element b that provides the same functionality as the element a.  I.e.  BEHAVIOR(a, A) = BEHAVIOR(b, B).  This weakest form of embedding indicates that functionally anything one can do with the language A one can in principle do with language B.
- **NATURAL EMBEDDING** -- the next level strengthens correspondence to not only require functional equivalency, but to further requires that NATURAL(a) implies NATURAL(b).  That is, problem solutions which are natural in the first language map to natural solutions when mapped to the second language.
- **DETERMINISTIC EMBEDDING** -- the highest level of embedding is one where not only is the result functional and natural, but it is also _deterministic_ in the sense that multiple engineers given the spec for A and B, along with an element a in A, will themselves agree without coordination on a single specific element of b that corresponds to a.  Formally:  a "corresponds" to b iff b=M(a)


These three levels of embedding indicate, in progressively more restrictive ways, that one language can "fit into" another language.



_---PERHAPS THIS TEXT CAN BE REMOVED NOW THAT WE HAVE THE DEFINITION OF NATURAL ABOVE???---_
In some ways, we are going to treat this notion of "natural encoding" a bit like the US supreme court decided to treat the concept of pornography.  We are not going to try to precisely define the term, but instead argue we don't need to, since we all know it when we see it.  In the same way, we are not defining the notion of "natural encoding" as we are arguing that software developers all "know it when the see it."

But as a rough way to think about the idea, we can use the following thought experiment as a rough definition for this notion of natural encoding.  Consider a team of software engineers that are given (1) ownership of a particular software system, (2) the requirement to then considerable future work extending/using the system, and (3) unlimited resources to refactor the system.  We say an aspect of this software system is a "natural encoding" of the problem it solves for that software system, if those engineers are NOT inclined to refactor that part of the system.  (perhaps by inventing a new language to better represent that part, or such.)  

Notice "natural encoding" is not trying to say the particular choice made was in some sense the absolute best in the universe choice or is in some way perfect.  Only that it fits its task so well, there is no pragmatic reason to fix it FOR ANY REASON.  So a system that used XML to encode some configuration parameters would be a natural encoding of that information, and so would a JSON encoding.  You know, tomAAAto / tomAHto, one engineer might have their favorite encoding, but at a practical level both are getting the job done, and neither is so much better than the other that it one would justify a refactor (unless some other reason was driving the refactoring).

But encoding configuration parameters as a vector of bytes, really might get refactored.  Each time you changed the parameters you would need to mess around with the opaque block, debugging errors when the wrong version of the data was being read in, would be a mess, etc.  So all of these functionally encode the problem at hand, but only the first two would be resistant to refactoring, so they are the only ones that are natural encodings.


_
### ESSENCE

In a way generality is about how "big" a language is, while, and embedability is about how "small" it is.  Using these together we can define a kind of "goldilocks" idea of being big enough in all the right ways, but not too big in order to exactly capture "the essence" a class of problems.

**ESSENCE** -- a lang, L, captures the _**essence**_ of a problem class, PC, iff 
	RANGE(L) contains PC,
	there does not exist L2 where 
		RANGE(L2) contains PC,
		L naturally embeds L2, and
		L2 does not naturally embed L

The idea is that a language captures the "essence" of a problem class of if it is _a_ smallest (most embeddable) language that is big enough to fully cover that class of behaviors.  Notice there could be several languages that each naturally embeds the others, and in this case all of those languages are said to capture the essence of the problem class.  This definition gets at this idea of being big enough and small enough in all the "right" ways, but it is very hard to reason with this definition since it is quantifying over all possible languages, and it might include "weird" languages whose range covers the problem class, but that are not at all useful for software engineers to use.

Therefore, for this reason, we extend this notion of essence to ALSO require natural embedability into those languages the software engineers already use for this class of behaviors:

**ESSENCE-FOR** -- a lang, _L_, _**captures the essence**_ of a problem class, _PC_, _**for**_ targeted languages, _T1, ..., Tn_, iff 	
	_L_ captures the essence of _PC_, and 
	For all  _Ti_ in {_T1, ..., Tn_}  _Ti_ naturally embeds _L_


The idea here is that not only is _L_ the goldilocks right size and shape for this problem class, but solutions encoded using _L_ can systematically be translated into ALL these other existing 'targeted' languages _Ti_ in a natural way.  Thus a programmer who understands both _L_ and some _Ti_ can easily utilize _L_ as an alternate form, such that solutions in _L_ will always naturally map back into their favored language.


_
### THE EXAMPLE OF XML AND JSON

With all of this machinery in place we now turn back to the XML-JSON example, both to make these abstract ideas more concrete and to use this machinery to provide one explanation for JSONs strong adoption over time.  We start by first noticing a number of reasons why one might have expected JSON to NOT have had large adoption when it was introduced:
1. WELL ESTABLISHED.  At that time, XML was already extremely well established.  
2. MORE EFFICIENT.  XML paring was specifically designed to be very efficient to parse, so XML parsers can be faster than JSON parsers.
3. MORE POWERFUL.  JSON is not more powerful in some way.  There is nothing that JSON does not do anything that XML cannot not also do.  Additionally, XML handles many pragmatically important cases like binary data, typed data objects, schema validation, and it can handle textual markup in ways that JSON cannot touch.
4. HIGH APP DOMINANCE.  XML was already used across a huge number of applications, and served as the basis for many interface standards that are themselves recursively defined in term of the XML standard.  
5. HIGH NETWORK EFFECTS.  In the world of data interchange languages, network effects are _**king**_.  A later _better_ standard often cannot penetrate into use cases where infrastructure is already built on an inferior earlier standard.  From that perspective, XML was a fortress!

Established XML toolchains, interfaces/languages built on it, massive adoption, and JSON is less powerful, and less efficient to parse.  _This is a slam dunk!_  No way for JSON to make real headway.  So why did it?!?


To understand what happed here we introduce a well understood but not generally named class of behaviors:  communication of tree-structured-object-data-values.  Many modern software systems are built on some kind of OO-style objects.  The data in these objects are typically referenced to other objects, or are simple numeric or string values.  Getting that information from one memory space (say from server-to-client in web programming) is an extremely common software 'chore' to be done.  There are fancy object serialization packages one can use, but often web programmers resist the complexity of these as they introduce tight linkages and versioning issues between different client-server software stacks which in many cases are not even using the same base languages.  Instead they opt to transmit (and sometime maintain) data that will be shared as recursive data containers that terminate in numbers and strings.  This data form is independent of language, and is often more independent of specific versioning of the endpoint software as well.  In simple cases, programmers often opt to just keep certain transmission-oriented data, in this data-value form (as lists and maps of strings and numbers) and avoid the use of the OO paradigm for that kind of data.  Thus communication of tree-structured-data-values-transmission is the problem of encoding such data to it can be transmitted: 

For precision lets define this tree-structure-data-values problem class:
TREE STRUCTURED DATA VALUES PROBLEM CLASS -- The tree structure data values (TSDV) problem class is the class of all losslessly encoded finite, recursive structures of strings and numbers where each sub container is indexed from its parent container "by-position" (using a natural number, like a list index), or "by-name" (using a string value, like an OO-field identifier).  Notice this definition is very close to what JSON can express, but of course XML can express such data quite nicely as well.  

For our analysis of XML and JSON here, we want to pick a specific list of languages to "target" as well.  Here we define "modern dynamic langs" as the ten dynamic languages having the largest number of active repositories currently listed on GitHub:
  MODERN_DYNAMIC_LANGS = {JavaScript, Python, Ruby, TypeScript, C#, Objective , R, Swift, Clojure, Lua}

Now we can assert our key claim:
	JSON captures the essence of the TSDV problem class for MODERN_DYNAMIC_LANGS, while
	XML does not capture the essence of the TSDV problem class for MODERN_DYNAMIC_LANGS

First let just convince ourselves these assertions are correct given our definitions above.  The TSDV problem class (encoding strings and numbers into containers index by position or name) is easily handled by both JSON and XML.  It is quite straight forward to encode such data in both formats.  Second, we see that JSON embeds naturally into XML.  We can define a simple mapping, M(), from all JSON expressions onto XML expressions such that for all target problems, p, if we had a natural encoding json_p for that problem, it would yield an XML instance xml_p = M(json_p) that would also be a natural solution for the original p problem.  (The typical mapping is to use XML ordering to encode lists, and use XML subtags for by-name sub fields in JSON that may contain recursive containers, and to reserve the XML key/value notation for by-name sub-fields that only contain simple string-like data values).  Anything JSON can do, XML can do too, and do it in a very parallel way.

But the reverse is not true.  XML has specific features to encode textual data, for example, which JSON does not share.  Consider encoding the following rich text using XML markup:
	This _simple_ example is a 3^rd level title right here.

In XML this is:  <h3>This <i>simple</i> 3<sup>rd</sup> level title right here.</h3>

What might this look like in JSON?  Perhaps:
	{type='header', level=3, contents= [
		"this", {type='italic', contents=[
			"simple"]},
		"3", {type='superscript', contents=["rd"]} 
		"level", "title", "right", "here."]}

JSON does _functionally_ encode this rich text information, but it is no surprise that it does not _naturally_ encode this data.  An engineer that was required to maintain and update 100K of markup text expressed using such JSON would refactor that garbage immediately!  It is just not a natural representation for such markup.  This is no surprise, JSON was not designed to handle such markup, while, XML on the other hand, is as a simplification of SGML which centrally _was_ focused on such text-related problem types.  Thus we see that XML does not embed into JSON since natural solutions in XML may not be natural solutions in JSON.  According to our definitions above JSON might capture the essence of the TSDV problem while XML could not capture its essence, since JSON is closer to a perfect match.

The second part of the assertion above is a claim that if an encoding of some data in natural in JSON, then it will continue to be natural when systematically translated into each of the MODERN_DYNAMIC_LANGS listed above.  We see this is true since each of these languages have some kind of "container" datatype that provides natural ways to encode recursive by-position and by-name containers of strings and numbers (as lists and maps).  The mapping of JSON onto these container types provides almost a direct one-for-one mapping.  (There is some complexity since these languages may provide overlapping numeric form, thus some flag must be set, but for each problem subclass there generally are natural choices to be made for this decision.)  Thus together we see that JSON seems to capture the essence of the TSDV problem for modern languages while XML does not.  (We say "seems to" since there is always a possibility that some even more embeddable language could exist, but this seems unlikely, as we seem to be fully utilizing all aspects of JSON in order to encode these data values.  Still it is usually difficult to prove a negative, so assertions of essential capture tend to be tentative assertions.)


Ok, so XML is good for rich text data and JSON is not, thus it is does not capture the essence according to our definitions above.  Fine.  Who cares?  Why does this matter?  To answer, consider the following two Python code snipits:
	
	native_collections_tree = json.loads( json_encoded_string )
	json_encoded_string = json.dumps( native_collections_tree )

Similar API call pairs exist for all of these modern dynamic languages.  These two calls ALONE are often all one needs when dealing with JSON data!  This API is beautiful, elegant, and covers all functionality that is often needed.  This API is so simple there is generally little need to even read documentation!  Its usage is intuitive even when arbitrarily complex recursive structures of lists and maps are involved.

Surely programmers would want this same simplicity for XML-parsing into these dynamic languages too.  All XML parsing API must also give such simple access as well right?  WRONG!  Generally there are DOM or SAX style parsers that parse XML into multiple speciality XML class objects with hundreds of specialized XML access methods.  These parsing libraries dozens or hundreds of pages of documentation.  And one cannot learn to uses these APIs without reference to these docs.  Wow!  Quite a difference.  Now of course those XML libraries are intended to do more than JSON libraries are intended to do.  So those APIs must be larger.  They can often perform stream processing, data validation, automatic object marshaling, etc.  So it's not a fair comparison.  

Still for the programmer that just wants to map simple recursive data-values structure from one place to another why not _also_ provide the single-call-simplified-API for XML as well?  Surely the programmers sometimes want that too.  Yes they DO want it!  Indeed we argue that is what has driven the adoption of JSON even as XML was already well supported.  So why not just add those two more call to the XML APIs, XML would have it all!

The problem is, while XML _functionally_ embeds into these container types in these languages, it does not _naturally_ embed into them.  Consider how this should be mapped:  '<list> one two three </list>' Maybe as a list?  That seems right, but then what about: '<h1 id="702"> one two three </h1>'  That also seems like a list too, but then where does the "h1" type info go?  and what about the "702" id?  Maybe one can argue that:
	{type: 'h1', id: '702', contents=["one", "two", "three"]}
is a "natural" encoding, given the complexity of the data that is being encoded.  Ok, but then our original example becomes:
	{type: 'list', contents=["one", "two", "three"]}
No Python programmer would want to operate over recursive versions of a list of list of these things when they could just operate on a lists of lists instead!  Construction, navigation and everything will just be messy.  The problem here is we are pushing the bubble under the carpet, but we cannot get rid of it.  There is **NO** natural XML to Python mapping that is definable since XML does not "fit" into Python lists and maps.  Thus the Python-XML libraries cannot provide those to beautiful load/dump methods... they are just not well defined.

JSON, on the other hand, is a goldilocks representation if one wishes to serialize TSDV between client and server, just as is very often needed in web programming.  And notice that goldilocks status is not a small difference, it is the difference between dozens or hundreds of pages of documentation with many specialized classes and hundreds of accessor methods on one hand, and TWO METHOD CALLS API on the other!  _**This**_ is why JSON has achieved such large penetration, even as the incumbent had so many other advantages.

Essential capture can really matter!
