.[[ABio]].  [[@Github]] [ABIO Folder](spot://ABIO~Folder) 
   [[ABIO.]]  [[ABIO Log]],
  , [[ABIO Behavior Templates]], [[ABIO CellMetabolism5]], [[ABIO Generator]]
  , [[ABIO Photosynthesis]], [[ABIO Prompts]], [[ABIO Queries]], [[ABIO References]]
  , [[ABIO Systems]], [[ABIO Task Example]], [[ABIO Tasks]],
  , [[Measuring Intelligence]],
  , [[ABIO Log]],
  , [[AlienBiologyWhitepaper]],
  , [ABio Note](spot://abionote), 
  DELS: [[Prj/ABio/AlienBiologyWhitepaper]], [[ABIO Log]],[[Alien Biology]],[[Prj/ABio/AlienBiologyWhitepaper]],

   [[ABIO CellMetabolism5]], [[ABIO Generator]], [[ABIO Photosynthesis]]
  , [[ABIO Prompts]], [[ABIO Queries]], [[ABIO References]], [[ABIO Systems]], [[ABIO Task Example]]
  , [[ABIO Tasks]],
  DELS: [[ABIO Behavior Templates]], 

  [[ABIO Queries]],
  [[ABIO References]]
  [[ABIO Task Example]]		-
  [[ABIO Tasks]]						-
  [[ABIO Generator]] 				-
   .
  [[ABIO Systems]]					-  EXAMPLE SYSTEMS
  [[ABIO Photosynthesis]]		-  Example
  [[ABIO CellMetabolism5]]	-  Example of 5 interrelated processes
  DELS: [[ABIO.]], [[ABIO Prompts]], 



# TODO





# OUT TAKES


**Problem**: Because LLMs are constructed from such colossal training sets, it's hard to find tasks to assess their ability to perform truly novel reasoning as opposed to some sophisticated form copying from its training data.

**Problem**:  It is difficult to assess the learning/inference performance of LLMs over novel tasks since nearly any complex task we might use likely already has connections to texts used to train that model.  Perhaps all of its learning and inference are just sophisticated forms of copying.  How can we tell?


**Problem**: It's difficult to assess complex LLM reasoning since nearly any complex test task is tainted by likely but unknown relation to the texts used to train the LLM.

