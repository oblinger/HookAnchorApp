
I'll look for research that utilizes Genesereth's General Game Playing (GGP) framework to construct tests for AGI systems, specifically focusing on measuring performance outside of the distribution of training data. This includes academic papers, technical reports, and other relevant materials that explore how GGP might serve as an evaluation tool for AGI adaptability and generalization.

I'll update you once I have the relevant findings.

# General Game Playing (GGP) as a Test for AGI Systems

## GGP Framework and AGI Evaluation  
Genesereth’s **General Game Playing (GGP)** framework was created to develop AI agents that can play a wide variety of games given only the rules, without game-specific tailoring ([General Game Playing: Overview of the AAAI Competition ](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c89c71dbe5617bea44383585b58cd0cbc37bf79a#:~:text=%E2%96%A0%20A%20general%20game%20playing,issues%20and%20logistics%20associated%20with)). This idea is widely seen as an important step toward **artificial general intelligence (AGI)** – the ability to handle new, unseen tasks. In fact, GGP *“is regarded as a necessary step towards creating [AGI]”* ([](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230563#:~:text=Abstract,In)). Launched in 2005, the GGP competition challenges an agent with a formal game description it **has never encountered before**, and success is measured by how well it plays across many such games ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=General%20game%20playing%20,This)). Because the agent cannot be pre-trained on any specific game, GGP naturally tests an AI’s capacity for **generalization and adaptability** beyond its training distribution.

## Testing Generalization with Unseen Games  
In the GGP paradigm, each match presents a completely new game drawn from a broad class of logic-defined games, forcing the AI to **reason from first principles**. As one study explains, GGP *“is a framework for evaluating an agent’s general intelligence across a wide range of tasks”*, where the agent is handed the rules of a new game and must immediately start playing it ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=General%20game%20playing%20,This)). This requirement makes GGP an ideal testbed for out-of-distribution performance – the agent can’t rely on prior training for a specific game since *“an AGI agent should solve problems that are never encountered by both agents and developers”*, i.e. novel tasks not seen during development ([](https://arxiv.org/pdf/2206.01044#:~:text=con%02trast%2C%20an%20AGI%20agent%20should,is%20solved%20by%20the%20developers)). Researchers emphasize evaluating **how quickly and effectively an AI adapts** to these unseen environments. For example, Xu and Ren (2022) propose that a proper AGI test should present problems in a way that developers’ real-world knowledge or hardcoded tricks can’t be directly applied ([](https://arxiv.org/pdf/2206.01044#:~:text=%281%29%20Independence,true%20in%20the%20artificial%20world)). GGP embodies this principle by keeping the test domain abstract and independent of any single prior world knowledge, thereby **measuring true generalization and learning on the fly** ([](https://arxiv.org/pdf/2206.01044#:~:text=An%20AGI%20agent%20is%20required,specific%20problem%2C%20the%20agent%20should)) ([](https://arxiv.org/pdf/2206.01044#:~:text=%281%29%20Independence,true%20in%20the%20artificial%20world)).

## Research Efforts Using GGP for AGI Evaluation  
Because GGP requires integrating knowledge representation, learning, and planning in an unknown setting, many researchers have used it as an **AGI evaluation benchmark**. For instance, Walędzik and Mańdziuk (2011) note that the GGP contest *“provides a research framework suitable for developing and testing AGI approaches in [the] game domain”* ([Multigame Playing by Means of UCT Enhanced with Automatically Generated Evaluation Functions | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-642-22887-2_38#:~:text=General%20Game%20Playing%20,performed%2C%20stages%3A%20generalization%20and%20specification)). Their experiments introduced a modified Monte Carlo Tree Search (UCT) algorithm coupled with automatically learned evaluation functions, aiming to make a single agent perform well on multiple games without game-specific heuristics ([Multigame Playing by Means of UCT Enhanced with Automatically Generated Evaluation Functions | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-642-22887-2_38#:~:text=General%20Game%20Playing%20,performed%2C%20stages%3A%20generalization%20and%20specification)). Similarly, classical reinforcement learning techniques have been tested in GGP as a way to probe general learning ability; Wang *et al.* (2018) used GGP as a **testbed for Q-learning** and other RL methods, arguing that GGP allows researchers to demonstrate learning in several different games with one algorithm ([](https://liacs.leidenuniv.nl/~plaata1/papers/bnaic2018.pdf#:~:text=For%20small%20games%2C%20simple%20classical,In)) ([](https://liacs.leidenuniv.nl/~plaata1/papers/bnaic2018.pdf#:~:text=Traditional%20game%20playing%20programs%20are,games%20can%20be%20played%201)). In these studies, success across diverse games is taken as evidence of more **general intelligence**. Researchers from the GGP community and the AGI community often cite GGP as a milestone: *“With decades of development, GGP has been regarded as a necessary milestone for AGI”* ([](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230563#:~:text=generalize%20abilities%20from%20one%20game,DeepMind%20developed%20an%02other%20program%2C%20called)). In other words, improving an agent’s general game-playing ability is viewed as progress toward the broader goals of AGI.

Multiple academic works explicitly connect GGP to AGI evaluation. Schofield and Thielscher (2019) describe GGP as *“a field which allows the researcher to investigate techniques that might eventually be used in an agent capable of [AGI]”*, using games as a **controlled environment** to evaluate reasoning and learning methods ([general game playing Latest Research Papers | ScienceGate](https://www.sciencegate.app/keyword/308229#:~:text=General%20Game%20Playing%20is%20a,imperfect%20information%20and%20introduce%20a)). The AAAI GGP competitions themselves were originally motivated by the long-term goal of AI that can handle “**a wide variety of tasks**,” not just one domain ([General Game Playing: Overview of the AAAI Competition ](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c89c71dbe5617bea44383585b58cd0cbc37bf79a#:~:text=match%20at%20L921%20makes%20in,In%20the%20words%20of)). In the words of the AAAI competition overview, unlike a specialized chess program, a general game player cannot rely on game-specific algorithms and thus must incorporate **general cognitive capabilities** (knowledge representation, reasoning, and learning) in an integrated way ([General Game Playing: Overview of the AAAI Competition ](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c89c71dbe5617bea44383585b58cd0cbc37bf79a#:~:text=%E2%96%A0%20A%20general%20game%20playing,issues%20and%20logistics%20associated%20with)). This makes performance in GGP a potential indicator of an agent’s general reasoning skill. Indeed, one overview notes that general game-playing expertise demands intelligence from the program itself, not just its programmer, and thus success in GGP could be seen as a step toward “generally intelligent systems” that can handle many tasks ([General Game Playing: Overview of the AAAI Competition ](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c89c71dbe5617bea44383585b58cd0cbc37bf79a#:~:text=match%20at%20L921%20makes%20in,In%20the%20words%20of)).

## Adaptability and Reasoning in Unfamiliar Tasks  
A key aspect of using GGP for AGI testing is examining **adaptability**: how well an AI can adapt to entirely new game rules and goals. Metrics proposed in AGI research, such as measuring the speed and efficiency of adaptation to novel problems ([](https://arxiv.org/pdf/2206.01044#:~:text=Metric,intuition%20is%20that%20the%20agent)) ([](https://arxiv.org/pdf/2206.01044#:~:text=resources%20consumed%20for%20an%20agent,The%20derivative%20dS%2Fdt)), align well with the GGP scenario. An agent that quickly learns to play a new game (few observations, few trial-and-error episodes) and achieves competent performance demonstrates the kind of rapid generalization AGI systems aspire to. In GGP, agents typically get a brief orientation phase to read the game rules (given in a formal language like GDL) and then must begin making decisions. Their ability to infer the game’s structure, formulate strategies, and adjust as the game unfolds tests their **general reasoning and problem-solving** in a way that pre-trained single-game agents never encounter. For example, a GGP agent might face a strategic board game one round and a cooperative maze game the next, with no shared features except the abstract logic-based rule description. Success across such jumps indicates a form of *general intelligence*: the agent can discover what matters in each new task and exploit it, rather than only performing well on familiar, narrow distributions.

Studies have examined these capabilities. One line of work looked at **inductive logic reasoning** in GGP: given traces (play-by-play records) of an unfamiliar game, can an AI *learn the underlying rules* and then excel at the game? Cropper *et al.* (2020) introduced *Inductive General Game Playing*, flipping the usual GGP challenge to evaluate an agent’s ability to infer game dynamics from data ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=General%20game%20playing%20,diverse%20games%2C%20such%20as%20Sudoku)) ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=Sokoban%2C%20and%20Checkers,for%20motivating%20and%20evaluating%20future)). Their results showed that many existing rule-learning algorithms struggled, underscoring that **generalization in GGP** (whether learning rules or learning strategies) is a hard problem and a good benchmark for AGI research ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=Sokoban%2C%20and%20Checkers,for%20motivating%20and%20evaluating%20future)). Another experiment by Samothrakis *et al.* explored hyper-heuristic approaches across many games, noting that GGP “forces one to get insights beyond certain hand-coded heuristics of playing a single game … to come up with algorithms that could attack any game from scratch” ([](https://arxiv.org/pdf/2406.18178#:~:text=The%20most%20obvious%20example%20of,Almost)). This reflects the emphasis on **general reasoning strategies** over memorization. In all these cases, the GGP environment serves as a **sandbox to evaluate general intelligence**: the agent’s performance on completely new tasks, its learning curve, and its ability to transfer knowledge or strategies between disparate games are recorded as metrics of its adaptability and reasoning.

## Extensions and Ongoing Challenges  
Researchers have extended Genesereth’s GGP framework to broaden the scope of AGI evaluation. One direction is incorporating **imperfect information and stochasticity**. Standard GGP games often have perfect information (like chess or tic-tac-toe). Schofield and Thielscher’s work on GGP with imperfect information games adds hidden information and uncertainty, which demand even more robust, general decision-making strategies ([general game playing Latest Research Papers | ScienceGate](https://www.sciencegate.app/keyword/308229#:~:text=General%20Intelligence,code%20so%20that%20others%20may)). They introduce techniques (like the HyperPlay family of algorithms) to handle partial knowledge, thereby pushing GGP agents closer to real-world complexity and testing more advanced reasoning under uncertainty ([general game playing Latest Research Papers | ScienceGate](https://www.sciencegate.app/keyword/308229#:~:text=General%20Intelligence,code%20so%20that%20others%20may)). Another direction is the shift to general video game playing. The General Video Game AI (GVG-AI) competition (introduced in 2014) was inspired by GGP and similarly presents agents with many novel 2D video games ([](https://repository.essex.ac.uk/20342/1/1704.06945v1.pdf#:~:text=II,GVGP%29%20was%20defined)). Unlike classical GGP, GVG-AI provides only the game screens and limited time per move, simulating real-time decision-making and sensory input. Crucially, it continuously adds new games over time, so agents must continually face tasks outside their training set ([](https://repository.essex.ac.uk/20342/1/1704.06945v1.pdf#:~:text=in%20an%20Artificial%20General%20Intelligence,the%20time%20this%20paper%20is)) ([](https://repository.essex.ac.uk/20342/1/1704.06945v1.pdf#:~:text=II,Unlike%20GGP%2C%20GVGP%20focuses)). Such frameworks are built on the premise stated by Perez-Liebana *et al.*: an **AGI evaluation should not be a fixed set of tasks, but an open, ever-growing set**, to prevent overfitting and ensure the AI isn’t just “routine trained” on a static benchmark ([](https://repository.essex.ac.uk/20342/1/1704.06945v1.pdf#:~:text=in%20an%20Artificial%20General%20Intelligence,the%20time%20this%20paper%20is)).  

Despite these efforts, truly **general** performance remains elusive. The GGP competitions have revealed that most agents, while flexible, still fall short of human-like generality. A recent analysis argues that GGP agents tend to exploit the provided structured models of games and often end up specialized to each game in turn, rather than finding *universal strategies*. As Hernandez *et al.* (2023) put it, *“the agents involved are not general in any sense… at best, what is learnt is how to act in a wide array of similar environments, i.e. the outcome [is] agents adapted to a specific game, not adaptive to wider environments”* ([](https://arxiv.org/pdf/2406.18178#:~:text=The%20most%20obvious%20example%20of,Almost)). This critique has led to proposals for making evaluation games even more unpredictable – for example, introducing **“open-ended” games** where rules can change mid-stream or where entirely new mechanics appear unexpectedly ([](https://arxiv.org/pdf/2406.18178#:~:text=has%20to%20get%20insights%20beyond,in%20a%20wide%20array%20of)) ([](https://arxiv.org/pdf/2406.18178#:~:text=similar%20environments%2C%20i,Quoting)). The goal of such proposals is to minimize any reliance on hardcoded assumptions and force AI systems to truly think on their feet. In the same spirit, researchers Xu and Ren (2022) outline an *“Artificial Open World”* concept for AGI testing, where an endless sequence of new problems is generated and an agent’s performance is measured by how swiftly it learns and generalizes (with metrics for adaptation speed, adaptation quality, and cross-task generalization) ([](https://arxiv.org/pdf/2206.01044#:~:text=Metric,intuition%20is%20that%20the%20agent)) ([](https://arxiv.org/pdf/2206.01044#:~:text=resources%20consumed%20for%20an%20agent,The%20derivative%20dS%2Fdt)). All these ongoing efforts show how GGP’s core idea – evaluating AI on **never-before-seen tasks** – has influenced modern AGI benchmarks.

## Summary  
Overall, **researchers view GGP as a powerful evaluation tool for generalization, adaptability, and reasoning in AI systems**. The academic literature (from AI competitions to AGI conferences and journals) supports the notion that success in GGP correlates with more general intelligence. By requiring AI agents to interpret new rules and devise strategies for unknown games, GGP directly tests an agent’s ability to **handle distribution shift** and **transfer its reasoning skills** to novel problems. Numerous studies and experiments have used the GGP framework to measure progress toward AGI, from early rule-based systems to modern deep learning approaches adapted for GGP ([](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230563#:~:text=generalize%20abilities%20from%20one%20game,DeepMind%20developed%20an%02other%20program%2C%20called)) ([](https://ebooks.iospress.nl/pdf/doi/10.3233/FAIA230563#:~:text=Artificial%20General%20Intelligence%20%28AGI%29,5)). They demonstrate both the potential and the challenges of this approach: agents have become competent across many games, yet truly human-level generality remains an open challenge. In summary, Genesereth’s GGP framework has not only been a crucible for game-playing AI but also a **valuable experimental ground for AGI research**, inspiring continued development of benchmarks that push AI systems outside their comfort zones in pursuit of general intelligence.

**Sources:** Genesereth *et al.*, *AI Magazine* (2005) ([General Game Playing: Overview of the AAAI Competition ](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=c89c71dbe5617bea44383585b58cd0cbc37bf79a#:~:text=%E2%96%A0%20A%20general%20game%20playing,issues%20and%20logistics%20associated%20with)); Genesereth & Björnsson, *AI Magazine* (2013); Walędzik & Mańdziuk, *AGI Conference* (2011) ([Multigame Playing by Means of UCT Enhanced with Automatically Generated Evaluation Functions | SpringerLink](https://link.springer.com/chapter/10.1007/978-3-642-22887-2_38#:~:text=General%20Game%20Playing%20,performed%2C%20stages%3A%20generalization%20and%20specification)); Schofield & Thielscher, *JAIR* (2019) ([general game playing Latest Research Papers | ScienceGate](https://www.sciencegate.app/keyword/308229#:~:text=General%20Game%20Playing%20is%20a,imperfect%20information%20and%20introduce%20a)); Cropper *et al.*, *Machine Learning* (2020) ([Inductive general game playing | Machine Learning
        ](https://link.springer.com/article/10.1007/s10994-019-05843-w#:~:text=General%20game%20playing%20,This)); Wang *et al.* (IEEE, 2017) ([](https://repository.essex.ac.uk/20342/1/1704.06945v1.pdf#:~:text=II,GVGP%29%20was%20defined)); Sironi (PhD Thesis, 2019) ([](https://dke.maastrichtuniversity.nl/c.sironi/wp-content/uploads/2020/01/Thesis-MCTSforAGIinGames.pdf#:~:text=games%20can%20model%20a%20wide,specific)); Hernandez *et al.* (ArXiv, 2023) ([](https://arxiv.org/pdf/2406.18178#:~:text=The%20most%20obvious%20example%20of,Almost)); Xu & Ren (ArXiv, 2022) ([](https://arxiv.org/pdf/2206.01044#:~:text=An%20AGI%20agent%20is%20required,specific%20problem%2C%20the%20agent%20should)) ([](https://arxiv.org/pdf/2206.01044#:~:text=%281%29%20Independence,true%20in%20the%20artificial%20world)).