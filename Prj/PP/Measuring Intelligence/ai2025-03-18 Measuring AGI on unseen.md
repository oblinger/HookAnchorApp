

# Open AI's Answer - Measuring AGI Performance on Unseen Problems: Major Evaluation Frameworks

Artificial General Intelligence (AGI) represents a milestone where machines can replicate the full spectrum of human intellectual abilities across diverse domains. A key challenge in AGI research is determining how to measure an AI system's ability to handle unseen problems, which demonstrates true general intelligence rather than task-specific performance. This report examines the most significant research frameworks designed to evaluate AGI performance on novel tasks.

## Leading AGI Evaluation Frameworks

## ARC Challenge (Abstract and Reasoning Corpus)

The Automated Reasoning Challenge, or ARC, introduced by François Chollet in 2019, stands as one of the most widely recognized benchmarks for assessing progress toward AGI. This framework specifically targets an AI system's ability to generalize its knowledge to new, previously unseen tasks[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec)[2](https://autogpt.net/arc-agi-test-nears-resolution-but-experts-question-its-validity-in-agi-quest/).

ARC consists of a series of complex, puzzle-like problems that require an AI to make predictions or generate answers from limited data. What makes ARC particularly valuable as an AGI assessment tool is its focus on abstraction and generalization rather than pattern recognition or rote learning[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec). The test evaluates whether a system can infer and apply rules without prior exposure to similar problems, thus testing true intelligence rather than memorization capabilities[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec).

As of 2023, the best-performing AI systems scored 55.5% on the ARC-AGI test, which shows significant improvement but still falls short of the approximately 85% threshold considered to represent human-level intelligence[2](https://autogpt.net/arc-agi-test-nears-resolution-but-experts-question-its-validity-in-agi-quest/). This benchmark has gained substantial credibility in the AI research community as a meaningful measure of progress toward AGI[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec)[2](https://autogpt.net/arc-agi-test-nears-resolution-but-experts-question-its-validity-in-agi-quest/).

## GAIA Benchmark (Generalized AI Agent)

The GAIA benchmark represents a more recent and comprehensive approach to evaluating intelligent agents across multiple dimensions. Unlike task-specific benchmarks such as GLUE (for NLP) or ImageNet (for computer vision), GAIA is specifically designed to measure generalized intelligence across various domains, making it particularly relevant for AGI assessment[3](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents).

GAIA consists of 466 curated questions spanning different complexity levels and assesses AI agents along four key dimensions:

1. **Task execution**: The ability to complete predefined tasks with minimal errors and without direct human intervention
    
2. **Adaptability**: How well an agent responds to unforeseen circumstances
    
3. **Collaboration**: Evaluation of multi-agent coordination and human-agent teaming capabilities
    
4. **Generalization**: Testing whether an agent can apply learned knowledge to novel, unseen scenarios[3](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents)
    

What distinguishes GAIA from other benchmarks is its focus on tasks that humans find conceptually simple but require AI systems to demonstrate fundamental skills such as multi-modal reasoning, web browsing, information retrieval, and tool usage[3](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents). This approach aligns well with evaluating an AI system's ability to handle unseen problems in real-world contexts.

## Additional AGI Evaluation Approaches

## Levels of AGI Framework

Developed by Morris et al., this framework introduces a classification system for AGI capabilities based on two core dimensions: performance and generality. This approach is notable for its emphasis that generality must be paired with performance measures to be meaningful[4](https://arxiv.org/pdf/2311.02462.pdf).

In this framework:

- **Performance** refers to the depth of an AI system's capabilities, measured against human-level performance for given tasks
    
- **Generality** refers to the breadth of capabilities, or the range of tasks for which an AI system reaches a target performance threshold[4](https://arxiv.org/pdf/2311.02462.pdf)
    

The framework establishes percentile-based performance levels relative to adult humans who possess relevant skills, creating a standardized method for evaluating progress toward AGI across different systems and approaches[4](https://arxiv.org/pdf/2311.02462.pdf).

## Extended Turing Test

The Extended Turing Test builds upon Alan Turing's original concept but expands it to assess general intelligence across multiple domains. While the original test focused primarily on conversational abilities that might fool human judges, the extended version incorporates broader cognitive tasks to evaluate whether machine intelligence can match human capabilities across various domains[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec).

This approach assesses not just linguistic competence but also reasoning, problem-solving, and adaptive learning when faced with novel situations. However, detailed specifications of this framework were not extensively covered in the search results[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec).

## The Coffee Test

Mentioned as one of several AGI evaluation frameworks, the Coffee Test presents a real-world challenge: the AI system must enter an unfamiliar house and successfully make coffee using the available equipment and supplies[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec). This test elegantly combines several essential AGI capabilities:

1. Environmental perception and navigation
    
2. Object recognition and functional understanding
    
3. Sequential planning and execution
    
4. Adaptation to unfamiliar settings
    
5. Understanding human preferences and social contexts
    

By requiring the AI to perform a mundane human task in an unfamiliar environment, the Coffee Test evaluates general intelligence capabilities rather than domain-specific knowledge[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec).

## Challenges in AGI Evaluation Methodologies

## Definitional Ambiguity

A significant challenge in evaluating AGI performance is the lack of consensus on what constitutes "general intelligence." Despite numerous proposed benchmarks, the AI research community has not converged on scientifically grounded definitions of AGI, making evaluation frameworks inherently contentious[5](https://arxiv.org/html/2502.03689v1).

Several researchers have argued that focusing on AGI as a singular goal may actually impede scientific progress by creating an "Illusion of Consensus" where researchers assume they're working toward the same objective despite fundamentally different understandings of what AGI entails[5](https://arxiv.org/html/2502.03689v1).

## Performance vs. Generalization Trade-offs

Current evaluation approaches struggle with balancing performance depth against generalization breadth. While some frameworks like ARC emphasize the ability to solve novel problems, effectively measuring both dimensions simultaneously remains challenging[1](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec)[4](https://arxiv.org/pdf/2311.02462.pdf).

The Levels of AGI framework explicitly addresses this issue by treating performance and generality as separate dimensions, but the inherent tension between specialized excellence and broad competence persists across evaluation methodologies[4](https://arxiv.org/pdf/2311.02462.pdf).

## Conclusion

The measurement of AGI performance on unseen problems remains an evolving field with several promising approaches but no definitive solution. The ARC Challenge and GAIA Benchmark currently represent the most structured and widely recognized frameworks, offering complementary perspectives on evaluating general intelligence capabilities.

Both frameworks specifically target an AI system's ability to handle novel, unseen problems, which is essential for distinguishing true general intelligence from narrow, task-specific AI. However, with the AI field rapidly evolving, these evaluation methodologies will likely continue to develop alongside the technologies they seek to measure.

For researchers and developers interested in AGI evaluation, combining multiple assessment frameworks may provide the most comprehensive picture of an AI system's capabilities across different dimensions of general intelligence.

### Citations:

1. [https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec](https://www.linkedin.com/pulse/how-do-we-know-weve-achieved-agi-nuri-cankaya-ivfec)
2. [https://autogpt.net/arc-agi-test-nears-resolution-but-experts-question-its-validity-in-agi-quest/](https://autogpt.net/arc-agi-test-nears-resolution-but-experts-question-its-validity-in-agi-quest/)
3. [https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents](https://workos.com/blog/gaia-benchmark-evaluating-intelligent-agents)
4. [https://arxiv.org/pdf/2311.02462.pdf](https://arxiv.org/pdf/2311.02462.pdf)
5. [https://arxiv.org/html/2502.03689v1](https://arxiv.org/html/2502.03689v1)
6. [https://pmc.ncbi.nlm.nih.gov/articles/PMC11614890/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11614890/)
7. [https://cloud.google.com/discover/what-is-artificial-general-intelligence](https://cloud.google.com/discover/what-is-artificial-general-intelligence)
8. [https://arxiv.org/abs/2411.14486](https://arxiv.org/abs/2411.14486)
9. [https://www.rand.org/pubs/perspectives/PEA3691-4.html](https://www.rand.org/pubs/perspectives/PEA3691-4.html)
10. [https://www.linkedin.com/pulse/agi-closer-than-we-think-insights-from-arc-agi-test-r-pillai-ukqoe](https://www.linkedin.com/pulse/agi-closer-than-we-think-insights-from-arc-agi-test-r-pillai-ukqoe)
11. [https://www.linkedin.com/pulse/arc-agi-test-tough-nut-crack-llms-mena-habeel-ojo9f](https://www.linkedin.com/pulse/arc-agi-test-tough-nut-crack-llms-mena-habeel-ojo9f)
12. [https://www.linkedin.com/pulse/arc-agi-benchmark-agi-asi-journey-superintelligence-amita-kapoor-7jpwc](https://www.linkedin.com/pulse/arc-agi-benchmark-agi-asi-journey-superintelligence-amita-kapoor-7jpwc)
13. [https://arcprize.org/arc](https://arcprize.org/arc)
14. [https://www.promptingguide.ai/techniques/zeroshot](https://www.promptingguide.ai/techniques/zeroshot)
15. [https://aws.amazon.com/what-is/artificial-general-intelligence/](https://aws.amazon.com/what-is/artificial-general-intelligence/)
16. [https://arcprize.org/blog/r1-zero-r1-results-analysis](https://arcprize.org/blog/r1-zero-r1-results-analysis)
17. [https://arxiv.org/html/2502.06559v1](https://arxiv.org/html/2502.06559v1)
18. [https://www.reddit.com/r/mlscaling/comments/1ht4emi/anyone_else_suspect_arcagi_was_never_much_of_a/](https://www.reddit.com/r/mlscaling/comments/1ht4emi/anyone_else_suspect_arcagi_was_never_much_of_a/)

---

Answer from Perplexity: [pplx.ai/share](pplx.ai/share)

I'll search for research efforts focused on measuring AGI performance on unseen problems, spanning standardized benchmarks, experimental methodologies, and theoretical discussions. I'll prioritize the most widely accepted and influential approaches first, covering all major AI subfields (reinforcement learning, symbolic AI, neuro-symbolic methods, etc.). Expect a detailed summary of each approach, including references.

I'll update you once I have the findings.

**Measuring AGI Performance on Unseen Problems**

Evaluating **artificial general intelligence (AGI)** requires going beyond narrow tasks to test an AI’s ability to handle novel challenges it wasn’t specifically trained on. Researchers have developed benchmarks, methodologies, and theoretical frameworks to assess **generalization and transfer learning** in AI. Below, we survey widely recognized approaches:

## AGI Benchmarks for Generalization

AGI benchmarks are *tasks, datasets, or environments* meant to gauge an AI’s ability to **generalize to unseen problems**. These benchmarks often provide diverse challenges and limit training examples to prevent overfitting. Key benchmarks include:

- **Abstraction and Reasoning Corpus (ARC)** – A set of 1000 hand-designed puzzles introduced by François Chollet ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=propose%20a%20set%20of%20guidelines,between%20AI%20systems%20and%20humans)). Each ARC task provides only a few input-output examples and then tests the AI on a novel input, requiring the system to infer the underlying transformation or rule. ARC is explicitly designed to measure *“a human-like form of general fluid intelligence”* by minimizing the role of prior training data ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=propose%20a%20set%20of%20guidelines,between%20AI%20systems%20and%20humans)). Success on ARC requires broad **abstraction, analogical reasoning, and few-shot learning**, making it a popular AGI benchmark. (Notably, until 2023 no AI solved more than ~20% of ARC tasks; recent models have just begun to approach human-level performance on this benchmark ([With o3 having reached AGI, OpenAI turns its sights toward ... - CIO](https://www.cio.com/article/3632509/with-o3-having-reached-agi-openai-turns-its-sights-toward-superintelligence.html#:~:text=CIO%20www,of%20all%20ARC%20tasks)) ([Is AGI Here? A Deep Dive into OpenAI's o3 Model and ARC-AGI ...](https://dansasser.me/posts/is-agi-here-a-deep-dive-into-openais-o-3-model-and-arc-agi-benchmarks/#:~:text=Is%20AGI%20Here%3F%20A%20Deep,This)).)

- **General Game Playing (GGP)** – The GGP competition (est. 2005 by Stanford) evaluates agents on **unseen games** described in a formal logic. A GGP agent receives a new game’s rules (e.g. a novel board game defined in a Game Description Language) and must play it well without human assistance. Games are turn-based and vary in rules, so agents rely on general **reasoning and planning** rather than game-specific learning ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=To%20compete%20in%20the%20General,and%20novelty%20of%20these%20games)). Crucially, *“new games (or variations of old games) are used for each competition, so that competitors cannot tune their AIs to specific games”* ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=To%20compete%20in%20the%20General,and%20novelty%20of%20these%20games)). Strong GGP agents typically use search techniques like Monte Carlo Tree Search combined with logic inference to handle a wide range of games.

- **General Video Game AI (GVGAI)** – An extension of GGP into real-time video games. In the GVGAI competition, agents face many 2D arcade-style games (dozens of public games plus newly designed ones each year) with various genres and rules ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=The%20basic%20idea%20of%20GVGAI,to%20modern%202D%20indie%20games)). Agents do *not* get the game’s rules upfront; they must **learn to play by trial and error** in each new game. As a result, GVGAI emphasizes adaptability in **unknown dynamics**. Every competition introduces 10 unseen games, so *“the basic idea of GVGAI is that agents should always be tested on games they were not developed for”* ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=The%20basic%20idea%20of%20GVGAI,to%20modern%202D%20indie%20games)). Success requires broad **perception, planning, and experimentation** abilities. GVGAI agents often use a mix of tree search (for planning) and fast learning, and the benchmark’s design (lots of games, easy addition of new games) helps prevent overfitting ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=To%20sum%20up%2C%20is%20any,broadest%20range%20of%20cognitive%20skills)).

- **Arcade Learning Environment (ALE) and ProcGen** – The ALE (Atari 2600 games) was a breakthrough in showcasing one algorithm (deep Q-networks) learning many different games. However, ALE agents typically train separately per game, so to test generalization, researchers introduced **procedural generation**. The *ProcGen Benchmark* (Cobbe et al. 2019) consists of 16 mini-games with procedurally generated levels ([[1912.01588] Leveraging Procedural Generation to Benchmark Reinforcement Learning](https://arxiv.org/abs/1912.01588#:~:text=,models%20significantly%20improve%20both%20sample)). An agent is trained on some random levels and must generalize to **held-out new levels** it never saw during training. This benchmarks both **sample-efficient learning** and robust transfer. ProcGen demonstrated that **diverse training environments are essential** for evaluating RL generalization ([[1912.01588] Leveraging Procedural Generation to Benchmark Reinforcement Learning](https://arxiv.org/abs/1912.01588#:~:text=,models%20significantly%20improve%20both%20sample)). For example, even an Atari-trained agent might overfit to specific levels; ProcGen forces it to learn more general strategies. (An earlier single-game benchmark called *CoinRun* showed agents often memorize training levels, highlighting the need for novel test levels ([Quantifying Generalization in Reinforcement Learning](https://proceedings.mlr.press/v97/cobbe19a.html#:~:text=Quantifying%20Generalization%20in%20Reinforcement%20Learning,agents%20overfit%20to%20surprisingly)).)

- **Animal-AI Olympics** – A 2019 competition using a 3D **simulated animal cognition testbed** ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=The%20Animal,human%20territory%3A%20some%20of%20the)). Inspired by experiments in animal psychology, it provides an environment with food rewards, obstacles, and objects, and evaluates if AI agents show **general problem-solving and “common sense”**. Each submitted AI is evaluated on *100 previously unseen tasks*, from simple food retrieval to challenges requiring memory of object permanence and navigation in the dark ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=Usually%2C%20AI%20benchmarks%20involve%20mastering,is%20%2012%20ever%20to)) ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=The%20tests%20will%20range%20in,navigate%20it%20in%20the%20dark)). All tasks are kept secret during training. The same agent must adapt to all sorts of scenarios, so the benchmark tests **general adaptability** rather than skill on one task. As the organizers explain, the goal is to measure *“the ability for a single agent to adapt to diverse environments,”* demonstrating a form of **generalized intelligence** ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=Usually%2C%20AI%20benchmarks%20involve%20mastering,is%20%2012%20ever%20to)). No entry perfectly mastered all tasks (even human 6–10 year-olds outperform AIs on many tests ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=According%20to%20Crosby%2C%20the%20most,is%20the%20capacity%20to%20adapt))), but some AI agents showed rudimentary generalization. This benchmark highlights the gap between narrow proficiency and general problem-solving.

- **BIG-bench (Beyond the Imitation Game Benchmark)** – A large collection of over 200 diverse tasks released in 2022 to probe the limits of *large language models* (LLMs). Tasks range from math word problems to logical reasoning, ethics, and even surreal puzzle questions. BIG-bench explicitly includes many challenges that are **outside typical training data** or that require creative reasoning, to test whether increasing model size yields more general problem-solving. Models like GPT-3 and others have been evaluated in zero-shot or few-shot settings on BIG-bench to assess their *transfer learning* ability. For example, GPT-3 (175B) showed surprising skill at some unseen tasks via prompt-based learning, demonstrating **few-shot adaptation** on novel problems ([](https://arxiv.org/pdf/2005.14165#:~:text=GPT,a%20sentence%20after%20seeing%20them)). However, BIG-bench results also expose where even huge models struggle, thereby charting progress toward AGI. The benchmark’s name alludes to the *Turing Test* (“Imitation Game”) – BIG-bench aims to go beyond it by quantitatively measuring broad capabilities.

- **Tong Test (2023)** – A newly proposed comprehensive AGI evaluation framework focusing on **embodied, interactive tasks** ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=innovative%20approach%20aims%20to%20provide,DEPSI)) ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=The%20Tong%20test%20proposes%20five,a%20virtual%20environment%20with%20DEPSI)). The Tong test defines five key dimensions for AGI evaluation: *“infinite tasks, self-driven task generation, value alignment, causal understanding, and embodiment”* ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=The%20Tong%20test%20proposes%20five,the%20delineation%20of%20AGI%20milestones)). It envisions a rich virtual world with **dynamic physical and social scenarios** (labeled “DEPSI” for Dynamic Embodied Physical Social Interactions) where tasks are *generated on the fly* for the AI agent ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=A%20recent%20perspective%20article%20published,DEPSI)) ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=The%20Tong%20test%20proposes%20five,a%20virtual%20environment%20with%20DEPSI)). The agent can be embedded in a physics-rich environment (e.g. manipulating objects in a house, interacting with virtual humans) and must continually learn. The Tong platform would *“provide a standardized test pipeline”* for unlimited tasks and even involve human–AI interaction for open-ended challenges ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=Image%3A%20The%20Tong%20test%3A%20a,Credit%3A%20Yujia%20Peng%20et%20al)) ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=Unlike%20classical%20AI%20testing%20systems%2C,dimensions%20of%20abilities%20and%20values)). The figure below (from the Tong test authors) illustrates the concept: an infrastructure to create endless new tasks and an evaluation suite covering multiple ability dimensions. This approach aims to **benchmark progress toward human-level generality** in a more holistic way than single-task tests.

 ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html)) *Illustration of the Tong Test framework, which generates endless embodied tasks (physical and social) to evaluate AGI across multiple dimensions ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=innovative%20approach%20aims%20to%20provide,DEPSI)) ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=The%20Tong%20test%20proposes%20five,the%20delineation%20of%20AGI%20milestones)). Such environments test an AI’s ability to continually learn and adapt in interactive scenarios, going beyond static benchmarks.*

Each benchmark above targets generalization in different domains (vision, language, games, real-world simulation). Together, they reflect a growing consensus that **truly measuring AGI requires evaluations that span many tasks** and prevent overfitting to any one problem ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=that%20solely%20measuring%20skill%20at,AI%20benchmark%20should%20look%20like)). As Chollet notes, *“solely measuring skill at any given task falls short of measuring intelligence, because unlimited training data allow one to ‘buy’ skill in a way that masks the system’s own generalization power.”* ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=gravitates%20towards%20benchmarking%20intelligence%20by,highlighting%20the%20concepts%20of%20scope)) In other words, a real AGI should acquire new skills *efficiently* – a quality these benchmarks attempt to expose by presenting fresh challenges.

## Experimental Methodologies for Out-of-Distribution Testing

Beyond formal benchmarks, researchers have devised **experimental methodologies** to probe AI generalization. These methods often involve creating training/test splits where the test conditions are deliberately novel. Key methodologies include:

- **Meta-Learning & Few-Shot Transfer** – Meta-learning algorithms train an AI on a *distribution of tasks* so that it can **learn new tasks rapidly** with minimal data. A prominent example is Model-Agnostic Meta-Learning (MAML) by Finn et al. (2017), which learns an initial model that can be fine-tuned to new tasks with only a few gradient updates ([[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400#:~:text=classification%2C%20regression%2C%20and%20reinforcement%20learning,shot%20image%20classification)). The goal is *“to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples.”* ([[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400#:~:text=classification%2C%20regression%2C%20and%20reinforcement%20learning,shot%20image%20classification)) For instance, a MAML-trained agent might experience many small reinforcement learning games; when faced with a **previously unseen game**, it can adapt in just a few episodes to perform well. This tests an AI’s **learning-to-learn** capability. Few-shot learning is also evident in large language models: GPT-3 famously showed that after broad pre-training, it could be prompted with a brief description or a couple of examples of a new task and then perform that task without gradient updates ([](https://arxiv.org/pdf/2005.14165#:~:text=GPT,a%20sentence%20after%20seeing%20them)). This *in-context learning* ability – *“one-shot and few-shot proficiency at tasks designed to test rapid adaptation or on-the-fly reasoning”* ([](https://arxiv.org/pdf/2005.14165#:~:text=GPT,a%20sentence%20after%20seeing%20them)) – suggests a form of meta-learning internalized by the model. Researchers evaluate such models on their zero/few-shot performance on *unseen* tasks (e.g. new QA datasets, novel reasoning puzzles) as a proxy for general intelligence.

- **Open-Ended Reinforcement Learning** – Instead of training on a fixed task, open-ended RL continually generates new challenges, pushing the agent to **generalize**. A striking example is DeepMind’s *XLand* environment ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=,learning%20progress%20of%20an%20agent)) ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=distributions%20and%20training%20objectives%20such,we%20demonstrate%20that%20the%20general)). XLand is a 3D world with a “universe” of games (cooperative, competitive, solo challenges) – agents might have to find objects, play tag, hide-and-seek, etc., in procedurally varied terrains ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=massive%2C%20rich%20space%20of%20challenges,an%20iterative%20notion%20of%20improvement)) ([Generally capable agents emerge from open-ended play - Google DeepMind](https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/#:~:text=We%20created%20a%20vast%20game,the%20agent%20exhibits%20general%2C%20heuristic)). The training curriculum constantly evolves: as the agent gets better, the system generates harder tasks or new variations, so *“the agent never stops learning”* ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=between%20successive%20generations%20of%20agents%2C,good%20performance%20on%20Hide%20and)). Evaluation is done on a suite of **held-out games and environments** that the agent never encountered during training. In their 2021 result, after playing **700,000 unique games** in training, the resulting agent could **zero-shot generalize** to novel games (e.g. a brand-new Hide and Seek level) and achieve goals it hadn’t explicitly trained for ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=distributions%20and%20training%20objectives%20such,we%20demonstrate%20that%20the%20general)). It even exhibited emergent behaviors like experimentation and tool-use that were not pre-programmed ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=distributions%20and%20training%20objectives%20such,we%20demonstrate%20that%20the%20general)). This demonstrates a practical methodology: use *procedural generation and ever-changing tasks* to force agents to develop general skills. Other projects like **POET** (Paired Open-Ended Trailblazer) have similarly generated evolving obstacles for robots, so that solving earlier ones sets the stage for tackling later, harder ones. Open-ended approaches effectively *simulate an endless “school” for AI*, and testing the agent’s performance on brand-new problems (e.g. a novel combination of obstacles) reveals its general adaptability.

 ([Generally capable agents emerge from open-ended play - Google DeepMind](https://deepmind.google/discover/blog/generally-capable-agents-emerge-from-open-ended-play/)) *An example of an **open-ended 3D environment** (DeepMind’s XLand) used to train generally capable agents ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=massive%2C%20rich%20space%20of%20challenges,an%20iterative%20notion%20of%20improvement)) ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=distributions%20and%20training%20objectives%20such,we%20demonstrate%20that%20the%20general)). The world is procedurally varied; agents (blue and red) face a spectrum of games. After training on hundreds of thousands of tasks, agents are evaluated on new challenges (e.g. a game of hide-and-seek in a novel layout) to assess their ability to generalize.* 

- **Neuro-Symbolic and Program Synthesis Tests** – Combining neural learning with symbolic reasoning is another methodology to tackle novel problems. For instance, some researchers attack the ARC puzzles by having neural networks *propose programs or logic rules* as solutions, which are then executed to produce outputs ([Exemplary ARC tasks: (a) Crop to the smallest unicolor rectangle; (b)... | Download Scientific Diagram](https://www.researchgate.net/figure/Exemplary-ARC-tasks-a-Crop-to-the-smallest-unicolor-rectangle-b-Draw-lines-over-the_fig1_348408303#:~:text=The%20Abstraction%20and%20Reasoning%20Corpus,solves%20tasks%20via%20grammatical%20evolution)) ([Exemplary ARC tasks: (a) Crop to the smallest unicolor rectangle; (b)... | Download Scientific Diagram](https://www.researchgate.net/figure/Exemplary-ARC-tasks-a-Crop-to-the-smallest-unicolor-rectangle-b-Draw-lines-over-the_fig1_348408303#:~:text=,)). This approach aims to leverage neural pattern recognition for perception and symbolic manipulation for **combinatorial generalization**. Testing such systems involves giving them puzzles or logic problems they’ve never seen (like new ARC tasks or math word problems) and evaluating if the AI can **infer a correct algorithm** on the fly. Success in these cases indicates the system can capture abstract relations beyond its training specifics. Similarly, **graph-based reasoning** engines and neural theorem provers are tested on new proofs or puzzles to gauge their general problem-solving ability. While each such system is often bespoke, collectively they represent experimental ways to measure how AI might tackle *structurally novel* challenges by integrating learned knowledge with reasoning.

- **Domain Randomization and Sim2Real Transfer** – In robotics, a common way to ensure generalization is to randomize simulator properties during training so that the robot controller doesn’t overfit to one exact scenario. This methodology, though narrower in scope, is used to test if a robot (or agent) can handle **new conditions**. For example, an arm might be trained in simulation with varying lighting, object shapes, and positions; it is then tested on a real-world task it never saw exactly in sim (like picking up a new object). The degree of performance drop indicates generalization. While not an AGI test per se, this practice enforces that the AI’s policy captures the *essence* of the task (grasping, etc.) rather than specifics. When scaled up – e.g., a household robot given entirely new chores – this becomes a way to evaluate general intelligence in embodied systems. IBM’s recent “Minecraft” environments and OpenAI’s “Virtual Home” have defined multiple household tasks and test robots on variants they weren’t trained on, measuring transfer learning in a realistic context.

- **Multi-Task Learning as a Proxy** – Simply training one model on many different tasks and then testing it on held-out tasks is a straightforward methodology to approximate AGI evaluation. For instance, the **Meta-World** benchmark provides 50 distinct robotic manipulation tasks (like push button, open door, pick-and-place). A multi-task agent can be trained on a subset of these and then must quickly solve a new task from the suite ([Meta-World: A Benchmark and Evaluation for Multi-Task and Meta ...](https://meta-world.github.io/#:~:text=world,50%20distinct%20robotic%20manipulation)). If it can do so, it shows it has learned some general strategy or representation that transfers. Similarly, in NLP, multitask-trained transformers (e.g. trained on translation, summarization, question-answering simultaneously) are evaluated on a new language task (like a new kind of analogy question) to test generalization. The best-performing systems on such evaluations often use **transformer architectures or other unified models** that can encode many skills and then be fine-tuned or prompted for novel ones. This line of research explores how *broad training* can produce more general AI, and uses performance on *unseen tasks as the metric*. Notable examples include Google’s **FLAN** and Microsoft’s **Turing Unified Framework**, which show that training on diverse instructions yields models that perform better on new instructions.

In summary, experimental methodologies for AGI evaluation deliberately **withhold certain problems for testing** to see if the AI can extrapolate. Whether through meta-learning, procedural environment generation, or multitask training, the theme is to mimic the **breadth of challenges** an AGI might face and measure how well an AI trained on some subset can *cope with novelty*.

## Theoretical Perspectives on Testing Novel Problem-Solving

Designing a true AGI test raises deep theoretical questions about what “general intelligence” means and whether it’s even possible to test *all* unseen problems. Key discussions include:

- **Turing Test and its Limits** – The classic Turing Test (1950) evaluates if an AI can imitate human conversational behavior indistinguishably from a person. While historic, it’s now seen as an **insufficient test of general problem-solving**. It focuses only on linguistic behavior and can be gamed by narrow tricks or adversarial conversation. As one critique notes, the Turing Test *“penalizes non-human intelligent behavior and may incentivize artificial stupidity”* in the effort to seem human. Thus, passing a Turing Test doesn’t guarantee the AI can *learn new tasks* or exhibit broad reasoning – it only demonstrates human-like dialogue skill. Modern definitions of AGI demand more: the ability to reason, plan, learn, and adapt across any domain ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=Researchers%20generally%20hold%20that%20intelligence,28)). This has led to alternative proposals like **Wozniak’s Coffee Test**, which posits that an embodied AI should be able to enter an average home and figure out how to make coffee (find the machine, water, coffee grounds, mug, and brew it) ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=The%20Coffee%20Test%20,has%20not%20yet%20been%20completed)). Unlike the Turing Test, the Coffee Test targets physical and procedural adaptability in a novel environment. So far, no AI or robot has *fully* passed this test ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=The%20Coffee%20Test%20,has%20not%20yet%20been%20completed)), underscoring the gap between today’s AI and human everyday competence.

- **Universal Intelligence and No Free Lunch** – Shane Legg and Marcus Hutter proposed a formal definition of intelligence as an agent’s ability to achieve goals across a wide range of environments ([[0712.3329] Universal Intelligence: A Definition of Machine Intelligence](https://ar5iv.org/pdf/0712.3329#:~:text=Consider%20how%20this%20equation%20corresponds,agent%20might%20have%20as%20all)). In their formulation, the **universal intelligence measure** considers performance over *all computable reward environments*, weighted by each environment’s complexity (an Occam’s razor prior) ([[0712.3329] Universal Intelligence: A Definition of Machine Intelligence](https://ar5iv.org/pdf/0712.3329#:~:text=Consider%20how%20this%20equation%20corresponds,agent%20might%20have%20as%20all)). This theoretically encapsulates “general” intelligence: an agent that can do well in many environments (not just one). *Specialized AIs* (like a chess engine) might excel in one complex environment but score near-zero on average, whereas an ideal AGI would have a high average score across **any task thrown at it**, with modest priors/training ([[0712.3329] Universal Intelligence: A Definition of Machine Intelligence](https://ar5iv.org/pdf/0712.3329#:~:text=)). In practice, computing this measure is impossible (infinitely many environments). But it provides a conceptual gold standard: any practical AGI test should sample a **broad and unbiased set of tasks**. This ties to the **No Free Lunch Theorem**, which states that no single algorithm can outperform all others on *every possible* problem ([Response to Blake Richards: AGI, generality, alignment, & loss functions — AI Alignment Forum](https://www.alignmentforum.org/posts/rgPxEKFBLpLqJpMBM/response-to-blake-richards-agi-generality-alignment-and-loss#:~:text=,Or)). In other words, if we consider truly arbitrary tasks, *“you cannot have a learning algorithm that outperforms all others across all tasks”* ([Response to Blake Richards: AGI, generality, alignment, & loss functions — AI Alignment Forum](https://www.alignmentforum.org/posts/rgPxEKFBLpLqJpMBM/response-to-blake-richards-agi-generality-alignment-and-loss#:~:text=,Or)). This doesn’t mean AGI is impossible; rather, it means an evaluation of “general” capability must specify a distribution of tasks we care about (e.g. tasks reflecting human-relevant skills). It also implies that an AGI will always have some domains it’s weaker in – perfect universality is unattainable. Researchers like Julian Togelius argue that *“solving every possible problem”* is a false goal (due to No Free Lunch), and instead we should define AGI as competence on a **large, representative variety** of tasks that matter to humans ([Togelius](https://togelius.blogspot.com/#:~:text=What%20would%20AGI%20mean%3F%20An,others%2C%20even%20compared%20with%20a)) ([Togelius](https://togelius.blogspot.com/#:~:text=One%20way%20of%20resolving%20these,with%20an%20average%20human%20professional)). This perspective informs benchmark design: the goal is to cover *enough* varied tasks that an AI cannot simply memorize solutions, but must truly generalize.

- **Defining “Truly Novel” Problems** – A philosophical challenge is what counts as an *unseen problem*. For humans, a problem can be “novel” yet we bring transferable experience to it. (For example, you’ve never encountered *this exact* question before, but you apply general reading and writing skills.) Likewise, an AGI will always rely on prior knowledge or priors – otherwise it faces a cold start for each problem. Theoretical work (like Chollet’s) emphasizes measuring the **efficiency of adaptation**: how much experience or hints does the AI need on a new task to reach competency ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=that%20solely%20measuring%20skill%20at,AI%20benchmark%20should%20look%20like)). An AGI test should probably allow the system *some* minimal adaptation (like a few trials or a brief description) and see if it “gets” the new task. Truly novel problems might be created on the fly (as in the Tong test’s infinite tasks idea) to ensure the AI wasn’t pre-trained on something too similar. Some have proposed *adaptive testing* regimes (analogous to IQ tests) that adjust difficulty and throw curve-balls to probe the boundaries of the AI’s abilities. The **feasibility** of testing AGI also raises practical issues: any fixed benchmark can eventually be learned or overfitted if an AI is trained on it enough. This is known as Goodhart’s law in AI evaluation – once a benchmark becomes the target, it ceases to be a good measure of general intelligence. Therefore, researchers talk about **suite of evaluations** and continual revision. An AGI might need to be evaluated more like an employee on probation – given ever-changing tasks – rather than a one-time exam.

- **Human vs. AGI Problem-Solving** – Some theoretical discussions compare AGI testing to human testing. Humans are considered general intelligences, yet even humans vary and specialize. Psychologists use test batteries to measure IQ or cognitive abilities, which inspired some AGI benchmarks. A difference, however, is that humans have innate common-sense knowledge and physical experience; to fairly test an AI, one must decide what prior knowledge it’s allowed. For example, ARC tries to give the AI only basic “innate” priors (like concepts of object, color, count) and not massive data ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=propose%20a%20set%20of%20guidelines,between%20AI%20systems%20and%20humans)). If an AI already trained on the entire internet, it might solve a novel task by recalling something similar from its training (which isn’t *pure* generalization). Defining fairness in AGI evaluation is tricky: do we reset the AI’s memory or not? Theoretical work by Legg, Hutter, and others often imagines an **agent starting from scratch**. In practice, most AGI candidates (like large language models) come with extensive pre-training. So some propose measuring *generalization margin* – how much better the AI gets on novel tasks compared to what we’d expect if it were just drawing on training data. These nuances are part of ongoing debates on evaluation protocols.

In conclusion, **testing AGI on truly novel problems** is recognized as a critical but complex endeavor. The community’s approach has been to **assemble many different challenges** – from playing unseen games, to solving abstract puzzles, to adapting in interactive worlds – and measure an AI’s performance relative to humans or an ideal learner. The most influential approaches combine practical benchmarks (to get measurable progress) with theoretical insight (to ensure those benchmarks actually reflect general intelligence). As AI systems become more powerful, these evaluations are continuously refined to stay ahead of the AI’s training: when an AI masters one “generalization” test, researchers develop new, harder tests. Ultimately, a convincing demonstration of AGI will likely involve **consistent success across a wide array of unseen tasks**, showing not just skill, but the *ability to rapidly acquire new skills* in any environment. Each benchmark and methodology described above contributes a piece toward that overall measurement of progress towards general intelligence.

**Sources:**

- Chollet, F. (2019). *On the Measure of Intelligence* ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=that%20solely%20measuring%20skill%20at,AI%20benchmark%20should%20look%20like)) ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=propose%20a%20set%20of%20guidelines,between%20AI%20systems%20and%20humans)) – Defines intelligence as skill-acquisition efficiency and introduces the ARC benchmark for general problem-solving.  
- Togelius, J. (2016). *Which games are useful for testing AGI?* ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=To%20compete%20in%20the%20General,and%20novelty%20of%20these%20games)) ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=The%20basic%20idea%20of%20GVGAI,to%20modern%202D%20indie%20games)) – Discusses game-based benchmarks (GGP, ALE, GVGAI) and the importance of multiple, unseen games in evaluating generality.  
- TechXplore (2023). *The Tong test: A new approach to evaluating AGI* ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=innovative%20approach%20aims%20to%20provide,DEPSI)) ([The Tong test: A new approach to evaluating artificial general intelligence](https://techxplore.com/news/2023-09-tong-approach-artificial-general-intelligence.html#:~:text=The%20Tong%20test%20proposes%20five,the%20delineation%20of%20AGI%20milestones)) – Describes the Tong Test framework focusing on infinite, embodied tasks for AGI evaluation.  
- MIT Tech Review (2019). *The Animal-AI Olympics* ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=Usually%2C%20AI%20benchmarks%20involve%20mastering,is%20%2012%20ever%20to)) ([The Animal-AI Olympics is going to treat AI like a lab rat | MIT Technology Review](https://www.technologyreview.com/2019/04/01/136245/is-ai-as-smart-as-a-chimp-or-a-lab-rat-the-animal-ai-olympics-is-going-to-find-out/#:~:text=The%20Animal,human%20territory%3A%20some%20of%20the)) – Outlines a competition using animal cognition-inspired tasks to test AI generalization to 100 novel scenarios.  
- Cobbe et al. (2019). *ProcGen: Procedurally Generated Environments for RL* ([[1912.01588] Leveraging Procedural Generation to Benchmark Reinforcement Learning](https://arxiv.org/abs/1912.01588#:~:text=,models%20significantly%20improve%20both%20sample)) – Introduces a 16-game benchmark to measure reinforcement learning generalization to new levels.  
- OpenAI (Brown et al., 2020). *Language Models are Few-Shot Learners* ([](https://arxiv.org/pdf/2005.14165#:~:text=GPT,a%20sentence%20after%20seeing%20them)) – Demonstrates GPT-3’s ability to perform unseen tasks with minimal examples, highlighting in-context learning as a form of generalization.  
- Legg, S. & Hutter, M. (2007). *Universal Intelligence: A Definition of Machine Intelligence* ([[0712.3329] Universal Intelligence: A Definition of Machine Intelligence](https://ar5iv.org/pdf/0712.3329#:~:text=Consider%20how%20this%20equation%20corresponds,agent%20might%20have%20as%20all)) – Proposes a mathematical measure of general intelligence over all tasks and discusses implications (no free lunch, specialized vs. general agents).  
- Alignment Forum (2022). *No Free Lunch and AI Generality* ([Response to Blake Richards: AGI, generality, alignment, & loss functions — AI Alignment Forum](https://www.alignmentforum.org/posts/rgPxEKFBLpLqJpMBM/response-to-blake-richards-agi-generality-alignment-and-loss#:~:text=,Or)) – Emphasizes that no algorithm is best on all tasks, framing why AGI must be defined over a distribution of tasks (and why truly “universal” intelligence is unachievable).  
- Wikipedia. *Artificial General Intelligence* ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=The%20Coffee%20Test%20,has%20not%20yet%20been%20completed)) – Describes various AGI tests (Turing Test, Coffee Test, etc.) and AI-complete problems, providing context on the goals and limitations of AGI evaluation.  
- DeepMind (2021). *Open-Ended Learning Leads to Generally Capable Agents* ([[2107.12808] Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808#:~:text=distributions%20and%20training%20objectives%20such,we%20demonstrate%20that%20the%20general)) – Reports an agent trained on a vast array of tasks (XLand) that generalizes its behavior to new games, an empirical step toward AGI, and discusses how to measure its progress.  
- Finn et al. (2017). *Model-Agnostic Meta-Learning (MAML)* ([[1703.03400] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400#:~:text=classification%2C%20regression%2C%20and%20reinforcement%20learning,shot%20image%20classification)) – Presents a practical method for training models that quickly adapt to new tasks, used as a benchmark for few-shot generalization.  
- Additional references in-text: GGP competition details ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=To%20compete%20in%20the%20General,and%20novelty%20of%20these%20games)), GVGAI competition rules ([Togelius: Which games are useful for testing artificial general intelligence?](http://togelius.blogspot.com/2016/07/which-games-are-useful-for-testing.html#:~:text=The%20basic%20idea%20of%20GVGAI,to%20modern%202D%20indie%20games)), Chollet’s critique of task-specific benchmarks ([[1911.01547] On the Measure of Intelligence](https://arxiv.org/abs/1911.01547#:~:text=gravitates%20towards%20benchmarking%20intelligence%20by,highlighting%20the%20concepts%20of%20scope)), Wozniak’s Coffee Test definition ([Artificial general intelligence - Wikipedia](https://en.wikipedia.org/wiki/Artificial_general_intelligence#:~:text=The%20Coffee%20Test%20,has%20not%20yet%20been%20completed)), etc.

# Perplexity's Answer

