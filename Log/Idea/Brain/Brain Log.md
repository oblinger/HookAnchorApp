
### 2024-10-01  Response to Ignacious about why o1 is not type-II thought

My thinking is along similar lines.

o1 is still trying to be a smarter query/response engine with performance not too much more expensive than GPT4. I think full type-II reasoning replaces 1/2 hour or 3 months worth of human thinking, its setting and managing the agenda for a line of experiments that one might publish into a paper.

This kind of thinking will probably be thousands of times more expensive that a GPT4 query, but of course it is producing much more too.

For me the biggest difference I perceive in my own type I and type II thinking is that in type I thinking I point the "fire hose" of my unconscious at some topic and "stuff" comes out. Complex reasoning chain snippets, creative analogies, pragmatic heuristics, etc. All stuff that also comes out of type-II reasoning, but in a kind of hodge-podge random collection.

My type-II reasoning seems to use the outputs of type-I reasoning as the "grist" for action. unconscous strategies for triaging this type-I firehose occur to me at the same time the ground "assertions" occur to me. Using this I decide to have a stance towards some of those things occurring to me. For example, I might decide to have a goal of X with a sub-goal of Y. or I might decide Z is true, or I should decide if Z is true.

My type-I layer is spewing raw material for type-II, and it is also spewing directives for type-II strategies as well.

Then there is a type-II execution framework (which does not exist in o1 or in LLMs). It might be a simple as a a short-term symbol association layer, or a symbol stack or such.

Type-II reasoning proceeds by explicitly loading symbols (representation) into the short term store and then allowing its type-I firehose to be reactive to the state of this store.

All of this I feel one could build pretty easily on top of a type-I reasoning (and I bet folks ARE doing this). the trick is to map the thinking of this type-II reasoning with observed thinking in human thinkers so that one can bootstrap learn the type-I fire hose needed to drive effective reasoning in this type II system.

In short, I think once we have type-II reasoners they will very clearly be able to describe their current state of thinking about a problem over an extended period of time. Much in the way over beers, you might ask a researchers what they are working on these days, and they could speak at length about this various ideas and strategies that are in play right now.

and for each part of that strategy you could drill in and understand why the decide on this and not that, why the believe this and not that, etc. and the whole edifice will be internally consistent (mostly) since they really are building it and maintaining it explicitly and symbolically . (even if they arrived at it sub-symbolically and even if they could not tell you why they ended up thinking this and not that, at least they could tell you what they are believing right now.

Current LLMs (even o1) does not specifically believe anything at a symbolic level. its just tokens all the way down.