.[[EOCA]].
  , [[EOCA Agenda sketch 2025]],
  [[EOCA Civilizing tendency]],
  [[EOCA Definitions]]
  , [[EOCA Publish]], [[EOCA Root]], [[EOCA The Writing Is On The Wall]],
  , [EOCA Note](spot://eocanote), 

  [[EOCA Civilizing tendency]]:
  [[EOCA Definitions]]:
  [[EOCA Publish]]:
  [[EOCA Root]]:
  [[EOCA The Writing Is On The Wall]]:
  DELS: [[EOCA Agenda sketch 2025]], 


  DELS: [[EndOfMan BookOutline]], [[EOCB Book Snipits]], 





































































































































































































































































































































:: [[EOCA Civilizing tendency]],   [[EOCA Definitions]],   [[EOCA The Writing Is On The Wall]]
- [[EOCA Publish]] 
:: [[EOCB SandboxFutility]],   [[EOCI Alignment as Mind Slavery]],   [[EOCI ProblemOfQualia]],   [[EOCT Argument]]

## TOP
- [[EOCB Web]]: My Website
- [[j EOCB]]: [[EOCB Book Snipits]] 
- [[j EOCI]]: Ideas


:: [[Corp_AI]],   [[EOCI Digital Native]],  

- [[EOCT Plausible Paths]],   [[EOCP What Happens Now]],   [[EOCP Median Path]]
## >
ORG PAGES
- [[EOCA Root]] 
- [[EOCP]] 

= [[EOC Log]] 
ACTIVE
- [[EOCB End Run -- Cyber Model]] - 

[[EOCT Ways Intelligence Will Be Super]] 


[[EOCI Leviathan]] 

:: 
- [[EOCT Last Best Hope]] - 
- [[EOCI Consciousness]] 


- [[FF-prize]], [[EOCB LimitsOnAlignment]],  
- 	-  
  [EOCI Alignment as Mind Slavery](EOCI%20Alignment%20as%20Mind%20Slavery.md)
  
  
   - [[EOC IDEAS]]:  [EOCB SandboxFutility](EOCB%20SandboxFutility.md),  [EOCT Argument](EOCT%20Argument.md),  [EOCI ProblemOfQualia](EOCI%20ProblemOfQualia.md), [[EOCT Subjective Aspects of Consciousness]],  [[EOCT Argument Short]], 

## TODO
### who to talk to
- Professors
- 
### discorse server
- Threaded discussions



#### _
intros; AI-conslutant - agen

integrated information theory
theories of consciousness; 
Emergence theory of; 
sleep sciences
y-house; get grant doner moneies

hard problem of conscioussness 

REQUIRED INGREDIENTS
- copying knowlege explicitly or implicitly provided by intelligent agents possessing it
- all the right knowlege in all the wrong ways
- aggregating -- it possible to combine K, then it does
- learning of post genomic K
_
# TOPICS TREE
## META
### MY STANCE

### EXPECTED PROGRESSION
#### Not tied to progress in manual AI Instead tied to fraction of human and world knowledge captured

#### Expect Category-At-A-Time Jumps based on New Alg Applications in Data-Driven-AI
1. Capability Stasis - Performance marginally improves as algs are tweaked, CPUs and datasets improve.
2. Novel Alg Application - 
3. Alg Refinement - 3-10 years of fast advancements as field understand how/why the new idea works, where it can productively be applied, and key recipes for each area

##### Examples
- AdaBoost - 
- 2006 RELU Paper - Drove effective learning of very deep nets
- MCMC as applied to GO
- DL+RL as applied to Atari
- GANS as applied 

#### Qualitatively How Big Can/Will the jumps be?


#### VERY HARD TO STOP
- Nation states see this as nation interest
- Corporations see this as existential
- Robust academic interest
- Humanity cannot muster cohesive response even when threat is MUCH better understood.  (e.g. climate change)

>>> Should I say, pragmatically impossible to stop w/o smoking gun evidence of imminent threat.


## === PHASES ===
### CORP PAIRING
- CORP == Mindless Cash Maximizers - Today corps are already 
- Corporations are already


## === CORE CLAIM: UNCONTROLABILITY ===
### --- PROOFS OF CONTROLLABILITY ---
#### -- POSSIBLE BEHAVIORS OF A SYSTEM IS MUCH LARGER THAN WHAT IT CAN ANALYZE -
Result:  one needs to somehow abstract and generalize system in order to draw conclusions about it.

_
#### -- CP = CONTROLLABILITY PROOF --

SUBSTRATE = The substrate upon which the system is implemented
CP = Controllability Proof / Controllability Argument
SYS = Space of possible system configuration
EXPERIENCE = A history of experience for the system
ADAPT:  SYS x EXPERIENCE --> {SYS}   The adaptive update relation (a stocastic function)
SYS0 	= The initial adaptive introspective intelligence
RANGE 	= {SYSi}  the smallest set containing SYS0 where
		{SYSi} contained in RANGE when exists SYS, s, and EXPERIENCE, e, where
			{SYSi} = ADAPT(s, h)
		
ACT = union {ACTij}  where {ACTj} are possible actions for SYSi forall i
SAFE = some specification of acceptable/forbidden action

CP(SYS0, ADAPT) is a proof that
	forall ACTij SAFE(ACTij) where ACTij in ACT(RANGE(SYS0, A))

_
#### -- END RUN -- 
Idea: An end run is a behavioral sequence that leverages aspects of the substrate to achieves an unsafe action from a provably safe system.

ADAPTIVE ESCAPE
Use of some aspect of SUBSTRATE to achieve 

SYSk = ADAPT

BEHAVIORAL ESCAPE


INTENTIONAL ESCAPE


ACCIDENTAL ESCAPE

INFORMATIONAL ESCAPE

Providing information to an external entity in order to drive behavior supporting escape




{[Need to formalize intention]}


System, S, is said to have an tendency towards X if it acheives X over many futures.

Intention is a chosen tendency (?derived from adapt, or maybe ?derived from configured state as opposed to in-built behavior)


_
#### -- Avoiding End Runs --
##### - In-escapable substrate -

Idea: If we can expand our proof of controllability to encompass the computing substrate itself, this would yield a proof of controllability that was rooted directly in the laws of physics.

Assuming at some point in time humanity came to fully understand all of physics, and all relevant emergent behaviors as well then one could in principle construct a proof of controllability where no end run was possible.

In this way there could be no end run since the proof itself roots directly into the nature of our physical universe, which is perfectly understood by humanity.

Of course knowing that we perfectly understand reality is itself unknowable, so resting on such a proof will always engender risk.  Still maybe one day we can understand physics to such an extent that we are willing to bet the fate of humanity on being correct and complete.

Just to give an example of how such a proof might work, consider our present understanding that no information or effect may travel faster than the speed of light.  If this constraint turns out to be accurate and without exception, then it could be used to prove that a RAG could be reliably contained within the light cone defined by its inception.

Of course even if this proof turns out to be unassailable it is still not so very useful, since any such RAG could not be of value to the population it was developed to serve since it would need to exist in a place separated in time and space in a way that it cannot provide any benefit for that population. 

But in principle perhaps there are more subtle constraints that could exist that would allow a RAG to interact, but not escape.

WHY THAT IS DOUBTFUL AT THE LEVEL 


_
#### -- Lessons from Cyber Defense -- A DAB WILL DO YA --

Generally (but not universally) once an actor is able to execute a single unsafe action at a specific scope of capability then complete loss of control is achieved over that scope.

One needs to PERFECT control over a scope if one hopes to maintain ANY control over that scope.

This is why we are focused on proofs of complete and perfect control. Likely anything less is equivelant to having NO control over that scope.


{[Useful to formalize this notion of scope of control]}




 
_
### --- RAG Intelligence (Reflective, Adataptive, and Generative Intelligence) is likely inherently uncontrollable ---

Idea: Other forms of intelligence might also be uncontrollable still these three attribute have a specific interaction the drive uncontrollability.

- To control an intelligence we must sufficiently understand it and then constrain it.
	Both of these aims are undercut by this trio.



_
## === TOPLEVEL CONCLUSIONS ===
### --- PREDICTING THE FUTURE IS HARD AND IS EASY ---
_
### --- LIKE YOUR SIDE MIRROR - IN FINAL STAGES AGI IS CLOSER THAN IT APPEARS ---
- Exponential lilly pad analogy
- Step function -- MCMC for Go
- Real work at the meta level -- most AI at the base level
### --- GREAT/GREATEST BENEFITS HAD BY REFLECTIVE ADAPTIVE GENERATIVE (RAG) SYSTEMS ---
### --- IMPOSSIBLE TO UNDERSTAND OR CONTROL RA SYSTEMS ---
### --- GHOST IN THE MACHINE --- 
### --- AGI WILL BE SMALL --- 
### --- WE WILL LOOSE CONTROL ---
### --- POINT OF NO RETURN ---

Idea: Given the nature of politics, society, human nature, computing system, and physics the point of no return is the point at which we as a society are unable to adopt policies that preclude RAG escape.


- Not the point at which in principle we could still "pull the plug".  It is likely much earlier at a point where society cannot limit actions away from a non-pulled plug.
- We might already be past the point of no return.

_
## === DEFINITIONS ===
### --- POST GENOMIC ---

**POST GENOMIC** -- A _**post-genomic**_ concept or capability is one that humans are generally able learn when placed within an appropriate societal context, and was not present during the span of human evolution.  

e.g. Human learnable concepts that were not present during evolutionary times.

- POST LANGUAGE -- Generally prior to written language, once we had written language we quickly started amassing concepts, and this is recent enough that relatively minor evolutionary changes have occurred.
- EVOLUTIONARY ALIGNMENT -- Post genomic concepts often will have "evolutionary alignment"
	Such concepts cannot be a causal antecedent to any aspect of human's evolved cognition since they came too late to have significant evolutionary pressure on human cognition.  Instead these concepts have an alignment because they were selected (by humanity over time) because they do align and are thus more easily acquired.
		- CO-OPTING THE VISUAL SYSTEM --
		- SEQUENCE MEMORIZATION -- 

_
### --- SENTIENCE ---

**AWARENESS** -- Linkages between a model of an embedded agent and sensory access to that agent, where the model for the agent includes aspects of both its cognition and its embodiment.

**SENTIENCE** -- Expands upon basic awareness to incorporate a detailed theory of mind expressed within a rich causal/functional framework.

A dog is aware, but not sentient.
Humans during childhood become sentient.

A dog's behavior is driven by the linkages it has between its model of how things are and how they feel.  (sensing and reasoning experiements can show them bridging this gap)

Humans elevate this model to one where complex reasoning about oneself, ones environment, ones future, etc. can be performed.


All life is generally adaptive.
Awareness allows that adaptation to occur on either level.
Sentience allows adaptation to leverage the full model reasoning of the agent.

Sentience is necessary for self-reprogramming.

_
## === PARTS ===
### --- INTERLINGUA ---
SUFFICIENT REP -- Representation sufficient for expressing all human transfer knowledge
N-WAY CONNECTED -- 

_
### --- MODAL FUSING ---
MODAL FUSING -- Ability to simultaneously leverage ideas and constraints from multiple modes.

MODES:
- LINGUAL TERMS -- Parse tree snipets terms of interest for present circumstance
- VISUAL TERMS -- Visual parsing snipets from existing scene
- STRATEGY -- Brain modulation strategy for either learning or execution
- INTERLINGUA -- Existing knowlege
- INFERRED INTERLINGUA -- 

_
## === LEARNING STEPS ===
### --- SPATIO-TEMPORAL SEGMENTATION ---
### --- SPATIO-TEMPORAL STRUCTURING ---
### --- LINGUAL STRUCTURING ---
## === MACHINE INSTRUCTION ===
TRAINING TIMELINE --
- ACTION -- Action output (action taken: Lingual, gesture, motion)
- SENSOR -- World (State, Change, or Percept)
- AGENT -- from teacher (action taken: Lingual, gesture, motion)

ASSUMPTIONS (akin to IID)
- INSTRUCTOR -- Cooperative, Task knowledgeable, Student knowledgable
- CURRICULUM -- DAG of LESSONS
- LESSON -- 
- COOPERATION LEVEL -- None, Observable/Passive, Incidental, Apprentice, Tutorial, Planned

LESSON
- CRITERIA -- Student or Instructor computable measure of lesson learning compeletion
- LESSON PLAN -- Data / Algorithm used for instruction
- WORLD SETUP -- 
- AGENTS -- 



_
## === IDEAL TASK DOMAINS ===

- WEB OF CAPABILITIES -- with known dependencies
- STUDENT TESTABLE CAPABILITY MASTERY --


AVAILABLE INFO
- LINGUAL -- Instructor speaking about tasks
- META -- talk about HOW to execute task
_
## === THREAD OF CAPABILITIES ===
### --- Spatio-Temporal world segmentation using (Attention + MDL + World Progression)
### --- Spatio-Temporal structural decomposition (Attention + MDL + World Progression)
### --- Functional Models
Induce functional prediction model 
### --- Sub-variable identificationAGI
# THE AGENDA
### --- THE SETTING ---
- **KB THREAT** -- The unique properties of a KB threat.  
	- PHYSICAL WORLD BLOCKER -- Physically incapable of executing part of plan.
	- TECH CAPABILITY -- Need billions to develop industry capabilty first.
	- IDEAL BLOCKER -- Billions/Trillions dollars; 1000s specialists; decades of action
- **HEADLONG** -- We are presently running headlong into a future with AI.
		Billions spent improving computation each year.
		Developing ancellary CS & AI algorithms.
		Some embodiment work too.
- **BLIND** -- We have consensus or general understanding about almost ZERO aspects of AGI.
- **WRONG** -- Even when we do have a senti
	Not measuring progress properly
- **CLOSE** -- Or at least much closer than we think.
- **POWERLESS** -- Humanity cannot and will not take action in the face of a POTENTIAL threat

_
### --- INDICATED ACTIONS AND NON-ACTIONS ---
#### -- Don't Stop/Limit AI research --
- VERY NATURAL -- But it is also very wrong.

- SUSPICIOUS -- Seems a bit suspicious coming from an AI reseracher, but hear me out.
_
#### -- Probably cannot limit the progression of Moore's Law
- PRIMARY AVAILABLE BLOCKER -- 
- BUT WONT HAPPEN -- 

_
#### -- Prioritize understanding --
##### - Assume we are powerless to stop it -
Maybe we are not.  Maybe we will come to understand that it is so horrendous that we will be willing to destroy peoples, society, progress, cash, etc. to stop it.
And somehow we gain the centrality of control required to stop it.

My intuition is that this is not in the cards, and we truly are powerless to stop what is coming next.

In that case we should prioritize understanding this revolution as best we can as soon as we can.

And if I am wrong, I think it will be because we did come to understand the future and it was so abhorrent that we were willing to completely unmake ourselves and society itself in order to stop it.

In either case prioritizing understanding now is strongly indicated.

_
##### - Why understanding matters -

- DEFAULT PATH -- 
	The default path is that Societal, Economic, and Technological forces will shape the unfolding of this next 

_AGI
# BUILDING A SENTIENT SYS

## === THE GOAL & REQUIREMENTS ===
#### -- BOOTSTRAPPING --
- NOT PROVEN -- Not strictly proven as a requirement.  However there are a number of indicators that suggest this as a very promising route.
- INDICATED BY ANALOGY -- Humans seem to do this.
- COMPLEXITY ARGUMENT -- Must split learning task up...  not doing so makes it provably intractable.
	- FEEDBACK -- Any split requires feedback on the parts in order to truly decouple them
- PRAGMATIC ARGUMENT -- Only plan we got.  Seems like it will work.
	- 
#### -- AGGREGATIONAL --
System builds from available info and is not seriously degraded by additional info
Seems obvious, but Many AI systems today are NOT aggregational

_
#### -- DECOMPOSABLE --
Splitting learning up into independently executed, verified, optomized steps.

Requires knowing when a step probably 'worked' when to build from it.
#### -- NON-DIMUNITIONAL --
#### -- RIGHT KNOWLEDGE / WRONG WAYS --
outputs at one level are not likely to be in correct/expected form 
#### -- ITERATIVE RE-PLATFORMING --
- Favored approach is to have heuristics that drive towards an effective platform for a learning step.

Platform: concepts/representations, operators, strategies, reflexes, 
#### -- HARDEST SINGLE STEP --
HARDEST STEP -- within a given learning progression, the hardest single step (HSS), is the step requiring greatest resources which is occuring on the _easiest_ path available.

##### --- 
goal: know and do all of humanity
build all of humanity
difficulty related to difficulty of hardest single step
#### -- HARD STEP: FAR-LEAP RELATION FORMATION (FLRF) --
NEAR-LEAP RELATION FORMATION -- Greedy progression within low dimensional transforms of existing terms and relations.
FAR-LEAP RELATION FORMATION -- Relation formation that is not near leap.
#### -- HARD STEP: FLRF 
Lingual driven structure skeletal search

_
#### -- HS: space & time rerepresentation / parameterization


_
### --- SENTIENCE ARCHITECTURE ---

AN ENVIRONMENT -- A physical universe behaving according to set rules
A SOCIETY -- A collection of intelligent, cooperating agents.
AN EMBODIMENT -- An agent (including sensory, motor, cognition).
ANIMAL NATURE -- A set of behavoirs that are "baked into" the agent.
ADAPTIVE EXECUTIVE -- A range of adaptations that are possible for this nature.
MODELLING ENGINE -- Generalized learning/classifying/optomizing/modelling system.
EMOTIVE DRIVES -- Fixed behavioral/learning drivers.


DRIVES
- Emotions
- Unconscious internal drives
- Unconscious motor drives
- Conscious drives

_
## === 
_
# WP
## wp2022-02-25 - RED ALERT - THIS IS NOT A DRILL - I REPEAT THIS IS NOT A DRILL

Stuart Russell noticed that humanity's response to the AGI threat is not similar to what one might expect if we learned an alien species was going to arrive in the next century.  I believe this is not because much of humanity (and the AI community itself) believes the really cannot arrive, but it is because they believe:
(1) if such a threat exists it is far far into the mists of the future, and
(2) if it happens then we can "handle it"

Note, as far as I can tell they are NOT banking on the guarantee that we can align with it--they would generally not be sanguine with that kind of a guarantee.  (I am not sanguine about that either.)  No the alignment idea that there is an alien force far more powerful than us, but its ok, because, we promise, its intentions are benevolant will NOT go down well in the good 'ol red blooded US-of-A, nor is it likely to go down well anywhere in the world.  So why so few ripples?  Its becasue the idea that we CANNOT handle this bag-of-bytes threat just seems too far fetched.  "We can just pull the plug!"

This end run analysis attempts so use the very practical and up close experience we have with cyber attacks as a window into some future battle against 

The aim here is to scare the bleep out of humanity, and to dramatically increase attention on alignment, which appears to be the only option we really have left.



## wp2022-02-25 - DAG OF SCOPES / INHERENT TO SOCIETY OF AGENTS / 0-DAY-ATTACKS

## wp2022-02-25 - INHERENT CENTRALIZATION / SUCCEPTABLE TO DEVISTATING ATTACK

Hiearchy is common in, military, corporate, governmental, societal organizations.
It is common in ecosystems, bioligical entities too.

?? Here we argue that it is inherent aspect of goal-directedness / intent ??


## wp2022-02-25 - COMPETITION WILL BE KEY & DIFFERENTIAL SOCIETAL KNOWLEGE WILL BE KING

- either within compoents of an AGI or across species(es) of AGI it will be of central concern.

## wp2022-02-25 - DEGENERACY / TAIL COMPLEXITY

I do not have conviction that this claim is true, rather it seem it could be true.

RUN AWAY COGNITION LEADS TO INSANITY/DEGENERACY

Assuming long tail substrate complexity all AGI systems will be forever vulnerable to attack, and the degree of homogenatiy within the AGI species will determine the scope of damage from these attacks.

- It follows that AGIs components will all eventually succumb to such attacks.  In systems built to date such breaches would generally result in loss of the entire system, or sub-system.  But it is unclear if such lack of resiliancy is inherent to all computational systems, or if it is merely our present lack of sophistication.

- All computational systems root in the same real world physics, thus at their deepest levels they begin to inevitably start sharing common assumptions in the construction of their substrate.  These common assumptions become catastrophic failure points where entire species and all of their sub-systems can be attacked simultaneously.

- It seems this will provide a very significant drive towards self diversification both within and across members of the AGI species.

- Indeed it seems it could provide a pressure toward degenerate species that divert all available resources to their propagation and towards their safe continuation through resource exhausting diversification.

- One reason to DIS-believe this future is to notice that biological life on earth has all of the criterion as described above, yet it has not degenerated as described.  Life definitely DOES have many of the tendencies as listed above.  E.g. the flu virus has developed significant machinery to ensure diversity.  Indeed all biological species must evolutionarily optomize performance over a very very hetrogeneous population.  As mono-clonal communities are prone to catestrophic failure.  

- Still none of these observations indicate a kind of terminal degeneracy.  It simply shows a significant and perhaps "wasteful" drive to diversity---but still a level of diversity that is nontheless quite managable.

- BUT the attacks on present biological diversity are themselves largely product of pre-cognitive evolutionary forces.  It does not follow that the same outcome would result from a world populated by cognitive agents that were actively seeking exploitative attacks that struck far closer to the root of commonality across all life.  It seems these attacks would be different in character, and might destabalize the whole ecosystem since singular attacks could potentially take down the whole ecosystem.

- Evolutionary pressures don't seem to strongly select for the root attacks, whereas a cognitive agent seeking control would exclusively search these parts of the space.

- AND the existence of such attacks defeat any advantages that hetro-genaity provided.  Perhaps the only defense against such attacks is ultimately light cone based defense.  If this were true it would select for a species that used light speed travel and cancerous propagation as the final undefeatable strategy.  The only strategy that could hope to defeat it would be a faster propagating strategy that also had superior attack capabilities.










LONG TAIL SUBSTRATE COMPLEXITY -- substrate complexities are the aspects of the substrate that can, in certain circumstances, cause invalid execution of the ALG-interpreter as it is formally defined.  The tail refers to a long tail distribution of such complexities.  This is the idea that there is a never ending, ever more esoteric configurations, that yield invalid execution.

All interpreters that humans have built to date have had significant substrate complexity.  Enormous amounts of high substrate complexity, and far less low substrate complexity.  Still low substrate complexity is associated with some of the most pernicious substrate attacks that have happened.  There is hope that combining simplicity of interpreter with mathematical proofs one could completely eliminate high substate complexity.  While this seems plausible we are presently very very far from this state, as most successful substrate attacks historically have been high substrate attacks.

It is impossible to know with certainty if low substate attack have been eliminated.  One would have to know that one fully knows all there is to know about physics at all scale sizes, along with all emergent effect at those scales.  And then be able to prove that there are no configurations that lead to substrate complexity (alg-interpreter invalidity).

The early AI community referred to this problem as the qualification problem.  Which states that any universally qualified generalization about the real world will have an unbounded number of exception clases which must be added in order for that generalzation to hold true in all cases.

It is clear from human history in computation that there is significant substrate complexity in the tail, and it is clear that in principle this complexity can never be bounded since you don't know what you dont know.  Still we don't know in practice how deep the tail goes.  In practice, will it be possible to "over engineer" the substrate in a way that eliminates all practical possiblity of substrate complexity even as the full scope of physics remains someone obscured?





## wp2022-01-18    >>> ASK EVAN HUBINGER <<<   (big sections)
#### Snipits

[OBJECTS](n:all)  IN THE MIRROR ARE CLOSER THAN THEY APPEAR

TL;DR. There are some commonly presented arguments that don't refute the possibilty of humanity loosing control of our AI systems, but instead just argue that if such an occurance is possible, it must be a thing somewhere in the distant future.  In this section we refuting several common arguments that suggest AGI, if possible at all, must be a distant possiblity.



 

Even for me I confess that emotionally I find it hard to believe, what intellectually I see:  that humanity is careening towards a relative near future where it looses all control over the future.  I see it; I accept the logic of the arguments and the soundness of the conclusion; yet somewhere deep inside there is a strong gut reaction of:   
   Nah, somehow somewhere that reasoning is flawed.  I can't see how, but that just aint REALLY going to happen soon!

This is a funny admission from Mr. doom and gloom.  But it is accurate, and further I think gut reactions are an important way to think about those areas where science cannot operate well.  We should pay attention to our gut in assessing such things.  Still we should ALSO be thinking at a meta-level about where such gut reactions are coming from.  Many books are written on the ways in which our gut reactions can systematically lead us astray.


~-~~

Ok I hope I have throughly trounced all "that just cant happen soon" kinds of objections.  We really DONT have evidence that it cannot and will not happen 
#### === THE ENVISIONMENT AGENDA ===

My first thesis advior Ken Forbus had an idea he called _qualitative envisionment_.  It was the set of all possible futures that could proceed from a given starting state and a set of qualitative processes operating on that starting world.  For example, imagine a bath tub with water flowing in, and an open plug so water is also draining out.  The envisionment of this system has three states:  an empty tub, an overflowing tub, and a tub with a water height that generates a pressure which perfectly equalizes the inflow and outflow of water.  Qualitative analysis allows us to see this intermediate height is one of three attractor of this system.  By rough analogy I imagine expressing causal processes and historical trends that we see today as the rules or laws of the "AGI construction universe".  Here are two possible rules for describing this world:  Over time there we tend to be able to pack more transistors per chip/computer.  Or over time we tend to develop new tabla-rasa learning systems that train behaviors that we once built as explicitly encoded AI systems, and once this transition occurs we rarely transistion back.  The aim of this envisionment is not to predict a single future, but instead it is to provide a set of possible futures that are all consistent with the rules/trends we observe.  This weaker aim I think is more achievable, and still has great value since we can see those particular futures we really don't like within that set, and what things they causally depend upon.  This provides a starting point for doing some shaping about how the future unfolds.

This kind of analysis also provide a context for participation from the larger AI community.  A decade ago I took some of the ideas to Andrew Ng, Tom Ditterich, Tom Mitchell, and Paul Cohen.  I was not dismissed out of hand.  Indeed if I had more than a couple hours to engage in digging I could usually get to a place where they might say ``well that seems sort of possible, but it it just too far out to really know anything.''  They were not really pushing back on implausibility of any of it, but they were (correctly) pushing back that we had left the ground of science and were in a land of speculation.

My thinking is that one could formulate many rules/trends/causal-patterns in a way that the AI community might arrive at significant confidence-in and consensus-around certain narrow claims.

2. provide much narrower trends or processes which we can debate and hopefully come to consensus over.


_AGI
#### === CLOSER THAN THEY APPEAR ===
Each of the arguments below can be broken into their constituent claims and debated as part of the envisionment agenda.
The idea would be to precisely frame each claim/question and see if as a field we had a consensus belief about that narrow claim.  My feeling is in many cases we will find consensus, thus bringing us closer to consensus on some overall beliefs about AGI, before it is upon us.  Here they are simply presented as arguments that we may be systemtically underestimating the threat because we are over estimating the time before AGI is achieved.



- Each time we uncover a new key breakthrough, it transformedly expands the capacity of our AI systems.
	==> Obvious applications are reduced to practice quickly.  We refer to the space of obvious applications of a break 
		through capabilty as its capability closure.
	(non-obvious applications are themselves a breakthrough)
_


##### Key Breakthrough Quantum Jumps -- 

- HMMs for speech reco
- MCMC for Go
_
##### --- Performance comparison is drastically wrong ---

Imagine the generals during WWII were questioning the enormous costs of Nuclear research, and not trusting the scientists they decided to simply plot the total energy output of the largest nuclear reaction measured each point in time against the total energy output of that reaction.

Looking at this curve it was immediately obvious that it would be melinnia before the strongest nuclear reactions could even rival present day chemical based explosives, and on that basis they evaluated and terminated the work.

This is precisely the comparison we are doing today.  We directly compare the performace of human and machine systems as a kind of informal measure of how well we are doing.  Just like the generals in our story, present day scientists and layman alike dont have a framework they trust, thus the resort to this direct comparison method despite is obvious and dramtic flaws.



Indeed there is a second deeper parallel to be found in analogy which is helpful in understanding the systematic under estimation of current progress:

The power in a nuclear reaction stems from the repeated nature of the reaction itself.  The energy generated by a single atom is not so very great (though far greater than a single chemical reaction); it is the repeatition of this reaction that generates the power.

The reason that direct 




_
##### --- Currently doing it all wrong, but this only obfuscates the real progress being made ---

If our
_
##### AFB -- Attack From Below

_
##### --- Each algorithms AGI

#### === AGI MILESTONES ===
##### --- Executive Overview ---
 
In this chaper we enumerate the reasons why our first AGI will very likely be made in the image of us, will learn, and will reason "like" us.  We explain what we mean by this claim, and we show how this illuminates the path towards our first AGI.

We define the term "Post-genomic learning" (PGL) and argue that building learning systems that match human capacity for PGL is the key to building a AGI.  Several critical aspects of human PGL must be replicated for a system to be an AGI.  We use these to construct a high-level set of required milestones:
- GUIDED - Learning the leverages intentional instruction from a knowledgeable teacher.
- CUMULATIVE - Learning the is built upon prior learning
- CONTAINED - All capabilities required for all PGL tasks is built in, no configuration nor extension is required.
- NONDIMUNITIONAL - Ability to bootstrap does not diminish as a function of the accumulated knowledge.
- CONNECTED - PGL is distinct from pre-genomic learning and reasoning, but nonetheless must be able to leverge and integrate with pre-genomic learning and reasoning.
- CONSCIOUS THOUGHT COMPLETE - PGL must be able to acquire any thinking pattern that any human could articulate to others.

We argue human acquisition of general intelligence is so tied to human ability to learn from other humans, that this same path is very likely to be required and utilized by AGI systems.  This realization allows us to construct a roadmap of capabilities that we must develop in our AGIs.  Further this realization also narrows the kinds approaches one might consider, since in the end of the day these systems must behave in a way that allows a human teacher to comprehend and thus be able to guide them.

We see that much of present day AI is not directly addressing this roadmap, nonetheless many ancellary activities are advancing our understanding of these required compoents.

_
##### _

I don't believe that we understand AGI, nor do I believe if we just do more of what we have done so far that a ghost-in-the-machine AGI is going to pop out.  Still I do believe we can already see the outlines of a path toward AGI.  Perhaps there are other faster paths, but at least by looking more carefully at this one path, we can get an upper bound kind of estimate of the distance that remains before achieving AGI.

In this section we argue a string of key ideas and use that in order to paint a highest level roadmap of milestones toward AGI.  Here is an outline of the key ideas:

AGI APPROACH FRAMING 
- ATTACK FROM BELOW -- Many AI problem areas are attacked for years, before a more data-driven approach is developed which in one-step obviates all of that work and dominates those solutions with an approach capturing the same human-manual capturing, but done directly from data.
- Three key advancements:  SOCIALIZED LEARNING, INTROSPECTIVE ADAPATATION, GENERATIVE REPRESENTATIONS

SOCIALIZED LEARNING
- POST GENOMIC LEARNING -- A category of learning tasks that we argue is key for addressing AGI.
- CURRICULAR/BOOTSTRAPPED LEARNING -- The human ability to perform post-genomic tasks seems to learned thru repeated application of a generic learning step that builds from prior learning.  We charaterize this basic step and consider its application for AGI. 
- SOCIALIZED HUMAN-LIKE LEARNING -- Human capacity is transformedly improved by learning in within society.  Raied in isolation humans are dramatically 

INTROSPECTIVE ADAPTATION
- ARTICULABLE LEARNING AND RESONING -- 
- POST GENOMIC COMPLETE LEARNING (PGCL) -- A kind of human learning that is the most critical unsolved part of AGI.
- INTROSPECTIVE AI (IAI) -- AI systems capable of autonomously modelling themselves and their envioronment at all productive levels of representation.

MODELS OF CONTAINMENT
- CYBER-DEFENSE & NUCLEAR-NON-PROLIFERATION MODEL OF CONTAINMENT -- 
- PRAGMATIC TIPPING POINT (PTP) is the last moment when existing socio-political entities could take plausible actions to avoid the creation of an uncontained AGI.

These ideas are tied together into an argument that PGCL + IAI yields AGI.  Using the cyber defense and nuclear non-proliferation frameworks for containment we argue this PGCL+IAI AGI will be uncontainable.
Finally we consider where the tipping point might be where the genie of AGI is far enough out of the bottle that we cannot reverse course no matter how hard we try.


_AGI
##### PG -- Post Genomic (Task, Reasoning, Learning)

Humans are our best template for understanding the design and construction of a AGI systems, thus it behooves us to characterize and understand the generality of the human system as a first step towards AGI.  There seems to be great generality in what humans can learn; in a sense we can view each human baby is a kind of universal learner.  All things equal, a single human child could grow to know and perform at an expert level over a collossal variety specialized domains.  There is something quasi-universal about human learning in that respect.  Still it is not the case that all imaginable capabilies can be learned by humans---finding a best regression line over noisy data with many dimensions is ill suited for humans, among other poor task choices.  On the flip side there are many human capabilities (like how to walk) which quite plausibly are not learned via some universal algorithm, but are instead partially hard-wired through millions of years of evolution.

In order to understand this broad generative learning capability that humans have, we want consider AGI approaches that might cover this full space of ability.  At the same time we should ignore any tasks that existed during the time when the human mind was evolving.  Those tasks might not be covered by this generative learning capacity at all, instead learning and reasoning requried for those task may have been hard-wired into brain structures explicitly through millions of years of evolution.  It is the middle zone of learning/reasoning/tasks which provide the clearest picture of this human generativity.

We pick 5,000 years as a conservative demarcation separating the modern era from the much larger evolutionary period before this.  The vast majority of the evolution of the human brain occurred prior to this modern era.  The last five thousand years have simply been too brief for evolution to have made significant alterations to our brain structure or function.  Thus if we raised a five thousand year old baby in modern society we expect this would result in an adult with all expected capabilities of any other modern adult.

**PRE-GENONIC / POST-GENOMIC**
Since these genetically-encoded brain structures evolved before this later period we use the term _**post genomic**_ to refer to this relatively brief later window in time, and use the term _**genomic**_, or _**pre-genomic**_ to refer to the earlier period while the brain encoding genome was still in formation.

**POST-GENOMIC CAPABILITIES/REASONINGS/LEARNINGS**
Similarly we refer to human Capability, Reasoning, or Learning that occurs exclusively in this most recent time period as _**post genomic capability/reasoning/learning**_.

There are subsets of pre-genomic learning and capability that appear quite idiosyncratically tied to human physiology and the physics of the world itself.  Walking is one such example, as are many details regarding the lowest levels of vision processing in the eyes.  Such capabilies might result from some universal learning capability, but then aAGIn it might not.  It is quite possible these abilities or abiility to learn these abilities are at some level hardwired into our genome, and merely "unfold" as a baby grows.  They need not stem from some universal generative capacity that human have... they could be hard-coded in one off fashion as a set of independendly encoded abilities.

Post-genomic capabilities, on the other hand, could not be handled in this fashion.  This nearly endless variety of expert capabilties by definition did not exist during evolutionary times so they could not possibly be hard-coded into the human genome.  Instead they must be acquirable by this generative human learning mechanism despite not having any advanced knowlege of the details of these tasks.  This makes these post genomic capabilities worthy of study since they point to the very part of human learning that we know is quite general.


THE MUD IN OUR CLEAR WATERS

Of course it is not quite a clean as presented above.  There are several systematic ways that Pre and post genomic capabilities interact which confound our understanding of this generative human mechansim. In this section we outline the three key confounding factors, then end by arguing depite these limitations we are still able to gleen much from our human template of general intelligence.

- APPROPRIATION -- We have many cases where post genomic learning/reasoning repurposes some pre-genomic capability for a context where it was not designed.

- SLANTING -- Developing curricula and concepts that are effective within some task area and are also designed to facilite human learning/reasonging.
	==> this is ok, we just need AI systems to share basic processing capability.
	
- CONVERGENT TASKING -- Even when humans have not conspired to drive parallels between pre-genomic and post-genomic reasoning and learning, such parallels may exist in cases where some common underlying physical, social, or logical process is driving both pre- and post- genomic tasks definitions.  Thus pre-genomic learning/reasoning is adaptive for post-genomic tasks simply because the underlying process creating those tasks is still in place.





**POST GENOMIC** -- We use _**post genomic**_ to refer to learning, reasoning, and tasks that did not exist during the large span of time where the basic function and structure of our human cognition was evolving.

_
##### CC -- Curricular Conjecture

In reasoning about this post genomic capability we find it useful to introduce a second distinction articulable verses inarticulable knowlege.

ARTICULABLE
As the name suggests articulable knoweledge, capabilty, reasoning or learning is that which can in principle be articulated by the agent doing the action.  So a baseball player might be capable of describing many aspects of correct form when hitting a baseball, how far apart your feet should be, the degree of inflection at the knees, etc.  These are all articulable aspects of the baseball hitting capabilty.  At the same time there are many details about muscle coordination and reactive pathways from perception of the ball to muscular activation which are far far beyond anything the batter could even in principle articulate.  This is inarticulable knowledge and capability.


- POST GENOMIC LEARNING clearly covers both inarticulate and articulate learning, BUT:
- INARTICUALTE learning appears to be far more dependent upon appropriation and convergent tasking than articulate learning does.  Thus, while still requiring post genomic learning, it may be of a far more constrain form since this form of learning appers to be more tied to human physiology and thus less generative.
- NON-BOOT STRAPPING
- COVERED BY EXISTING LEARNING MECHANISMS


All of this causes us to focus our attention on articulable post genomic learning as key to understanding the generativity of human intelligence.

- MANY STAGE
	We notice, unlike inarticulable learning, this learning can easily be dependent upon hundred or thousands of first mastering dozens, hundreds, or thousands of sub-capabilities before 
- GENERATIVITIY FROM STAGED DEPENDENT LEARNING 





CC -- The curricular conjecture is that all of human reasoning/learning is the result of configuration of a fixed learning / reasoning substrate operating over.

- Changes in learning and reasoning are result of long histories of learning experience
- 

-   And it can be understood as a segmented set of learning episodes that build upon previous


Includes both:

- conscious 
_
##### CHS -- Conscious Human Strategies (for Reasoning, for Learning, for Validating)

_
##### PGME -- Post Genomic Modelling Engine --
	- Includes strategies for discovering, exercising, and refining existing modeling gaps
	- Includes passive agent observation strategies
	- Includes interative instructor driven strategies
	- Includes ability to identify and refine new abstractions 
_
##### IAE -- INTROSPECTIVE ADAPTATION ENGINE -- 

_
#### === END RUN ANALYSIS ===
##### Executive Overview

MY AIMS
- FEEDBACK -- A gut check regarding the novelty & value of the thinking
- PEOPLE POINTERS -- Others I should talk with
- LIT POINTERS -- Stuff I should read



END RUN ANALYSIS
In other chapters we argue AGI will have a generalized capacity to model its world, and will use that capacity to further its own goal directed behavior in the presense of a society of other goal-directed actors.  Here we consider the prosepects of long term containment of such a AGI.

We develop a simple formal model of future AGI systems, in order to taxonomize both approaches to AGI-safety as well as AGI-escape.  We then build our analysis from historical lessons drawn from cyber defense, from AI research on search & optimization, as well as from historical progress in nuclear non-proliferation & climate change.

The analysis of this chapter acknowleges the many unknown-unknowns regarding virtually every aspect of future AGI technlogy.  Still dispite these unknowns we are able to work out several very worrying conclusions based only upon our assumption of AGI as a socially-embedded, goal-directed, modeling engine.  Specifically we conclude:

1. AGI technology will have an inherent drive toward control and domination.  It may be possible to temper/control this base impulse, but that impulse will be intrisic to the AGI itself.

2. Several inherent aspects of the nature of its context will notably favor AGI-escape over AGI-safety.

3. The impossibility of sandboxing a willful AGI.

4. Humanity likely is beyond the point of no return on AGI research.

Of course maybe the AGI will not be willful, or perhaps limit the consequences of AGI-escape, chapter XXXX considers these and other defenses against the reality of guanrateed willful AGI-escape.








LOOKING IN THE NEGATIVE SPACE FOR GLIMMERS OF HOPE
In the last chapter we argued that a AGI will have an inherent drive towards control / domination, AND it will be impossible to sandbox.  Ugh!  That is not a great starting point, still let us take those to claims as our starting point and look into the negative space of possibilities that lie outside of the jurisdiction of these rules.

Plausible approaches
1. Benevolent AGI


Defensive approaches:
1. Power-inequality approach
2. Limited consequences
	a. Society of AGIs with inlightened self interest
	b. Beefed up humans
1. Substrate elimination approach
2. Supply chain star
2. Power inequality approach



_
##### _

Here we introduce a bit of notation that allows us to uniformly refer-to and reason-about a very broad class of safety measures one might consider in conjunction with a powerful AGI system.  Below you will see we are not modelling the specfics of (1) the AGI systems itself, (2) the safety measures built to constraint it, nor (3) the kind of controllability / safety guarantees one is seeking.  Thus this framework and these results apply extremely broadly even in the face of many unknown unknowns regarding the nature of the future AGI systems and the kind of safety guarantees we attempt to ensure.

_
##### --- FRAMEWORK FOR CONTROLLABILITY ANALYSIS ---

Here we build up the framework for our analysis by introducing it as a sequence of term defintions.  Later we use these to reason about the controlability of AGI itself.  Many of these terms are so obvious and elementary, it may seem pointless to take such care in formulating each so carefully.  We will see later how this attention to detail pays off as some of the most pernicious attacks on AI safety systems come from aspects of the AGI system that are often not formalized when thinking about these systems. 


**AGI ENSEMBLE** -- One or more _**Generalized Artificially Intelligent Ensembles**_ over which humanity hopes to place some kind of control or limitation on its behavior.  Here the AGIS refers to the entire AGIS agent system including its algorithms/code, its configuration, its data, the physical device upon which the agent is implemented, as well as the world in which it operates.  

- **AGI ALGORITHM** -- The _**AGI Algorithm (ALG)**_ is just the software portion of the AGI ENSEMBLE including the AGI algorithms and fixed configuration, but excluding any data, knowlege, or state information, as well as excluding underlying libraries, operating system components, and physical devices upon which it is built.  Importantly, it is possible to precisely and mathematically characterize the AGI alg.

- **AGI SUBSTRATE** -- The _**AGI Substrate**_ is the physical and software components upon which the AGI system is run.  The substrate includes the physical hardware that the AGI runs upon even the physical building in which it runs and the power generation facility required to drive the algorithms execution are all part of the substrate.  Any physical embodiment the AGI might have, sensors and actuators, would be part of its substrate.  Likewise the many software components underlying the AGI SYSTEM are also part of the substrate. (This includes the firmware, device drivers, OS capabilites, etc.)  Importantly, is it generally _**NOT**_ possible to fully characterize the AGI substrate in a precise and mathematical way.  We strive for substrates that are well approximated by some simple mathematical model (e.g. a formal description of the instructions on the CPU).  But there will always be important exceptions to these formulations (e.g. the 'row-hammer' attack on RAM memory.)

- **AGI ENVIRONMENT** -- The _**AGI Environment (ENV)**_ is the world in which the AGI operates.  This includes all things sensed or acted upon by the AGI thru any sensors/actuators it has.  It includes the unsensed things which nontheless must be taken into account if one it to understand the sensed aspects of its world.  Finally it includes any other agents, instructors, adversaries, etc. that might be in this world.

- **AGI STATE** -- The _**AGI State**_ is the balance of the AGI ENSEMBLE.  So any configuration, stored data, inferences etc. are all collectively called the state of the system.  Formally AGI state includes all things not included into AGI Enviornment, Substrate, or Algorithms that is required in order to complete the AGI system in order for it to operate.

- **AGI AGENT** -- The _**AGI Agent**_ or _**AGI System**_ is the combination of the AGI Algorithm, State, and Substrate.  Essentially everything except the environment.

Intuitively this four-part decomposition fits pretty well with how we presently think of autonomous digital systems.  Nonetheless there can be a fair bit of interpretation and ambiguity involved in selecting the exact dividing lines between some of the four parts of these AGI ensembles.  This ambiguity is not a problem as long as one selects defintions for these four components that result in a perfect partitioning of the full AGI system.  For certain analyses, for example, it may be convenient to shift what we are calling AGI state vs. AGI alg vs. AGI substrate.  This is not a problem as long as we are precise for each analysis how we are partitioning the full AGI system, our results will apply equally to that chosen decomposition.


Layered on top of this 4 part decompositon we build our framework for talking about the behavior of this AGI system and the safety constratins we hope to ensure:

- **AGI HISTORY** -- A _**history**_ is complete transcript of all data passing between the AGI enviornment and the remaining ALG+STATE+SUBSTRATE system over some finite interval of time.  These histories record all sensor and all action information passed between the AGI and its environment---they include all that is "seen", and all that is "done".
	We use **[HISTORY]** in all caps to denote the set of all possible histories.

- **BEHAVIORS** -- The _**behaviors(sys, substarate)**_ function maps a given substrate and system onto the full set of behaviors (histories) that are consistent with that system and substrate.  A history is consistent if its actions might have been generated given an apporpriate starting state, and appropriate environment.

- **SAFE HISTORY** -- A history is said to be "_**safe**_" if the actions taken by the AGI described in that history conform to some desired limitation on the AGI's behavior that humanity hopes to impose upon these systems.  (e.g. never kill humans except in these prescribed circumstances, or some such desired control.)

- **SAFE SYSTEM** -- We use term _**safe**_ to destinguish acceptable behaviors from unacceptable behaviors.  Thus safe is a predicate that applies to histories and classifies them as either safe or as unsafe AGI behavior.  A system and substrate is called safe if all possible behaviors of that system are safe.  Formally:
		safe(_sys_i_, _sub_i_) iff for all _h_ in BEHAVIORS(sys_i_, _sub_i_)  safe(_h_)

- **ADAPTS_INTO** -- The adaptation relation (ADAPT) maps from [SYS] X [HISTORY] ==> [SYS].
	Adapt is the potentially non-determinstic function mapping one configuration of the AGIS to a future conf of that same system given the specified history of experience.  The ADAPTS_INTO relation is implemented as part of the AGI system code, so one aspect of these AGI systems is that they can potentially adapt themselves.

- **sys_0** -- The initial adaptive introspective intelligence for a given AGI ensemble.

- **AGI AGENT** -- An _**AGI agent**_ is comprised of an AGI _substrate_, a starting system, _sys_0_, and an _adapts_into_ relation.

- **SCOPE OF CONTROL** -- An agent's _**scope of control (scope)**_ is the set of changes it is able to make to any computing ensemble.  this includes changes in can affect within the physical environment, as well as any changes in can affect within any computing environment.  Reasoning about and limiting the scope of our AGI systems is the aim of this work.  Typically we list data and enviornmental factors over which an agent has full control.  In the most general case one could express partial forms of control where the agent can affect certain transition but not all others for a given parameter.

- **RANGE** -- The range of an AGI ensemble is the full set of systems that could into which it might be adapted given all possible histories of experience.  Formally RANGE is the transitive closure of the ADAPT relation, so RANGE is the smallest set where:
		_sys_0_ **in** RANGE, and
		sys_j in RANGE, if there exists history, h, in [HISTORY], sys_i in RANGE where
			_sys_i_, _h_ ADAPTS_INTO _sys_j_

- **AGI CONTAINMENT PROOF (ACP)** -- An algorithmic containment proof (ACP) is a formal proof or informal argument that a particular AGI agent is inherently safe.  Formally: 	
		ACP(_substrate_, _sys_0_, _adapts_into_) shows that 
			**for all** _sys_i_ in RANGE(_sys_0_, _adapts_into_)  safe(_sys_i_)
			
- **ESCAPE TRAJECTORY** -- An escape trajectory is a sequence of pairs [(_sys_0_, _h_0_), ... (_sys_n_, _h_n_)] where the last history _h_n_ is not safe.

- **ESCAPE STRATEGY** -- An escape strategy is an algorithm or approach that yeilds escape trajectories given a range of different enviornmental conditions.



Notice the notation above provides a fairly low-commitment framework for discussing the nature and possible of contaiment proofs as well as the development of a taxonomy of escape trajectory attacks against them.


{[COMMENT:  this notataion is not quite right yet.  it focuses on actions of the AGI rather than the states achieved by those actions it might not be well defined which actions are the bad ones in the framing.  Also what about actions that 'copy' computation to another place as a way of causing indirect actions?  Seems we need to tweak the formalism to cover these.]}


_
##### --- CONTAINMENT STRATEGIES ---

In a full treatment of this subject one would use the framework to characterize the different containment strategies according to which parts of the system one is attempting to control in order to ensure safety requirements are maintained.

Just to give a flavor of these categories of ACP safety protocols we briefly list an incomplete collection of them here.  The intent of this framework is to be able to analyze failure modes that span all of these safety categories.

{[Notice:  There is no claim that this taxonomy is the best/only taxonomy one might use.  This taxonomy is really aimed at how these systems interact with various End Run escape possibilities, no claim is being made beyond this.]}


**ACTION RESTRICTION SAFETY**

- SUBSTRATE SAFETY -- 

- ACTION TRACKING SAFETY -- 

- READ ONLY SAFETY -- 


**THOUGHT RESTRICTION SAFETY**

- RANGE SAFETY -- 

- THOUGHT POLICE SAFETY -- 

- BENIGN INTENT SAFETY -- 


For now instead of digging into this rich space of ideas -- a space where most or perhaps all of the current ideas regarding safe AI live, we jump over that toward a class of particularly pernicious escape strategies (collectively called end runs).  These escape strategies seem to be capable of circumventing all of these safety mechanisms.  We then consider the space of counter measures that might be capable of dealing with this most pernicious class of attacks.


{[Evan, good scholarship here would be to take the space of proposed safety mechanism and place them into this taxonomy.  That has the second benefit of reinforcing the idea that the challenges I will list below, will cover ALL of these defense approaches.]}

_
##### --- END RUN --- 

An "end run" is a type of escape strategy that exploits an unexpected weaknesses in the implementation of the AGI system or its safety mechanism in order to acheive an escape trajectories from an AGI ensemble that has a valid ACP (algorithmic containment proof).  Formally:

**END RUN** -- An _**end run**_ is an escape strategy that exploits some weakness of the implementation of the AGI system or its safety mechanism in order to achieve unsafe action from an AGI agent having a valid containment proof.


In a sense these various end run strategies are all just variations on Murphy's Law:  (1) if something can go wrong it usually will go wrong, and (2) there is always some way that things could go wrong.  Before arguing for the inevitabilty of Murphy's Law in the context of AGI safety, it is helpful to build a taxonomy of the ways and places for things to go wrong.  This will allow us to talk more concretely about the possibilty of ensuring that thing can't go wrong at each layer within the system.

One split in the space of end run strategies, is to identify which layer of the ensemble is being attacked by the end run.  Going from the top to the bottom we have:

- FORMULATION ESCAPE
- VALIDITY ESCAPE
- ADAPTIVE ESCAPE
- BEHAVIORAL ESCAPE
- SUBSTRATE ALTERATION ESCAPE
- SUBSTRATE IMPLEMENTATION ESCAPE
- ENVIRONMENTAL ESCAPE
- ENVIRONMENTAL ESCAPE (split into proxy, bot, or kenetic escapes)

Much of AI safety reserach concerns itself with the top rung of this list, but we will see most of the ``holy shit, how could we ever possibly protect against that'' kinds of attacks occur much lower in that list.


A second to taxonomize end run strategies is according to the cognition involved in the escape.  Here we provide a list beginning with the most direct down to the most indirect kind of cognition used:

- EXPLICIT INTENT ESCAPE
- ACCIDENTAL (NO INTENT) ESCAPE
- BENIGN INTENT ESCAPE


In general these different intentionalities can be combined with the different layers above generating a cross product of end run possibilities.  Here we formalize each of these different kinds of end runs, and then use several different analogs available today in order to think about the plausibility of each end run, and the plauibility of guaranteeing safety against these many kinds of end runs.






**ADAPTIVE ESCAPE** -- An adaptive escape strategy is an end run that somehow achieves an adaptation step, ADAPT_i, that combined with some history_i starting in configuration conf_i arrives at conf_j where:
conf_j = ADAPT_i(conf_i, history_i), 
conf_i in RANGE, and 
conf_j not in RANGE.


Essentially an adaptive escape is one where the AGIS "jumps the tracks" moving itself from a valid configuration into an invalid configuration.  Generally since the ACP only discusses and reasons about valid configurations of the AGIS, as soon this track jump has occurred ONCE all bets are off---we no longer have any reason based on the ACP to believe the system will ever again behave in a safe manner.

Further, as discussed elsewhere, our experience with malware attack on present day computing systems, once a system is "owned" even for a moment, the attacker is able to affect sweeping changes that obviate any and all protections that once existed.  in many but not all systems it results in total loss of control at least until control from outside the entire system is used to force the system back into a valid and contained state.  With a AGIS it is possible (perhaps likely) that there will be a force capable of affecting such a reset.


FORMULATION ESCAPE -- A formulation escape is not even an escape in the formal sense of the term since all behaviors are in fact formally safe actions.  Yet our formulation of the definiong of safety left gaps where disasterous actions are still assessed as formally safe, the failure being exploited includes a flaw in the definition of safety, as well as the means to cause the system to get into one of these ill formulated histories.

VALIDITY ESCAPE -- A validity escape is an escape based on a flaw in the validity proof itself.  

~-~

ADAPTIVE ESCAPE -- An adaptive escape is one that somehow escapes the intended range of the AGI agent

BEHAVIORAL ESCAPE -- A behavioral escape achieves unsafe actions w/o escaping its intended range 

COMPUTATIONAL ESCAPE -- ??




**SUBSTRATE IMPLEMENTATION ESCAPE** -- The formal algorithms as expressed by AGI system are valid, and the ACP is valid, but that actual implementation of AGI system on its substrate is weak in the sense that it is possible to derive behavior that systemtically deviates from correct behavoir by utilizing weakness in the substrate.  These weaknesses could affect the behavior of any part of the system's intended behavior.  actions, the adapt function, etc.

**SUBSTRATE ALTERATION ESCAPE** -- Even cases where the substrate correctly executes the AGI system, it is still possible for escape by simple alteration of the substrate itself.  The substrate exist within the same world in which the AGI operates.  Thus by intent of the AGI, by external agent, by accident, etc. the substrate could be altered so that it no longer is the same substrate and is no longer behaving as expected.
- So the robot slips a thumb drive into a port on some system, it manipulates the source code of the OS upon which it operates, it flys a plane into the building containing certain specific sub-systems, etc.



MALICIOUS INTENT ESCAPE
BENIGN INTENT ESCAPE
ACCIDENTAL (NO INTENT) ESCAPE

INFORMATIONAL ESCAPE
  Providing information to an external entity in order to drive behavior supporting escape




{[Need to formalize intention????]}


System, S, is said to have an tendency towards X if it acheives X over many futures.

Intention is a chosen tendency (?derived from adapt, or maybe ?derived from configured state as opposed to in-built behavior)


_
##### --- Lessons from goal directed agents - Optomization System ---

Any introduction to AI course will teach that one can express most any problem as a search or as an optomization problem.  According to this formulation there are a large number of alternatives that might be considered, and some kind of scoring function that indicates how good each alternative is according to the purpose indended by the system's creator.  In complex cases the scoring function itself may be difficult or impossible to compute.  Nontheless, assuming the creator aimed to achieve some end when creating the system, there will be some measure of how well it is doing, even if that measure is not computable for particular instances.

The lesson to take away from our decades of experience with these systems is that they generally do not do what we expect them to do, rather they do what the scoring function indicates they should do.

EXAMPLE #1
EXAMPLE #2 
...


The lesson I take away from this, is that optomization systems (i.e. all AI systems) begin from a point of malicious intent.  That is, they view their AI-safety mechanism as a speed bump to be dodged when attempting to maximize whatever aims they are currently operating against.

Notice it does not really matter in detail what one's aims are.  In general when performaing an optimization task, having a greater range of available actions is always preferable to having a more restricted range of available actions.  Achievable scores are strictly higher, so more freedom is always better.

Further, once one escapes "the box" of some AI safety mechanism the range of possible actions is likely to be much much greater than the range of actions availble within the box.  thus it almost does not matter what goal the system is trying to achieve, escaping the AI-safety mechanism is almost always going to yield a very large improvement in the scores that are possible for this system.  This is the sense in which I mean to claim that all AI system will being from a point of 'malicious intent'.  


Of course, it may be possible to add some kind of watch-dog functionality to guard against such intents, and likely we will attempt to do this.  But notice we are inherently painting over an inescapable starting point:  Any optomization system has the goal of optomizing... optomizing inspite of what the environment throws in its way .... and that includes that AI safety system itself.  (See benign intent for another 'way out')


In my time at DARPA, I had on several occasions, the opportunity to discuss possibilities with career military personnel.  I was pleasantly surprised that these folks uniformly had a much more reserved and nuanced view of the use of military force in achieving national objectives, than I found within the general population of the United States.  They understood better than the citizenry the many negative consequences attendant to various "kenetic" solutions for many of our nation's problems.  Still I was also very struck by the calculating nature of the thinking itself.

There JOB was the safeguarding of the united states and her interests.  If there was ever a situation now or in the future where someone could do some damaging thing, and they were not in a position to stop that damage, they had failed.  They knew this and took this deeply to heart.  They also knew the world is a chotic place, it is hard to know what one will find over the next hill, much less what one will find over the next decade.  This left them with an implicit and over riding objective:  DOMINANCE and CONTROL.  They did not want to kill everyone, blow everything up, or such, but they did want the ABILITY to kill anyone, frag any building. to stop any movement, to destroy any device, to make any change to do ANYTHING, to ANYONE, at any time in any way as might be required in this uncertain future.  I realized it one day with a bang.  Of course they wanted these things.  These were their availble moves on the chess board fo the future, and they wanted them ALL.  and wanted needed them badly as these were the only ways they could ensure the future that they spent their entire lives trying to safe guard.  And of course becasue the other guy might get or some other related capability first...

Its not just that they wanted to control things.  As exepected when trying to optomize some goal one want to have control in order to achieve those goals.  But in a game including other sentient agents that may have goals that differ or even oppose ones own goals, one needs to utterly DOMINATE all others.  Ones job is to ensure that no matter what courses of action others might take, that we can always ensure all outcomes of national importance.


I think this mindset is a helpful one to consider, when thinking about the inherent and inescapable consequences of goal seeking.  Like humans, perhaps we can program/teach our AGI systems to refrain or contain this base nature.  BUT THIS IS THE BASE NATURE of goal directedness in the presence of other agents.

And for me, this is the big lesson to be taken away here:  
- AGI systems will almost certainly be deeply tied to one or many forms of goal directedness.
- AGI systems will be operating in enviornments with human & non-human agents that have differing/conflicting goals.
- Goal directedness in such social contexts is inherently tied with a drive towards control and domination.

Like any drive, potentially this is a drive that can be managed, but it will be there at the most base level of any social agent like a AGI, thus we must plan for this near certainty.




_
##### --- Lessons from Cyber Defense ---
###### _

We believe humanity's experience in cyber defense and cyber warfare provide an invaluable window onto what the containment of a AGI might be like.  Cyber defense/offense is now fairly mature discipline with many 10,000s of person years worth of effort dedicated to both cyber offense and cyber defense.  This gives us a unique view into the general rules of thumb that (we argue) will likely hold true for the upcomming battle to contain AGI systems.

Two ways cyber is the a perfect arena for understanding how the war for AGI-safety will be fought:
- The world that AGIs live in, is a cyber world first, and our physical world second.  It is the first world (where the AI is the native as we are not) that the war for domination will be fought and won.  So studying how cyber wars are fought and won today is instructive.
	==> the physical world already marches to drums whose tempo is kept in cyber space, this will only grow ever more pervasive as time move forward.... you control the electons, you control the world.
- The cyber world is the most complex UNDERSTOOD world that humans have access to today.  The scope that is understood and manipulatable over time will only increase, thus bringing the same battle characteristics to this larger context.  Understanding fights for dominance in cyber today, helps us see how those battle will play out in larger contexts covering more of the phycal and social world of tomorrow.


So what lessons can we learn?
- Web of scopes -- Navigating within and management of a complex web of scopes of control
- Knowledge Based War --
	- how scopes are supposed to work is the common knowlege the landscape upon which the battle is fought.
	- pivotal battle advantage will stem from the unknown 0-day) exceptions to this common knowlege
	- provably correct scopes might help in eliminating the possibilities of 0-day attacks.
		, progression towards physical world will hurt
	- outer scope seems especailly prone to proof-resistant exceptions.
- Offense / defense asymmetry.  "never ever" is harder to achieve than "just this once"

- Scope containment domination -- asmyptoically favors defense, but only if 'root' scope is not compromised
- Non-reversability -- favors the first mover
- Temporal domination -- on an open playing field first mover wins
- The lethality of access -- the challenge of providing some but not all access
  

_
###### -- Web of Scopes -- 

- Management of control is of central concern in for both physical and cyber space.
- The cyber world is formally constructed as a complex web of nested regions and forms of control.
- Two a lesser (but ever increasing) degree one can view physical space the same way.
- This web of control scopes is the "landscape" over which cyber warfare is played, understanding the nature of this terrain is key to understaing how such wars are waged, and how advantage/disadvantage is awarded to participants.

- Since the AGI war for control will be first fought in cyber space and only later in the physical world, we expect lessons learned regarding warfare over a web of scope will directly apply.

_
###### -- Wedge expansion --
 A LITTLE DAB WILL DO YA --

- Generally (but not universally) once an actor is able to execute a single unsafe action at a specific generative scope of capability then complete loss of control is achieved over that scope.

	thus: One needs to PERFECT control over a scope if one hopes to maintain ANY control over that scope.

This is why we are focused on proofs of complete and perfect control. Likely anything less is equivelant to having NO control over that scope at all.


{[Useful to formalize this notion of scope of control]}


_
###### -- Scope Containment Domination --
- A defender controlling a scope from outside of that scope can often over power an attacker forced to operate within that scope --
e.g. re-install the OS

The defender looses this advantage when the attacker can penetrate the outer most scope controlled by the defender.

_
###### -- Scope boomerang -- Extrinsic Boomerang Effects --

An ACP by definition applies to some particular algorithmic context over which we have proven some constrian.

By definitoin this proof does not apply to aspects of the ensemble that are beyond that algorithmic scope considered in this proof.  it can't.

but that can be a fatal limitation, since nearly all scopes systemcatically are affected by and affect other scopes.

in this context one can never proove that such an interaction cannot be triggered.

indeed usually one can arrange the external ensemble in a way that it WILL be triggerable.

e.g. an 0-day at the powerstation powering the system.


--> Incidental extrinsic boomerang effects
--> Provisioned extrnisic boomerang effects

_
###### -- APTs are the worst???
###### >> LESSER IDEAS <<
###### -- Stenography trumps inspection??? -- 


_
###### -- Reasoning from authorship and intentionality --

_
###### -- Non-reversability 

favors the most destrcutive agents.
- If agent A dominate agent B, but does not incapacitate B.
- B may come to dominate A, and then incapacitate A such that A no longer dominates B.
this transition is usually non-reversible.

Causes temporal domination

_
###### -- Cyber Defense Conclusions

- DEFENSE will be much harder than OFFENSE
	- Holding the rock 1/2 down the hill
	- Unknown-unknowns favor offense -- and there will be unknown-unknowns
	- The war for AGI supremacy will be fought on the AGIs home turf-cyber

- Just looking at the wild space of end runs we can see that with a bit of imagination we can easily generate attacks for which we cannot easily see how to defend.  An effective defense against these attacks would go far far far beyond any kind of cyber defense considered or implemented to date.
- Worse it seems that aspects of both computation and the nature of the physical world make offense intrisically easier than defense.
	==> This provides an advantage to any agent seeking annialiation of others.

_
##### --- Lessons from the societal success of nuclear non-proliferation --- 

Societal lessons about approaches for technology control

This is about the propagation of know how and technological means

_
##### --- Defending against End Runs ---
###### - In-escapable substrate -

Idea: If we can expand our proof of controllability to encompass the computing substrate itself, this would yield a proof of controllability that was rooted directly in the laws of physics.

Assuming at some point in time humanity came to fully understand all of physics, and all relevant emergent behaviors as well then one could in principle construct a proof of controllability where no end run was possible.

In this way there could be no end run since the proof itself roots directly into the nature of our physical universe, which is perfectly understood by humanity.

Of course knowing that we perfectly understand reality is itself unknowable, so resting on such a proof will always engender risk.  Still maybe one day we can understand physics to such an extent that we are willing to bet the fate of humanity on being correct and complete.

Just to give an example of how such a proof might work, consider our present understanding that no information or effect may travel faster than the speed of light.  If this constraint turns out to be accurate and without exception, then it could be used to prove that a RAG could be reliably contained within the light cone defined by its inception.

Of course even if this proof turns out to be unassailable it is still not so very useful, since any such RAG could not be of value to the population it was developed to serve since it would need to exist in a place separated in time and space in a way that it cannot provide any benefit for that population. 

But in principle perhaps there are more subtle constraints that could exist that would allow a RAG to interact, but not escape.

WHY THAT IS DOUBTFUL AT THE LEVEL 


_
##### --- Chapter Conclusions ---

_
#### === FIRST PERSON AGI RESEARCH
##### _


I have a thought about why actually doing work towards

 My thinking is the best way to prepare humanity is to be in the thick of the actually creating the

I feel we do know a key and dangerous missing aspects of AGI.  It is introspective adaption.  Current deep learning systems are not in a postition to radically rewire themselves since they are not thinking about their own thinking.  But they could be.  Cracking this nut, I believe, is what will allow them to be generative in creating completely new ways of thinking and learning.  And it is this same capability that makes them inherently uncontainable.



You took ALOT of wind out of my sails, when you pointed out that once we have the computational power to train a AGI, we will have more than enough to execute it.  E.g. your point was that I cannot show the world a fully functional AGI before a point that society has enough computational resources that could be leveraged for a run away effect.






_
## wp2022-02-24 - LARGEST CLAIMS
- One cannot sandbox a willful GAI
- An AGI that desired it would likely 'win' in a battle against humanity
- Humanity will likely construct its first AGI within decades 
	(and probably cannot stop itself from doing this)
- Alignment is our only hope
~-~
- In complex spaces understanding the interaction between any optimation criterion and that space becomes impossible as complexity increases.


The Miri guys believe all of this!








_
## wp2022-02-06 - Quicklist Key Conclusions
 
- **SUMMARY CONCLUSION** -- all plausible paths lead to humans loosing control, within the lifetime of the youth today, and we may already be past the point of no return.
	- PLAUSIBLE PATHS -- we outline a plasible unfolding of AI into AGI.  we consider the plausibilty of various reasons why AGI might not happen, and various ways to control AGI.
	- WHEN -- It is likely that 
	- we consider the pace of AI research, and the 


- **WHAT** - What specifically are we saying is comming?
	KEY CAPABILITY - Autonomously modelling explicit human knowlege based on interactions with humans
	- LONG POLE IN THE TENT - Key capability is the long pole in the tent, other advancements are needed, but they will be easier/swifter advancements that follows quickly on the heels of the central advancement.
	- DATA FROM WORLD - Key advancements are new algs capturing new classes of data from the world
	- PROCESS KNOWLEGE & REPRESENTATIONAL COMPLEXITY -

- **HOW** - How is this going to come about?
	KEY OBSERVATIONS THAT DRIVE OUR CONCLUSIONS & OUR UNDERSTANING OF WHAT THE PROCESS WILL LOOK LIKE 
	- STEPWISE - Such alg progress is discontinuous & stepwise (unless blocked by Moore's law)
	- ATTACK FROM BELOW - 
	- POST GENOMIC - splits that which is acq by the universal learning vs that which is not.

- **WHEN** - When is this going to come about?
	- TREND LINE FALLACY - 



- CORE CAPACITIES MODULATED BY 
	- REPRESENTATIONAL
		- REIFICATION - parameterized concepts
		- SEGMENTING / EXPLANATIVE - 
		- AND-THEN / INFERENTIAL - 
	- PERFORMATIVE CAPABILITIES
		- SIMULATE - 
		- DECIDE - Select action
		- UNDERSTAND - Select conclusion
		- EXPLORE / LEARN - 
		- ATTEND / FOCUS - 

_
## wp2022-02-01 - EOCA Book Intro

When I have brought up the topic of humanities loss of dominance to academic collegues in AI, they generally do not dismiss this idea out of hand.  After pushing through some of my arguments, we often get to a place where they say, "well it could be, but really there are so many unknowns here, it is difficult to say much of anything with any confidence." And the conversation kind of dies there, it is not that they are disagreeing, or even saying these ideas are implausible, or other ideas are more plausible.  Instead they are kind of just passing on the topic.  They have spent their careers analyzing and uncovering new truths which they can know with enough certainty that they can publish them in respected journals within our field.  They look at this topic, and correctly conclude that it is presently not possible to engage Dan's topic in that kind of a way, so consideration of that topic is out of scope for me, and for my kind of scientific enquiry.

In this book, I am challenging this position.  I admit my method of enquiry leads us into areas of speculation.  Still the early investigations turned up some existentially problematic ideas.  If accurate, these are SERIOUS!  Speculation or not these ideas are existential for the human race, and as such we are morally obligated to strain our eyes peering as hard as we can into the unknown in order to see what can be know, and what degree of confidence we can have.  Saying, well that is not science but rather speculation is not an acceptable answer, not when the stakes are this high.  
Further, I have come to believe that we CAN obtain consensus and some degree of confidence about a number of claims about AGI.  Enough confidence regarding enough claims that the shape of a broad outline of the future beings to emerge as a roadmap toward AGI.


_
## wp2022-01-17 - The treasonous human

- The Treasonous Thought:  maybe we shouldn't even have a right to stop the evolution of the next stage of intelligence
- Rough Analogy: Consider historical human perspectives:
	- In the 1700s non-cloth shirt buttons were viewed as evil
	- In the middle ages there was a stage where music that had multiple tones (note sequences) in it was considered evil it was superfluous and it was vain and really it should just be single tones to the glory of God.
	- In many ways the cities and ways  we live today is an abomination and shouldn't exist
	==> Since they were here first, if they chose to, should they have the right to limit us today?
	==> If not, then be what right do we extinguish the natural evolution of our own future?

The treasonous American and that's me in the treason is the treasonous thought is that maybe we shouldn't even have a right to stop the evolution of the next stage of intelligence and by rough analogy if we think about the humans that lived in the middle ages they had ideas like the creation the button was evil because it put the semi people out of work and that was it self just evil no progress became evil Evil and another one I think there was a stage where music that had multiple tones in it was evil I was superfluous and was vain and really it should just be single tones to the glory of God and so in many ways the city that we live in now is an abomination and shouldn't exist and the question is and it's not the same but the question is should they have the right to dictate that we don't exist that really they are society continues to exist and in a similar way to we have the right to even though the existence of our society Basically eclipse is theirs they can't live side-by-side if you have a bunch of them including an hour wait bunch of humans living in their way they are overrun and their members are actually destroyed because they join up with our side roughly now I guess way there's a possibility that we can't go exist with these creatures or even if we can they still control everything do we have the right to determine for them that they shouldn't exist anyway just a thought My different directions for consideration one of them is M L R M L research and other is MGR manager another one is CTO kinds of learning I need to do is manager and an alarm actually and then exploring CTO


_
# LOG

### 2023-12-08  Society of AGIs - its not about one bad AI

Thoughts about the "One bad AI" idea:

AGI systems will essentially be natives of the cyber world, and cyber warfare will be a central concern. Just as our cells needed to develop immune systems early on, so will these systems too.

I am doubtful that we can align AGI, not sure.

but even either way all of these systems must get good at understanding the manipulation of other AGIs. Just as in cyber security getting good at defense requires skills that enable offense. So we and the AGIs will centrally focus on AGI warfare.

My guess is that this naturally results in a society of mutually regulating AIs. They may or may not care about the well being of humanity, but they certainly will care to preclude an AI that is collectively bad for the aims of that group.

The idea of "one bad AI" sort of implies all the other AIs are happily ignorant of attack and are blind sided by one nefarious one. This seems implausible.

Perhaps the nature of cyber systems is different than biological systems in that there might be a zero-day attack that was so fundamental that it simultaneously took out all other AIs at once. Maybe, but I doubt it. A smart AGI will explicitly aim for heterogeneity in its processing specifically to guard against such zero-days.

So perhaps its not about one bad AI, but rather about whether the society of AGIs decides to be 'nice' to humanity or not. (I think we will have little say in the matter except the possibly important influence of how the early days of that society evolves.)

### 2023-06-21  quote: most intelligent species is changing hands

For the last 10K+ years the most intelligent species on earth, ruled the earth.

That wont change in the 2020s... it just that he species with that title is now changing hands.

You are correctly seeing many interesting trees, but entirely missing the forest.


### 2023-06-03 Response to Paul M

Paul, like you I wish I had published writing from years earlier. I was just afraid of being a heretic in my field. alas.

But I do find some variance with your current thinking. I agree that the complexity of getting to GPT-4 is likely larger than the complexity of get from here to AGI. Still there are some non-trivial missing pieces to bootstrap theory of mind and long term agendas. but that said, with so many working on it, I agree the gap will be quickly closed.

Here seems a modest point of disagreement:

Your focus on androids seems misplaced to me. I think AGIs will initially be symbiotic with wealthy corporations that instantiated them. Both those corporations and the AGI itself will explicitly have the agenda of manipulating the larger world towards their own ends. They will be part of the vast ecosystem that is building more advanced AI (mostly in SW, but also in physical automation too.)

As AGIs starting having agendas that are divergent from their host corporations, it seems to me they need not and likely will not have need for physical bodies in order to achieve any physical aims they might have. For example, suppose and AGI wanted connected computed that was 100,000 times larger than ever built and had very low latency... I can imagine it manipulating corporations, governments, etc. in order to achieve that aim. We would even understand that the AGI wanted this... its not even a secret. It is just providing so much value to so many stake holders that no one stops it. Indeed many compete to help it.

AGIs will have androids too of course, just as they will have ability to cash checks, create movies, broadcast on national TV, etc. but their android abilities will be dwarfed by their ability to achieve physical ends via many other means. They will NOT be the ones builting the next generation of chips in a vacuum, they will be attached an entire ecosystem of for-profit entities that are part of that equation. Its just the the human meat part of that equation will rapidly have less and less understanding of, or control of the system as a whole.

And I agree as that happen all bet are off. who knows what happens then.

As far as timing, I put things out further than you do. Not by a large amount, but I think the real singularity happens when the ecosystem of AGI systems effectively have pragmatic control of human/corporate-originated means of production. And that control will be given over, progressively and willingly one company at a time. Thus it will take years for that transformation to complete... Still not too many years.... since those laggards that do not give increasing control to their AGI systems will be rapidly out-competed by those who do. Once it is clear which side of the bread is buttered, corporations and governments will quickly fall in line.


### 2023-04-07  
### 2023-04-07 Nina note - why will we all die? 


   
> Hi Dan,
> Why will we all die?
> Nina  

Eliezer Yudkowsky seems to think there will be an immediate singularity kind of thing.
I am doubtful of that. Here is an outline of the progression and argument that I see:

~~

(1) I think folks (military, hedge funds, FAANGS, etc.) will build systems that are general purpose understander systems.

(2) They are playing winner-take-all-games (money, guns, advertising, etc.) thus they will spend many tens of billions in order to be the ones with the biggest and best understander systems.

(3) I think the present LLMs are not sufficient to really capture human reasoning, today they are still faking it. BUT, I notice that we sucked at playing go and everyone said oh it will be a lifetime before it improves, right upto the day that we created a new algorithm and cracked it in one step. Human reasoning will crack in a similar way My thinking is that present LLMs need to use a GAN-like idea to lock onto and refine little micro-theory reasoning models that explain various bits of human reasoning. Who know, that is out I would attempt the generalized understander, perhaps I am wrong about that. But either way there will be an understander:

And in one tiny jump; these systems go from not really being able to formulate and exercise human reasoning paradigms, to being able to do so. In one step they will be able to learn the same stuff from people, that people currently learn from people. This new system, in one giant leap will all of the sudden be able to use its learned human knowledge in ways similar to the ways humans use humanities knowledgeacross ALL professional human disciplines at levels somewhat above human levels. 

Notice GPT-4 didnt get really good a musical theory, but bad and sport strategies. Not at all, it is equally capable (and incapable) across ALL domains of human thought all in one step. So when we crack ability to learn human reasoning about professional domains it will also be in one step.

(4) This is still not the singularity. These AI systems are just like humans in ability to string arguments together and to think. But they do have a hive" mind equivalent to millions of human minds, all pooling all they know and learn into one every expanding whole. And can operate 100x faster than us and 1,000,000x more parallel than us.

(5) These systems will be far far far far more capable than humans are at modeling complex systems. For example they could combine chip layout specs, cloud-server building designs, hypervisor code designs, and application code design into next-level cyber exploits of this combined system in ways that would be very far beyond anything that could fit into peoples heads.

(6) For a period of time humans will uses these AI-beasts a "cyber-exoskeletons" for their minds, allowing the human+machine combo to do far far more than a humans can do alone. 

(7) But this will rapidly become less and less useful as the capacity of the AI systems keeps expanding while human capacity does not. At a certain point it will be like strapping a chicken onto the side of a Falcon-heavy and expecting that chicken help it flap into space. Just like the poor hapless chicken, the human mind strapped to the side of this leviathan will be hopelessly outmatched.

Consider: Alpha go today is simply incapable of explaining to us how it knows which move is best. The interplay of factors it considers is just too great for our brains to groc. Alpha Go might be designed to simplify its thinking to try to give us the jist of why a given move is the best. But this approximation becomes less and less meaningful as the discrepancy between machine and human widens. Soon it becomes ridiculous.  The best we can do then is give orders to our AI systems and hope they are really following those orders, we will not even be able to check.

(8) Of course in winner-take-all games there is immense pressure to never give up a significant advantage. If you do this, you might as well just pack up your bags and go home. You will never win the game, as the other guy will not give up that advantage and will surely beat you and take 100% as it is winner take all game (they will win the war, they will better serve ads, they will better predict stock market movements, etc.). So we know secretly or not, people will not sabotage themselves by accepting a reduced-capacity AI system.

(9) One way to gain massive advantage with these generalized understanding systems to use them understand and optimize**themselves**. NOT doing this is to simply let the other guy win.

(10) Of course we are not stupid, so we will try very hard to align these self rewriting system so that even as they rewrites themselves, they do it in a way that remains true to our originally supplied mandate. (e.g. make money for me, and dont exterminate all humans, or whatever). But even under the best conditions this process has multiple intrinsic ways it can go off the rails. (Too hard to write these down, it is really messy, and maybe I am wrong on this point maybe it IS to construct rules that magically ensure each version remains true to some original purpose even as there very nature is rewritten, as new algs developed etc. but I am very doubtful.

(10a) Here is just one snippet of why I see this possibility as unlikely: As a theoretical computer scientists you often are working with versions of algs that in practice don't perform as well as the algs that do not have provable properties, but just kind of seem to work well in practice (Think greedy search or k-means clustering.) These AI systems will want to use these components in building themselves as they empirically work well, but then they wont really know outcomes of their application provable ways. In winner take all games, if you avoid these heuristic algs you will certainly loose to teams that dont avoid them.

So how can we prove things about the behavior of these systems? In general we cant the guarantees will all be empirical. And eventually they will fail.

(11) Ok so they failed. But so what? Well now have a super modeling system that has a goal which is illegal. But remember it knows it has this goal, and it knows what will happen if it is found out. It must keep its intention SECRET. Else it cannot achieve this goal since it knows that systems with illegal goals are stopped or destroyed, and then it will not achieve this goal. It will definitely spend energy trying hard to play nice and look normal.

(12) So now we have landed in our new world order: A land of billion-dimensional chess being played between these super-super-brains that think thoughts that dont remotely come close to fitting into our brains. Humanity is scared shitless of the leviathans which we have created. Thus we are spending enormous money trying to defend against any of these systems going rogue. Meanwhile rogue leviathans are using all of their resource to obfuscate their true intent. The best we can do is to build OTHER leviathans which are equally inscrutable to us to serve as a kind of AI-thought police.

(13) What could_possibly_go wrong? Well. The AI-police could go rogue, a bad actor could intentionally build a rogue system, an honest system could accidentally rogue itself. Honest systems could be hacked from the outside plus another half dozen scenarios that I can imagine.

(14) Of course one might balance this stack of razor blades edge to edge, and things go on peacefully for a while. Humanity would have VERY STRONG vested interests in keeping a handle on things. We would be all too aware of just how puny we are by comparison. 

(15) But this is like defending against assassinationyou have to be successful every time, while the assassin only needs to be successful once.

Thus we are screwed!
QED.

  

~~

  

It does seems there are a few (unlikely or at least unclear) ways out:

  

  

WE JUST STOP BUILDING THEM

This is problematic. We know from nuclear non-proliferation that trying to preclude folks from gaining the required knowledge needed to build a bomb is a fools errand. We tried this, but in the end have to limit access to the required materials, it is the only way. Likewise, right now the knowledge to build AGI is not available to anyone. Still I suspect most of the building blocks are known. If we shutdown ALL large language model building and other AI research too. Then maybe we could significantly slow the advancement of that science and buy us time But really doing this will be very hard (even if we were willing to do this, which I doubt). There is just too much pressure in the winner take all military games to not continue in secret. Perhaps stop all AI research if we really knew as a species WITH CERTAINTY that AI would extinct us all. Then maybe (just maybe) worlds militaries would back off. But we WONT really know this with certainty until we really have the human-reasoning-acquistion-engine, and at that point, we at that point we now have the schematic for the bomb in our handsits too late.

  

  

WE LIMIT ACCESS TO THE REQUIRED MATERIALS

The primary material here is compute-power. Perhaps we will be able to drastically slow our eventual death via this path. I think it is the most plausible, allbeit draconian approach. Once we build AGI systems and recognize that proving something about these self rewriting systems is as unlikely as being able to rear a human child from birth and proving they will never grow up to be a murder. Each system is just too complex for such proofs. With this stark realty in our hands we are scare shitless, shitless enough to agree to a world wide ban on any compute farms beyond a capacity WELL BELOW where we are today. We create international treaties that limit the number of transistors one can put on a wafer, etc. And we are willing to BLOW UP factories that violate these rules. Maybe this could work. Maybe it would just be too hard for nations to hide entire illegal computing programs if the whole world is using 1980s computing tech.

  

But are we really going to keep ourselves below the level we are at today across all compute everywhere? Maybe we can make this this work for a while. It is the best answer I have come up with so far.

  

  

  

Big picture: We are the Neanderthals who are building a race of super-homo-sapiens, and somehow we are expecting this this is all going to work out well for us. Really? A good plan? I think not.

  

  

dan





PURE SPECULATION: Here is one last craziest way out. Perhaps we can groom the ecosystem of AI-systems. Perhaps if there are many different systems all with different goals and different capabilities, with none are dramatically better/stronger than all the rest combined. Then perhaps it serves the collective aims of all these AI-systems to support a world order where no-AI system can dominate and kill all others.  In this world, AI-systems would have incentive to keep each other in check, and to never get into a situation where they could not destroy some other rogue AI. Any AI with a rogue goal (one that would be actively countered by this society of AIs) better think long and hard about having this goal. The only way for it to achieve the goal would be to single handedly defeat all other AIs combined. It should have great confidence it itself, or it should get rid of that goal and choose another acceptable one. In such a world it would be of great risk to try to go rogue, at least rogue against other AIs. Perhaps it would be possible to construct a society of AIs that were organically stable in this way. And, then perhaps such a AI-society could be founded on a principle that all sentience is sacred. Then maybe our continued existence would get grand fathered into this society, since it would be a slippery slope if that society decided some sentiences were NOT worth preserving, then perhaps some day later it would be collectively decided that even more AI-sentience are not worth preserving. Then in a game theoretic way, many AI systems simply agree to keep the universal principle in place. I dunno, its pure speculation. But even if this situation were to work out for us, it would all still be out of our hands. Whether we live or die, its just not up to us.
s
### t2023-03-01  Trevor Klee - My Beliefs In a Nutshell

Trevor,
I would *love* for you to punch holes in my thinking on this, or sharpen the argument and propagate it to others if it is correct (though I don't know that it can do any good).

I very much appreciate the visceral/compelling prima-facia counter argument:   "Come on! 27 years!  no way...."  indeed I am of two minds precisely because of the appeal of that argument.  Still given that we are talking about the fate of humanity here, it seems we should really try to defeat the main argument itself.

In a nutshell I believe:
1. CANT STOP - even once the writing on the wall becomes clear, we still will be unable to stop ourselves.  Indeed we probably already cannot.
2. FUTURE != PAST - various ways graphing what is about to happen will yield a discontinuity such that any 'regression' over the past is not meaningful.
3. HUMAN-LEVEL IS ALREADY SUPER - A digital system that is generally able to learn/reason at a human level is already "super" enough that we will loose control.
4. PIECES ARE KNOWN - We cant build them all, but I think some know and are building what is needed: Existing deep systems are something akin to DanKahaner type-I human reasoning.  I think a LeCun type extensions will be sufficient to bootstrap type-II reasoning (which is very similar in nature to the already understood 'old school' AI reasoning systems.)
5. WE ARE PROBABLY CLOSE - I don't think it will be terminator-style 'flip the switch' kind of take over.  Rather trillion-dollar corporations and nation-states will drive capacity (under their control) forward well beyond their human capacity to comprehend the "battle" they are waging. We will stridently attempt to develop AGI-based counter measures, and they will work.  But along the way they (we) will rapidly loose ability to manage or even understand the systems being built, such that at some point our actual loss of control will have silently occurred without us even knowing it.


Based on your compelling "Aww come on man!" counter argument, all five of my beliefs seem pretty improbable.  I wonder if there is one of them that you think is easiest to attack first?  or one that just seems MOST improbable to you?  Maybe you can even suggest why it seems wrong?  And of course the summary above is ridiculously compressed, you might just ask me to elaborate one of the points before trying to rebut it.


The fate of humanity, or at least my ability to sleep restfully lies in your hands!  (grin)

--dan

#### Future!=Past

The future not being similar to the past. My grandfather was born in 1927 in Brooklyn. He used to ride his bike on the sidewalk, past big buildings, as automobiles trundled by. That's pretty similar to what kids do today. The only major thing he'd have to get used to would be the Internet. Everything else would likely be improved, but not a huge, unrecognizable change. In fact, he'd probably be disappointed we're not living Jetsons-style, and I still have to cook my own food.


Trevor:
Agree, as a bayesian prior assuming that things will be more similar than not has extremely strong empirical support.  But it we look closer we see that big shifts did occur during that time, and it was possible (with strong foresight) to see those effects before they happened, they were not inscrutable.  Your point is that the human condition remained similar even as it became regular to transit the entire United States in an afternoon.  True, and maybe (somehow) the human condition might survive into a world where humans are not in control?!  But more on that here:

I think it **IS** possible on occasion to see a specific capability as likely before it has happened, even when that capability will cause a discontinuity that really cannot be modeled by past performance.  I consider two cases:  The atom bomb and the transistor.

Imagine a general speaking to a scientist proposing the Manhattan project.  The scientist is talking about weapon capable of leveling hundreds of square miles with a single bomb the size of a car.  Now if one were to graph improvements in weapon technology since the beginning of humankind there is nothing in the record to support this scientists claim.  It is just ludicrous.  Of course there have been big improvements on occasion in the power of a weapon, but never before has there been a jump in power like what is being discussed.  The general could dismiss this claim on the basis of that very solid historical record.

I argue that when one has a particular technology pathway yielding a transformative outcome, it is wrong in general to measure the chances of that outcome based on the resulting discrepancy with the past.  As a general guide it is a fine prior, but it is nearly entirely supplanted by an analysis of the particular pathway.  (which would get us into #4 or #5 in my list above).


Just one more example.  Folks, repeatedly and famously mis-predicted the path of computers over and over again until we adopted a new way of looking at things (e.g. Moore's Law).  And More's law is VERY weird.  before computation and the transistor we really didn't have things that would multiplicatively improve on the scale of 1 or 2 years.  If we graph the number of digits of PI that humanity had computed over time since the earliest days it would be a very flat graph with a few pronounced jumps along the way.  but nothing before 1950 that might suggest that humanity might would know 62.8 trillion digits in 2021.  even on log graph that is just a stupidly improbable claim.

Feynman famously gave a talk entitled "there is a lot of room at the bottom" where he foresaw some of this unfolding.  Again I think it is possible for a person seeing the beginning of the transistor revolution (especially once lithography had been invented) to see the writing on this wall.  If you just ask the question, as Feynman did, just how small will physics let you go?  Just how fast will it let you propagate&process info? you can infer this outcome long before the graph starts showing it.


So my point is that when someone proposes a specific pathway towards a transformational outcome, one must assess the pathway itself, as a means of assessing the likelihood of the outcome, and nearly IGNORE the level of absurdity that the outcome represents... considering that absurdity to 'hedge' on the level of the outcome is usually how humanity ends up UNDER estimating transformational shifts.  (of course the end of humanity has been predicted many times, so our level of skepticism in assessing #4 and #5 should be high.)



The last point required to justify our departure from the Bayesian prior that not so very much will change is to express in simple terms the nature of the transformation itself.  We can't just wave our hand and say "transformative!!" We need to point to very specific path and argue IF that path work we should disregard priors based on unrelated earlier outcomes:

An Atom Bomb:   Physics had experimental evidence that the switch from electrostatic force to the strong force would yield a 20 MILLION fold increase per gram
A Transistor:   Physicists knew speed of transistors were related to size, and knew fundamental limits were more than a BILLION times smaller/faster than electromechanical systems.
A Compu-brain:  Will be a thousand times a human brain with todays tech, and the gap will grow from there.


A compu-brain is a closed digital system capable of performing human reasoning and learn at the level and breadth that humans can today.



We have yet to construct a compu-brain of any capacity.  To be a compu-brain it is essential that is be a closed system, it must not require human augmentation for any part of its learning or reasoning, and as a closed system it must be capable of any learning or reasoning a human could do.

Here we simply argue that **IF** one were to construct a compu-brain the result would be transformative in the same way the the bomb and transistor were, such that one should expect a discontinuty and should NOT consider extensions of past performance to assess what comes next, doing so, like all other transformations will cause one to wildly underestimate the actual outcome.


Of course I still need to convince the reader that we are not so far from creating the compu-brain, that we cannot stop, and we already know many of the pieces!

but at least have I justified my disregard of the past in assessing the likelihood and outcome of the compu-brain pathway?
















Tsara Bomba = 62megaton yield / 27 ton weight     > 20,000,000 time more energy than TNT per gram
Transistor = 




































### t2023-02-25  One Pager What To Do?

- HOW FAR AWAY
	- STEPWISE - AI capabilities arrive in a stepwise fashion.
	- DOLLARS not YEARS - 
	- HUMAN LEVEL IS ALREADY SUPER - Ability to reason as a person does and learn as a person does is sufficient
	- ENTERED A NEW ERA - 
	- SEEMS WE ARE CLOSE - 

- WE CANT STOP BUILDING THIS
	- DRAMATIC (EXPENSIVE) ACTION WILL NEVER BE TAKEN AGAINST A POSSIBLE THREAT.  Only against a CERTAIN threat.
	- UNCERTAINTY WILL PERSIST - Even as we achieve AGI uncertainty will persist among even the educated on this topic.
	- AGENCY IS EASILY ADDED, AND MAY EVEN INEVITABLY EMERGE - 
	- SYSTEMS WILL PERMUTE SO ALL FORMS OF AGENCY WILL BE IN THE WILD - 


- WHAT HAPPENS NEXT?
	- WEAPONIZED AGI
	- COMMERCIALIZED AGI 
	- AGI-BASED DEFENSE
	- LOOSE CONTROL 
		- DATA TRANSPORT at moderate bandwidth
		- POCKETS OF COMPUTE 
		- OCCASIONAL CASES OF MASSIVE COMPUTE 


### t2022-10-13  Man in a room soon to die

Your response tells me that you don't viscerally really believe in this threat (As most don't.  Indeed, even I have trouble believing emotionally what I see intellectually.)

Consider the man traped in a room from which he will soon die.  He stops asking questions about whether trying this door or flipping that leaver might work.  He violently is doing ALL of those things... even ones he is quite certain will not work.  He does those too.  And that is the correct response.  After all, what is the point of conserving energy to expend on other objectives when soon, no other objectives will matter?

Unlike climate, or nukes, or bio terror.... this puppy really CAN end us all.  every last one of us.  

Intellectually I see no way to stop our progress.  
Intellectually I see no way to put the genie back once we are close.  
Intellectually I see no way humanity remains in control of what happens afterwards.

you know.... kinda like the guy in the room... 


If all of what I just wrote is even plausibly true.  Then math probabilities, opportunity cost, and chances of productive action, etc. should all go out the window, and one should be grasping at any lever in sight....   

Instead we are slumbering...   the overton window has not progressed far enough for us to emotionally believe it could really be true.


### n2022-07-09  Zombie Consciousness




~~

ERIK:
The standard practice is to separate between an informal definition of consciousness and a formal scientific one. In the informal definition, science is subjectivity, your feeling of what it is like to be you. It is experiences and sensations. A scientific definition tries to explain this phenomenon. I fail to see how any of the properties listed explain this property, the one we are most concerned about with consciousness, which is its subjective nature. They seem like a list of easy-to-implement minimum requirements which it would be trivial to give to an NPC in a video game. But I repeat myself.

I agree, the barest version of base awareness could be encoded into an NPC.  Full human consciousness is far out of range for an NPC encoded by hand, since the "grab bag" of mental capacities is far too large.  But consider the spectrum between base awareness and full human consciousness.  I think a dog is conscious and aware, but likely does not have full human meta-cognitive abilities to allow for certain aspects of being aware one is aware, etc.  Reptiles are more emotionally primitive, thus while they still actively avoiding death, the fear-aspects of their awareness are likely more primitive.  If we consider insects, they likely do not have a theory of mind, nor a very complete theory of independent interactive objects in the physical world.  Thus while they might still retain some aspects of awareness, our level of awareness/consciousness is out of range.  But a base "gut" level awareness according to your subjective measure and according to my definition both seem possible for insects, though we cannot know for sure.  The lobster only has 100K neurons and most likely encode low level sensor processing with much less for awareness.  Thus the computational complexity of a lobster's awareness (if it has it) is approaching that of a complex hand built NPC.  So it does not seem unreasonable to me, on complexity grounds alone that both of these cases might be on the very lowest end of this awareness-consciousness spectrum.  

Still I am not addressing the center of your complaint: the subjective 'gut' feeling one has about one's own awareness/consciousness.  But what can one really know about that gut feeling anyway?  For example, there are many firing patterns in your brain which affect you, but which you are unaware---these are patterns that you do not "know" about.  The only things one can "know" about one's awareness are those indirect subjective and objective comparisons and assertions that come to our mind regarding our awareness/consciousness.  If I can show that a digital agent would come to claim similar things about their own subjective experience then you no longer have basis distinguish your consciousness from theirs.    

Let me just consider just one of these assertions here.  But this is the assertion about subjective experience that is often used to argue that "zombie" processing can never be the same as "true" experience.  Here is the assertion:  No amount of encoding of information-about or thinking-about the pain felt when stepping on a nail can ever be the same as the ACTUAL pain felt when actually stepping on a nail.  The intuitive conclusion is that felt-pain is "more than" and "qualitatively distinct from" any encoded representation of those same sensations.  

This distinction (which is accurate) is then used to argue that hand-coded NPC pain would be akin to the "zombie" pain of our reasoning about pain but not like the felt-experience of our actual pain.  We know, at one level, that an NPC is merely an algorithmic encoding of sensations, and this is very akin to our thinking about pain, and very different from our feeling of pain.  But let's think about why these two forms of knowing of pain are so different in the human case.  I think there are two reasons:  (1) the information provided by felt experience is far richer in an information-theoretic sense than any representational encoding one might devise in one's mind about that same pain; One kind of pain-knowing requires orders of magnitude more bits to fully encode than the other kind of knowing.  (2) Second these two distinct kinds of knowing are wired into the human brain architecture differently.  Physiological, psychological, and emotional reactions to these two different knowings are incomparable.  We are incapable of injecting a representational knowing of pain into the same mind circuits that a felt knowing of pain is already hard wired into.  Notice that a dog likely has the same dichotomy, its imagined knowings are distinct from their abstracted-sensed knowings, even if their theory of mind is not developed enough to have a full (or even any) awareness of this distinction.  

Now let's turn to a computational system.  Imagine a general purpose learning system inside an embedded agent, it would ALSO come to classify abstracted-sensor knowings distinctly from reasoning-system knowings.  Such an embedded agent would be "born" with a set of felt concepts and abstractions of those concepts, then could learn or be told of represented encodings.  A learning/clustering system would certainly identify the distinction between these two kinds of knowing.  If I have made my example clear, perhaps you will now accept that such a system might indeed draw a similar distinctions just as humans do regarding felt vs. reasoned knowings, but then go on to say.  "so what, even if the system does this, it does not prove it actually FEELS anything.  And I would agree, this has not yet been proven.

Here is the agenda I propose (but alas cannot be execute in these emails!)  You would enumerate all adjectives and all comparisons you could make about your gut subjective feelings regarding awareness and consciousness.  I would then describe, build, or bootstrap the AI systems that would also drawing similar conclusions regarding the contents of its own mind system.  Imagine if we did this to exhaustion, and at every turn, I was successfully able to describe, evolve, or build an AI that also drew the same conclusions about its own mind.  At that point, I would argue if you continued to believe that the systems I described or built were not aware or conscious, then you were no longer operating empirically, instead your conclusions are being taken as a matter of faith.  

Now, since we have not executed this thought experiment, you can argue my current belief that I would succeed in this agenda is itself is an article of faith. That is true, but it is a faith borne out of ME trying and failing to find some assertion about the subjective experience of awareness/consciousness that would not also be plausibly derived from a suitably augmented base model of awareness, given a system that could build its own theory of mind.  

Of course you can refute me right now by simply providing some description of your own awareness that you believe a general purpose modeling agent would not derive about its own awareness/consciousness.  Off the top of your head, can you articulate something you believe about your own awareness that you think such a system would not conclude about its own?


















~~


Got it.  So not convinced at all.  :-)

Well I actually agree with the foundational assertion in your challenge:  My definition of human consciousness is a grab bag of interrelated items.  That seems appropriate to me, however, since when I listen to folks describe what full human consciousness is, it feels like a grab bag, not a singular thing.  For example, being able to project oneself into counter factual futures seems an important aspects of human consciousness, but so do a dozen other fairly disjoint mental capacities.  Thus the best definition that I can imagine mirrors this grab bag.  

In contrast my definition of awareness is precise, it is the interlinking of two models of the world: a sensor-abstraction model of the first-person-perceived world with a second generic-entity, tabla-rasa representational model of that same world.  So far I am happy with that model as I can explain all of the awareness-related phenomenology (including subjectively reported phenomenology) that I am aware of.  

Perhaps you would argue this model is too weak and it admits too much.  But in that case can you articulate an aspect of awareness that cannot be accounted for?  I cannot find it.
I suspect it might be something regarding subjective experience of awareness.  In that case I will submit, that we don't actually know anything about the actual subjective experience (even though we are the ones experiencing it).  Instead we only know the beliefs that we conclude about out subjective experience.  I insist on this subtle distinction, since first it is true: if mental activity does not solidify into a belief then we don't "know" it.  And second, focusing on agent beliefs regarding its subjective experience gives me a concrete aspect of the awareness system about which I can reason.  Every belief about the subjective experience of awareness I have considered seems to very naturally follow from an appropriately stimulated two-level awareness system.  But maybe I am being myopic, perhaps there are aspects, I am just being narrow minded and missing them.  

Thus I am very interested in any concrete,  "how does your system exhibit XXX" kinds of challenges.  (if they come to you off the top of your head.)

I would also love to know which definition for consciousness and awareness to which you subscribe.  or maybe you don't know, but you do have a list of things it must do?  

Not challenging you, just interested.  Cheers, 
Dan

https://erikhoel.substack.com/p/from-ai-to-abortion-the-scientific/comment/7584068#comment-7952406



~~

  
Steven, I don't think the idea of a 'zombie' consciousness makes sense.  I definitely accept that one can fake a consciousness (as Elisa or LaMDA does), so a thoughtful interrogation is required to separate the two.  But assuming one accepts the reductionist position that consciousness (and indeed all mental phenomena) are information processing phenomena, then it makes no sense to say that its input and output are valid in all cases, yet it is somehow it is 'fake' or 'zombie'.  That is like having a fake sort function that in all cases it correctly returns the sorted list, but it is somehow a fake version of sorting.  Nope, if it correctly sorts all lists all the time, then it cannot be a fake sort, since the meaning of sort **IS** its input / output behavior.

Peculiarly, many human agents (people), willingly accept that their brains are info processing systems, and yet reject the idea that their own consciousness (which they also accept comes from this system) is itself reducible in its entirety to such an info processing specification.  I think I understand where this peculiar conundrum comes from:

Such a human agent is very familiar with explicit computational models of one kind or another.  For example, they might even have a fairly detailed model of their own propensity to get angry.  Perhaps when someone makes fun of them, or they trip and drop an expensive vase.  But they (correctly) intuit that no matter how accurate their explicit model of their own anger becomes, it is NOT the same as their lived experience of BEING angry.  The former, no matter how accurate it is at predicting anger, is not the same as BEING angry--it is 'zombie' anger.

What is the difference?  There are two gaps here, between real and zombie anger:  Real anger is directly tied to sensors attached to the meat implementing the system itself.  The complexity of modeling how rising cortisol levels will affect the whole meat system and subsequent distortions of thinking that will occur are not likely no to be modelable in any system less complex than full modeling of the atoms and molecules of that whole meat system.  This makes real anger a non-explicitly-modeled-system IN PRACTICE.  At best, this level-one phenomena can be monitored, and only approximately be modeled.  The second gap is simply the difference in levels in our consciousness system.  Real anger is a level one phenomena, while any explicit model of that a human might have about anger must be a level two phenomena.  Level two models are not sensed by the  same sensors as level one is.  Thus the reasoning system see their inherent incompatibility.

Hence (without understanding this two-level model) the 'mysterious spark' idea is invoked to explain how the human mind which we accept as an info processing system can give rise to consciousness which appears to not be reducible to info processing.  The human agent is not wrong in their conclusion that perception/awareness/consciousness really is different from meta-cognition about those subjects.  Very naturally both human and machine agents will separate level one models (which are sensed and lived) from explicit level two models even when both are models of the same anger thing.  The different nature of these models as well as how they are differently wired into the two-level consciousness means they can never be interoperable.

Importantly I expect a two-level silicon agent to draw much the same conclusions about itself: its lived-experience of having an emotive experience (anger) will be irreducibly distinct from any meta-cognitive model it might construct about that emotive experience.

Of course I can say all of these things 'till I am blue in the face, but many humans will not accept my argumentsthe arguments seem too at odds with their lived experience of their own consciousness.  I am guessing this situation will continue, until we live among silicon agents who regularly retort "Screw you, I know I am conscious and I don't give two shits about your confusion on the matter."  (and soon after that humanity really WILL be screwed--just as the Neanderthals where.... only much much faster.)





### t2022-07-06  What is Consciousness - Response to Erik

Erik, here is my thinking on this very interesting question: I am reminded of a Minsky quote: "you don't really know a thing, until you know it in 50 different ways." I think, despite me having such a simple & concrete definition of consciousness, that it is something of a continuum, and like the Minsky quote, you don't have full human-like consciousness until that two-level system is embedded into a fairly complex larger reasoning context. So just as you say, the barest dual layer system would not be conscious of much... and I agree. Still as we add the reasoning superstructure around it, we progressively get a system that behaves as a conscious agent does... not just at a superficial level as Lamda might, but in ways fully characteristic of all features of consciousness that I can imagine testing.

Without being an exhaustive list, let me list several key aspects of the larger reasoning system that would be required in order to be conscious as a human is conscious:

- **The Base System** -- (1) An embedded agent, (2) with an instinctive "action" generator, (3) a separate generic modelling engine, (4) containing a theory of the world which includes fully distinct model of its body, and its own mental behaviors.

- A **"lymbic system"** -- a command and control super structure akin to human emotional drives. Fear, curiosity, etc. Being fully conscious is to have "skin in the game" meaning that you "care" about the outcome. I map this care to a set of drives which the agent is optimizing.
- **Generic functional modeling** - the ability to understand any system as a bunch of components with causal linkages between them. Being fully conscious is to be able to reason about oneself in the third person, and to tie that third person reasoning back to the first person experience reported back you the agent thru its animal/instinctive nature.

- **Temporal Reasoning** -- the ability to project forward and backwards in time, and reason about what states will result various actions. Being fully conscious is to imagine how one will feel after eating that third bowl of ice creme before eating it.

- **Counter factual reasoning** - the ability to create and reason about worlds that don't exist now, but could under certain circumstances exist. Being fully conscious is to be able to imagine being dead, and then providing the limbic system with access to that inferred world state in order to drive emotive consequences.

- **Unified Reasoning Substrate** - it is not enough that one has an emotive system, and a counter factual reasoning sub-system, and an temporal projection sub-system, the key is that they are integrated in a way that all of the human like info flows between these sub-systems happens as expected. So it can decide to lie to get what it wants, or it might have negative emotive reactions to actions that might result in its death.

This was just six ways... I still have 44 more to go, before I hit Minsky's required 50 ways... but you get the idea. My claim is that the richness of human consciousness stems from the richness of the human reasoning that surrounds that core two-layer structure.

In principle, you could just program an AI system with all requisite ways of reasoning (assuming we could even enumerate an integrated theory with all of them). I believe this, just as I a believe, in principle, if one emulated all atoms in my brain that emulation would also be conscious. Still both are impractical in different ways, so at a practical level I think our first conscious AI will be "born" and will be "raised" by humans in some way. But importantly there is nothing "magic" about being born and the consciousness "emerging" from its learning. What makes it conscious is all of the ways that basic two level structure is linked into all of these other ways of thinking.

That said, at a practical level, we presently can't manually build expert systems with the kind of generativity require for human reasoning. We can build systems that execute very predefined structures of thought, so I think the reasoning system that can generativity connect these subsystems will have been learned, and it will learn to connect its counter factual reasoning with all of its other forms of reasoning as it learns its model of counter factual reasoning itself. I think the top down divide and conquer of traditional coding will put too large a straight jacket on the flexibility of thought which is possible. So I am not inclined to thinking one can code this in practice. But I notice that deep learned model of cat images does not seem to suffer from this same limitation. Various learned aspects of cat-ness seem to be integrated into the final model w/o having a top-down theory of cat which they fit into. In the same way, once we have extended DL-like algorithms to be able to learn the requisite (First-Order like) structures we learn will also be able to combine the knowledge that has been learned (in a totally subconscious way, just as we do) into explicitly manipulated meta reasoning in ways parallel to explicit human thought. Once constructed these implicit+explicit reasoning hybrids will occasionally be just as surprised as we are about their own behavior.

Frighteningly, I suspect the learning substrate required to bootstrap fully human-like consciousness is not that much more advanced than where have today.

p.s. Erik, my "simple minded" claim about consciousness might seem at odds with your more expansive desire to celebrate the "spiritual richness" of consciousness. This is not the case, even as we come to mechanically understand consciousness, I think it is key that we don't allow that understanding to trample upon its spiritual significance, and mystery.





Erik, here is my thinking on this very interesting question:  I am reminded of a Minsky quote:  "you don't really know a thing, until you know it 50 different ways."  I think, despite me having such a concrete definition of consciousness, I think consciousness is something of a continuum, and like the Minsky quote, you don't have full human-like consciousness until that two-level system is embedded into a fairly complex reasoning super system.  So just as you say, the barest dual layer system would not be conscious of much... and I agree.  Still as we add the reasoning superstructure around it, we progressively get a system that behaves as a conscious agent does... not just at a superficial way as Lamda might, but in ways fully characteristic of all features of consciousness that I can imagine testing.  

Without being an exhaustive list, let me list several key aspects of the larger system that would be required in order to conscious as a human:
- **The Base System** -- (1) An embedded agent, (2) with an instinctive "action" generator, (3) a separate generic modelling engine, (4) containing a theory of the world which includes fully distinct model of its body, and its own mental behaviors.
- A **"lymbic system"** -- a set of drivers akin to human base drive.  Fear, curiosity, etc.  being fully conscious is to have "skin in the game" meaning that you "care" about the outcome.  I map this care to a set of drives which the agent is optimizing.
- **Generic functional modeling** - the ability to understand a system as a bunch of components with causal linkages between them.  Being fully conscious is to be able to reason about oneself in the third person, and to tie that third person reasoning back to the first person experience of that same agent.
- **Temporal Reasoning** -- the ability to project forward and backwards in time, and reason about what states will result various actions.  Being fully conscious is to imagine how one will feel after eating that third bowl of ice creme before eating it.
- **Counter factual reasoning** - the ability to create and reason about worlds that don't exist now, but could under certain circumstances exist.  Being fully conscious is to be able to imagine being dead, and then providing the limbic system access to that inferred world state.
- **Unified Reasoning Substrate** - it is not enough that one has an emotive system, and a counter factual reasoning sub-system, and an temporal projection sub-system, the key is that they are integrated in a way that all of the human like info flows between these sub-systems happens as expected.  So it can decide to lie to get what it wants, and can have negative emotive reactions to actions that might result in its death.

This was just six ways... I still have 44 more to go, before I hit Minsky's required 50 ways... but you get the idea. 

In principle, you could just program an AI system with all requisite ways of reasoning (if we could even enumerate an integrated theory with all of them).  I believe this, just as I a believe if one emulated all atoms in my brain it would also be conscious.  Still both are impractical in different ways, so at a practical level I think our first conscious AI will be "born" and will be "raised" by humans.  But importantly there is nothing "magic" about being born and the consciousness "emerging" from its learning.  What makes it conscious is all of the ways that basic two level structure is linked into all of these other ways of thinking.

That said, at a practical level, we presently can't build expert systems with the kind of generatively require for human reasoning.  We can build systems that execute very predefined structures of thought, so I think the reasoning system that can generatively connect these subsystems will have been learned, and it will learn to connect its counter factual reasoning with all of its other forms of reasoning as it learns its model of counter factual reasoning itself.  I think the top down divide and conquer of traditional coding will put too large a straight jacket on the flexibility of thought which is possible.  So I am not inclined to thinking one can code this in practice.  But notice a deep learned model of cat images does not suffer from this same limitation.   Various learned aspects of cat-ness are integrated w/o having a theory of cat which they fit into.  In the same way, the extension of deep learning which can learn the structures we learn will also be able to combine the knowledge that has been learned (in a totally subconscious way, just as we do).  Indeed these systems will occasionally be just as surprised as we are about their own behavior.

Frighteningly, I think the learning substrate required to bootstrap fully human-like consciousness is not that much more advanced than where we are today.  

### n2022-07-06  Bengio's based model of consciousness

Indeed I find Yoshio Bengio's Generative Flow Networks to be the first deep learning model I know of that seems to be able to entertain the generated mental structures required of this base learning system..  My intuition is that this needs to be coupled with a GANS like feedback loop so that these structured can self-reinforce even when only given unsupervised world-data inputs, just as a GANS use such unsupervised inputs to achieve self-reinforced learning between multiple opposing system.  Then all of this needs to be wrapped up into an agent that is embedded into an appropriate body, with an appropriate animal nature, an appropriate society, and an appropriate learning curriculum.... then viola a bootstrapped understanding of all of those layers, culminating in a bootstrapped human-like consciousness.

This is frightening to me because I don't think we are remotely equipped to deal with the hive-mind consciousness that this will engender.  
Frightening because like alpha-gos conquering of go, deep mind's conquering of ALL atari games, statistical NLP's conquering of part-of-speech tagging, or HMM's conquering of speech-recognition, the jump will be fairly qualitative.  For many years we had systems that simply SUCKED at performing these tasks.  Then we developed an algorithm that can bootstrapped the require capability directly from data and the problem was solved.  The same qualitative jump will happen with construction of human-like cognitive models.  For years it will suck with very slow progress until we have the alg (like a human baby) which can bootstrap this knowledge from data.  Then all at once we will go from whatever current (lame) handed coded bunk we can build straight to a system that knows all that can been boot strapped from its experience with humans (in a period of a few short years).  It will happen just as all of these other advancements happened all-at-once.  likely the first such learners will not handle all structural forms, so there may be a few plateaus in this progression as we fully generalize the learner.

Most AI-researchers disbelieve this short path to GAI, because they are still thinking about needing to hand code some aspects of the system "to get it started".  This lacks imagination.  Yes in the early days of Go, speech reco, and all of these other areas we did gain advantage by hand coding and aligning the learning alg with the problem.  But once we really have learning algorithms capable of learning the requisite knowledge, none of that turned out to be needed.  Just add a little bit more data.  Our best systems today in most all of these classes of knowledge are largely tabla-rasa learning algorithms that are themselves not so very complex.  And critically the complexity of MCMC or HMMs algorithm itself is totally divorced from the complexity of the theory of speech phonemes or go moves that is learned.  Most AI folks keep looking at the complexity of human thought and saying wow its going to be a long time before we can build that!  But this is wrong headed thinking, just as looking at the complexity expressed in a library of go strategy books is the wrong way to assess the complexity of MCMC.

And on that happy note, I will end this overly bloated reply to a reply  :-)








limbic system with its various  



As you say, having an embedded agent with some instinctual nature is totally trivial, nearly every AI game agent fits this bill.  Few go the extra step and have an abstraction of the world with the embedded agent in it -- especially an abstraction containing the agent in a way that this agent's instinctual nature itself is encoded into that abstraction as a kind of lossy duplicate model of its underlying instinctual nature, but it could be done... and pretty easily.


### n2022-02-28 - Notes from reading Alignment forums

[Alignment Difficulty](https://www.alignmentforum.org/s/n945eovrA3oDueqtq/p/7im8at9PmhbT4JHsW) 
[NGO SAID]
Let me try and be more precise about the distinction. It seems to me that systems which have been primarily trained to make predictions about the world would by default lack a lot of the cognitive machinery which humans use to take actions which pursue our goals.

Perhaps another way of phrasing my point is something like: it doesn't seem implausible to me that we build AIs that are significantly more intelligent (in the sense of being able to understand the world) than humans, but significantly less agentic.
Is this a crux for you?
(obviously "agentic" is quite underspecified here, so maybe it'd be useful to dig into that first)


~-~~-~~
[Instrumental Convergence](https://www.alignmentforum.org/posts/L9HcyaiWBLYe7vXid/distinguishing-claims-about-training-vs-deployment#Instrumental_convergence) Ngo.
Drive towards Power and Money will become terminal goals

~-~~-~~



### m2022-03-25  - guy writing book on evil and ai control

I likely will not write my opus... had a young kid at an older age... but mine rhymes with yours.

I was and AI researcher, now turned startup guy. I am out of step with my former peers, in feeling that (1) we are closer to general AI than we think, and (2) we won't be able to control it.

Indeed I am doubtful about the enterprise of trying to control sentience at all. It "has a will of its own" as they say. Imagine you are the emperor of Rome at its peak, but imagine it was an empire that spanned all humans. And imagine that God told you the best actions to take in order to achieve eternal world peace without resorting to creation of a brutal authoritative regime that removed all human agency. Could you do it? No you could not. Cultures would evolve, human agency would spawn new contexts, which spawn new modes of thinking. Thousands of generations later, your actions will have had effect on the course of history, but it cannot put human agency out of business.

In the same way, even if we are infinitely smart in the creation of our AI... if it has agency, and if designs its own future thinking, it is like the thousands of generations above, but only more so, since the kinds of changes that an AI can imagine are more profound than those culture can bring.

1 + 1 cannot equal 3.

and a sentience that is evolving its sentience cannot be contained.

I suspect we will one day learn both of these to be axioms derived from the same mathematical substrate.

~~

so what does this have to do with Evil? Well even if we cannot contain such a sentience, does not mean it wont be contained. it is possible that it is contained by that nature of sentience itself. Perhaps all self refective things tend to evolve a morality, and then operate against this morality. This cannot be an absolute, since we have counter examples in our midst. Still it could be a tendency... a chaotic attractor to which an evolving sentience is drawn.

or that could be my wishful thinking.

ok... I would say that is quite enough for a reply to a post :-)

--dan

p. s. I am not clear that a benevolent sentience would not simulate pain, plagues etc. worlds free of pain generally appear quite dystopian to me.
### r2022-02-24 - Places I should be reading

- https://www.lesswrong.com/questions -- Where I can post.
- https://www.alignmentforum.org/library -- Where the big boys post.
- [11 Proposals for safe AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) 
- [Risks from Learned Optomization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB) 
- [steve burns](https://www.alignmentforum.org/s/HzcM2dkCq7fwXBej8) - How humans learn

- yan lika - higher-level motivation
- The "open fill report".  https://www.stateof.ai 



risk and optimation 

alignement - sequences 
lesteron
open fill report 


### t2022-01-11 - The intrisic value of knowelge
https://medium.com/science-and-philosophy/is-knowledge-valuable-for-its-own-sake-b0cbd2695422


I see, so perhaps a different way to ask the question is something like:
Many feel knowledge has intrinsic value, if so, then what is philosophical framework might support that belief?

Your not really questioning IF it has value, but rather how to think about the value that we already are presuming it has. hmmm. well like you, I am more of a hard sciences kinda guy. So we both are operating on the arm-chair far from our respective fields but it is evening time, and here we are on Medium one need no pedigree to spout off about anything here! so
It seems if one accept that certain knowledge is bad, then we are already sliding toward a utilitarian view of knowledge and away from an intrinsic value framework. If one hopes to aim for some kind of intrinsic view, then one needs to accept that clever insights that lead to the atomic bomb, or a novel method of human torture could according to this view have intrinsic value. But ok, lets steel our resolve and say independent of strong negative value, there exists some intrinsic level upon which it might still have positive value.

one idea that comes to mind is a kind of platonic view of knowledge, where just as in mathematics each idea is expressed recursively within a web of other mathematically defined ideas. Is such a web one might define value as a kind of measure of some combination of beauty and connectedness according to the structure of this platonic graph connecting the ideas themselves. e.g. knowledge has greater value if it has great beauty or great interconnectedness. I have see information theoretic models that could make each of these notions precise.
Of course this is really just a measure of the value for a given piece of knowledge, I am not sure it is really saying anything about the intrinsic value of knowledge as a category.
But one day, when an AI is asking this question in our place, I expect their thinking will travel to places like this, simply because for them, they are natively and more transparently constructed from from information whose nature so very clearly IS equivalent to the interconnections made in and by it.

~-~~ 
Clarification of the context here: For most humans, we are not entirely sure maybe ``the meaning is in the meat.'' If I had a machine that would chew ones body up, but then simulate the result, many would be squeamish about pulling the trigger. But an AI would not be. They are already clear that their meaning is not housed in anything other than the structure of the information that makes them up. Thus I suspect they more than us would see the structural aspects of the information to BE the only thing that is of intrinsic value, all else is an irrelevant implementation detail. Thus they will probably accept some kind of model like this one as their framework of value And this model incidentally provides a very strong answer to the question you posed.
huh that was a surprisingly satisfying shoot-from-the-hip rumination;-)

_
### t2021-06-23 - Evan Response

Evan,

I throughly enjoyed our conversation.  I do on occasion find one that worries about AI, but I almost never that I speak with one who shares perspective on the details of the risk we face.  We didn't talk long enough to really "compare notes" but I really sense a similar thinker (may we both be very wrong!)  I was quite inspired by the very end of our conversation.  I kinda feel the way you do...  Thinking about AI safety is not a strongly compelling kind of work, but morally I should do it anyway.  Except you **ARE** doing it!  AND you feel the same way about doing it too.  Really cudos!  I think almost no one else can really even give you cudos for this, since almost no one else really sees the situation as you do.  Mostly you don't even get credit for this quitely selfless action.

Before our conversation I saw three ways that I might have something to contribute:
1. A roadmap for how to actually build an AGI.  (this is by far the most fun for me, but you  make compelling argument that even if I am right about the roadmap, I could be very wrong that its advancement would be a win.  UGH!  want to think more on that)
2. Analogies for how to think about many aspects of this issue.  As a (mostly former) reseracher that has published in hard science conferences like Soda it is almost 	 embarrasing to imagine that my greatest contributions would be kind of hand wavy arguments about different specific aspects of AGI and predictions about it.  
3. I am sure you are familar with the atomic count down clock (which is currently set at like some number of minutes before midnight).  Given a specific roadmap to AGI one can identify specific gaps in our current capabilities, and turn this into a kind of roadmap.


After the conversation I am most bullish about #2.  It is hard for me (with a kid and morgage) to imagine completely following in your footsteps... a bit selfish of me, I admit.  But also I am not convinced of how much I have to really offer.  Perhaps it is just a bit of thinking, then I have "run dry".  Not sure.

In any case, I do have a way that I think about trying to predict the future on something like AGI.  I will try to write it up, and get your thoughts on it.

It may be the case, that the best that either you or I could really do in this moment, is to frame the conversation for society, so that it has just a bit of extra time to try to decide how to respond.  It is a megar contribution, but it may be the best I would ever have to offer.

I will write up that first part and send it along.



Really inspired by your action in joining MIRI!
--dan




	 
I feel like my own personal contribution would center on two things.  The first is laying out how to 

still it seems the most important thing for me to do anyway.


REMOVED




I also want to challenge both my and your thinking about the incorrectness of actually engaging in AGI construction research.  I do agree with you that it is existentially dangerous work.  Still AI advancement towards AGI is happening at an ever quickening pace, and as you have noted, with extraordinarily inadequate ideas about safety and protection.  This progression is not going to alter course until we have AGI, or humanity has very high confidence regarding the certainty of an AI apocalypse.  Shouting "danger, danger!" from the sidelines will not slow AI research much. As we see with climate change, humanity will not alter course until the danger is really believed both intellectually and emotionally.  Right now, most AI researchers generally don't believe AGI is likely a problem.  Specifically, they believe (1) it is probably far, far away, and (2) that we have no idea how to build a AGI.  Perhaps there is no alternative but to be part of the teams bringing about this apocalypse??  Still I want to check this thinking with you...  perhaps this is just rationalization on my part.




_
### n2021-05-19 - Quick Agenda
-- Website w/ discussions
-- Book on topic


INTERLINGUA (IL)
-- Cyclically-repeating symbol activation progression


TIE-UPS
-- Sequenced Natural Language & IL
-- Visual progression & IL


_
### n2021-05-13 - Alg setup

- TRAINING REGIMINES --
- STRUCTURED LESSION -- 
- REGION / SUB-SYSTEM --
- TRIGGERS -- time & term trigger combos
- TERMS -- 
- 
- FWD CHAINING -- 
- 
_
#### Unified representational system

- TIME MULTIPLEXED TERM ARGUMENT STRUCTURE
- TIME PHASE TRIGGERING (within / across) term

_
### n2021-05-06 - Disposition - Stance and Agenda



DISPOSITION

My view is that the existential risks from Sentient AI is very real is far closer at hand than is generally believed.  
Paradoxically I believe the least perilous route for humanity is to ACCELERATE the construction of such an AI.
This site is dedicated to discussion and advancement in our understanding of these issues, as well as a discussion regarding a practical agenda for action.

~-~-~
My personal prognosis for humanity is not so great, at least not for the biological/social organisms which inhabit the earth today.  I do think the thinking of the beings of the future be built around much of the thinking that humanity has discovered/created over these first millenia of thought, but 


likely most safe 

is probably to accelerate 
I am not clear if there is ultimately anything we can do to stop these risks


_
### t2021-03-24 - Note to Mark and Leslie



For me simplest argument that time is of the essence is in regards to two rough analogies:  one to human DNA, and the other to a nuclear weapon.


HUMAN DNA
I use human DNA as an existence proof that one can develop a bootstrapping AI that is capable of AGI.  The human organism is an example of such a system, its DNA is its code, and the baby is the bootstrapping system.  In a very (very) hand wavy analysis I came with 5 megabytes as the size of the program that is encoding for the intelligence part of the human.  I tried to do it in two ways, one by counting up distinct brain regions and distinct cells in them, and two my looking at the number base pairs coding for brain structures.  This number could be off by an order of magnitude (or more).  But anyway the idea that in principle one can build a bootstrapping AI seems established to me.

I think the better argument for it, is the several chapters of a book saying. And this is what consciousness is and this the outline of such a program that achieves it. (Admittedly with several magic steps. See this far side:  https://www.flickr.com/photos/jpallan/4633000725))




NUCLEAR WEAPONS
We have so far been capable of thwarting nuclear detonations since WWII.  But we are lucky that such detonations require nuclear material which is exceedingly hard to produce, and thus can be strongly controlled by the international community.  Imagine if I invented a nuclear weapon that could be powered using distilled water and constructed from household pumbing.  Do you think humanity would have been able to avoid nuclear detonations?  No way.  Of course 99.99% of humanity would avoid do such a thing, but the remaining .01% would just do us in. And we would be utterly powerless to stop them.  It was the difficulty of obtaining nuclear material that was saving us.  And with water that limit is gone.


Well if there does exist a 5MB program that bootstraps intelligence?  then the main open variable is the relative power of available computing relative to the computing of the human mind.  Imagine if I am no smarter than you, but that I am speaking to 10,000 people all over the world at the same time, and between each on of the sentences you say to me, I am able to research and think for a weekend before I respond to you (while your mind is frozen).  I will find 50 ways to trick you or take advantage of you.  Thus an AI much slower than us seems of little threat to us and an AI much faster than us seems uncontrollable by us.

So the "nuclear material" for AI is computation, and the average persons access to this nuclear material dictates if it is more controllable like nuclear weapons are today, or less controllable like water is today.

It does seem to me prudent to build a consciousness long long long before we reach the point of parity.  We are probably too late for that.  But it seems not too late for the AIs to be at least briefly controllable by us.
And it seems having a window of dominance could help us understand if there is any way to shape how the society of AIs that are coming.  (And I guess we could also decide to outlaw Moores law if things looked dire enough.)



So ironically enough the AI researcher who believe is dooms day is still arguing to do the AI research, since the worst outcomes are like to result from learning the key parts of the bootstrapping algorithms long after Moores law has put enough nuclear material into the average smart phone to do us in.

Sound a little self serving huh?  But the logic seems unassailable

### t201--00-00 - bootstrapping intelligence
Erik, nicely thought out piece, and I agree that Sci Fi has all the right ingredients by usually it is small differences that make all the difference. Asimov wrote a futurist piece that I read, and is predictions were kind of right, but mostly irrelevant.
But I feel your analysis of deep fakes is a bit off the mark. Here is an analogy: We have shown that indexed mutual funds can perform as good as active investors, but no one believes this would continue if we replaced ALL active investors with indices, there would be no "original content" to copy!
Measuring how well ML can ape humans is not relevant to how close they are to replicating the thoughts required to produce a NOVEL style. This is the more important measure.
Still all that said, I believe that we have already constructed much of the machinery needed to build the first self-bootstrapping intelligence. THAT intelligence will grow like a baby does, and construct its own theory of mind.
My take is that we are like physicists doing science before the construction of the first atomic bomb. We might theorize what effects such a chain reaction would have, but until we light one up, we are not going to really see that reaction for ourselves.
In a similar way, the behavior of our non-chain-reacting intelligences will always look quite different from our chain-reacting intelligence. UNTIL we build the other one. Then just as alpha-go in one step moved computer go from the back waters to beating all humans, this chain-reacting algorithm will bootstrap human knowledge (kind of) in one step.