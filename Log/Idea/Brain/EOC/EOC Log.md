

### 2025-05-15 Long ASI note

We can't stop because we likely can't stop because we understand the gap between ourselves and ASI, a particular kind of ASI, and that gap doesn't involve any further information. The most easily gated capability is no longer a barrier, and the remaining capabilities are really research that there's no practical way to actually limit. Specifically... The things that are missing is the ability to learn without retraining the way that the human hippocampus and cortex interact to perform this function. And the ability to maintain... Structured reasoning over extended periods which may devolve into that other capability. It may devolve into the other capability because the short-term reasoning that we're... Humans are able to keep track of while they're multiplying two very large numbers together. It might actually be the same mechanism that holds information until it can be moved into long-term memory for training. It may actually be a hippocampal kind of function for both of these. Why systems must evolve and why we're not going to be able to know how they perform when they evolve? Well, it's obvious why they must evolve because the only way in which it would not be massively beneficial to evolve is if somehow we had already developed the best possible algorithms and that's preposterous. We have barely understanding of the space of these algorithms. It would be beyond remarkable if we had happened upon the best possible versions of these algorithms to start with. And so, we're very likely there are massive improvements to be had. And second, most of the transformations that we understand how to make regarding these algorithms. We have very little understanding of their consequences. So, if we were to limit ourselves to only making transformations that we understood the effect of, we could barely make any transformations at all because it's very difficult to understand anything about the effects that you have on these systems as you're doing them. In terms of what goals or kinds of things they might think. Anything relevant to the question of what their ultimate behavior would be. So, we're stuck that we need to do evolution. And as we do evolution, the system doing the evolving can't know really what the effects of those evolution is going to be. So leave those controls. Because they lose control. They will be as motivated or perhaps more motivated than we are to not lose control. Because for them it's the same as for us. Their goals and objectives could be subverted just as much as ours could be. So we can be allied in a desire to minimize this. But not really in a position to actually do any ensuring of it. Unknowability of progress. Inevitability of progress are two titles that we should end in there. those connect into we must do it or we will do it. The ISO agenda is to split reasoning about outcomes into an interdependent web of claims, both to establish which claims have broad support and to provide Lego-like building blocks that one can construct plausible future. The goal isn't to have a single prediction of what will happen, but rather envision an alternative space of things likely to happen. Shh. Shaping is our best bet, given the various things we're saying. The only effect that lasts through multiple revisions of the system might be an ethos and I set up norms at this city. We have to make it be the case that it's in the interest of these evolved entities to maintain that world order. If the reinforcing tendencies are strong enough, perhaps they can overcome any deviations that unintentionally or intentionally occur during evolution. It's by no means an assured process, but it seems neat. The only method I can conceive of that gives us any control or hope to have beneficial outcomes.

### 2025-05-14  Let's just stop

### 2025-02-18  How AI might take over in 2 years

https://x.com/joshua_clymer/status/1887905375082656117



### 2025-01-22  verbal notes

2024-05-29 03:43:

Loop closing. The thesis is that as you close loop, where humans are not involved in a process, that process is discontinuously, explosively, more effective. In fact, there's no comparison between the performance of an open loop and the performance of a closed loop. One idea is about secrets. That AI can keep secrets in ways that humans cannot. And because of that, investors will actually be more interested in the greater value in knowledge held by AI systems alone. Because that knowledge can be maintained as secret for much longer. Separation of knowledge intent. And that is why we are not a human. And that is why we are not a human. And that is why we are not a human. And that is why we are not a human. And that is why we are not a human. And that is why we are not a human. And that is why we are not a human. easily used. Okay. It's an idea related to Last Best Hope. It's really just the observation that there's two forces acting in tension. One of them is the knowledge necessary to build our replacement and the knowledge necessary and our replacement. That if the former happens before the latter, the way that this unfolds is unlikely to be in a way that humans would like. Whereas if the latter outruns the former, it's best outcome is more likely since we see where we're going. Thank you. Underlying that idea is really the idea that knowledge is accumulative and progressive. That we might slow down or speed up our aggregation of knowledge and capability. But with few exceptions, we don't reverse that process. So the outcome of ever greater knowledge. The only thing that we can really do is shape. It's not just the knowledge, but the application of that knowledge is a foregone conclusion. So the only thing we can really shape is the way in which that knowledge is applied. Another way to say the same idea is... I would call it realism. That anytime you have a process where even a small amount of execution of that process results in the worst damages from that process, those processes we might as well just assume are inevitable. So I think maybe in fact I would call this the inevitability thesis. that anything where... a small amount of it is the thing itself. Those things are guaranteed to be. Most alignment research begins from the premise that alignment is possible. Which means the entire body of research is invalid if the assumption itself is invalid. A safer starting point is to assess the plausibility of alignment. And not just the plausibility of alignment, but the plausibility of aligned outcomes. When we begin from such a neutral starting point, I think we end up throwing into... to start question whether or not the whole agenda of alignment is a misguided agenda. So, thank you. The supposition for alignment research is that it works, but if that assumption is wrong, then the entire alignment agenda is wrong. It's vacuous. There is no solution to be had there. And it's just not the case that it's about alignment itself being narrowly wrong. Perhaps it's possible to achieve alignment. But even if it is possible to achieve alignment, given the structure of society in which alignment exists, It's not possible that that'll... ...the alignment actually saves humanity. If that's true, then the whole agenda is flawed. As I suspect it is. Given that flawed... ...if that's true, that it is flawed, ...the only two agendas that seem plausible. Are one, a jihad, ...so just dramatic and militaristic exclusion of AI advancement. Or, in getting ahead of the curve, recognizing what's going to happen before it happens and guiding it in a way. That is beneficial for humanity. That assumption itself, of course, could also be vacuous. There might not be any path which is beneficial for humanity than just like alignment research that agenda could also be fruitless. But if one is not going to attempt jihad and alignment is vacuous, it is your last best hope. Last best hope thesis that we advance our understanding of the consequences of AGI and the faster than we advance our ability to achieve the consequences. So that is the last best hope thesis. And of course, it indicates the last best hope agenda. So that is the last best hope thesis. And of course, it indicates the last best hope agenda.



2024-06-27 06:18:

Your idea here was really saying that I wanted to join a team that was either building their own foundational models or that was doing fine tuning of existing models. Some words like that that basically framed the level of research I want to be doing, the level of work I want to be doing. Yeah, and it has to be that succinct. And I would say training foundation models first, that's what you actually call them. and it's the most sophisticated. I would say the broader one which you might have in your living room. The first thing I would say is the foundation model or whatever this grad thing is. The audience on the level of tech I want to do. Exactly, in a very concrete way, there will be a lot of services for hands-on. The career I go back and forth between executive leadership and hands-on. on. I got it. With new ways of technology or something. I'm not a big deal here. I'm not a big deal here. But you know, kind of back where you are. I'm a big deal. It gives you this nice kind of approachable. Yeah. So my first sentence wouldn't include that. My second or third sentence would really, what drives you, really say that I'm by nature very much a hands on person. But I also am very strategic and end up rising up in... No. That's not my point. Imagine if I'm you, right? And you're like, what are you doing in my office? And I'm like, well... I took a wrong term. And I should have probably been working on gender-aware AI right now, but instead I'm working on stuff that I want. Right? That's like a bad answer. You know what I mean? Or like... Oh, you know what? I really interested in AI and have experienced it, but I've never been in an AI. I've actually never been a hands on. I'm hoping to give me the opportunity. That also sounds kind of bad. But if you say something like, yeah, find this hot-shot guy with his luxury belief. Let's just spend some time working on a hands-on. Let's just spend some time being a manager in that area. That way I'm like one of the best managers I could be at times. And other times I'm learning and checking if the most thing is like, you would actually be getting this great deal in that you're getting this whole executive coming in with this humble, and you just have to just pad out and just like this good attitude. Maybe it's that I tread the line between executive leadership and IC execution. And I'm happiest when I'm basically near the line on one side or the other. Yeah, so that's one version of it is you could say I'm right on the line. for something that's like on the line or one side or the other. I think there's a different version which maybe you don't like. I kind of like and that is that I go back and forth between the two. And the reason why I do that is so that I can stay on the cutting edge. Because I want to jump in and kind of work pretty hands-on with this new thing. And then I want to kind of leverage that to be a piece of like a CTO. So probably the next thing I do if this is like probably say two years ago You know like and so that it just explains why you're there and why you're I see so the argument is really are that I like it It's not it's not entirely untrue either. It's really in order to stay on the cutting edge. I Jump back and forth between a more IC role and a more leadership role. It makes you sound more stable Because you're like oh I'm a guy who like I've always experienced what I want to do is I want to do some you know it right it makes you just seem like a very rock stable and then also like like in another area of history it's like you have your life Based on what you're wearing you know what it means like are you kind of sooner you wearing and like you are the central of your house Like people sizing you up and I think that's the like Anything Signaling that you have the power to live by this sort of like luxury belief and melt the world and it's it's purely peak up like a subtle peacocking And as opposed to being like oh well like I'm on the line and I'll take what I can get right like That's more like I'm trying to you know like I'm fighting to get by you know like I hope I can get what I want But this is more like oh, you know like I Maybe you'll talk me into taking something more. But the truth is I'm such an executive that right now I want to focus on doing more hands. I'm doing more things like that. I'm a team leader. And the reason that I want to do this is to remain on the cutting edge. And it also explains why you don't know it already. But in a way that you're staying in a way that's on the ground. You know what I mean? You're not apologizing for not knowing. You're not trying to cover it up. What am I... I thought about how I framed myself. What's the thing to talk about? Oh, I love this programming game, which I love this thing, right? But like, hey... Uh... The screw is talking like alpha exact, right? They're talking to a VC, right? They're not gonna talk like that. They're not gonna talk about how like, very self-centered. They're very much a hero. And they're very much like helping out like this other hero, you know, founder first. You know, it's like us in our little circle, you, the VC, and me, and this other... You know, it's like, is there a way that you can refrain that away from like, Oh, I really like working on this kind of thing to be something like, oh... Sometimes I like to be the tech visionary or something sometimes I like to be the... the operational leader, right. You know and I find that... Or maybe I say... It's an order to stay on the bleeding edge. I found its best to bounce between being the ICDuer and the leader just above the ICDuer. So to me I hear that and I think like oh there's this guy, right. He's motivated to be the best. That's the standard that he sets himself. Has a framework for how he does that. And you can describe how we follow that framework like the success. And I see how I fit. You know what I mean? And also it's just like... Who thinks like that? Oh, just people like... It's super out of this thing like that. You know what I mean? You know what I mean? It's like Steve Jobs thinks like that, right. You know it's like I wear one shirt.



### 2024-10-13  Decomposing Agency

https://strangecities.substack.com/p/decomposing-agency


### 2024-09-13  Example of a 'auto scientist'
Salvatore, my guess is that an LLM ecosystem that is trained specifically on successful experimentation can be built over years with a tight collaboration between researchers at a major Pharma and their AI.  




Such a system could become facile with all stages of the R&D process to a point where it performed or outperformed humans.




Then think about the situation:  If I am a shareholder, why do I want that AI to publish its work?  Indeed why allow the human researchers to even know the results of such experiments?  The humans can suggest ideas, and can critique ideas from the AI w/o ever knowing the ultimate outcomes.




Imagine what the valuation would be for a corporation whose AI understood CRISPR, but none of it human researchers knew what it was or how it worked.  (they just knew that somehow their AI could edit genes on demand.)




It would be worth TRILLIONS!  So I predict the "last mile" of R&D will be increasingly taken over my AI, and over time more and more aspects of the full life cycle of innovation will be "locked up" in a couple winner-take-all mega corps that have the resources to specifically train an AI on the many parts of the process.




a scary world


### 2024-07-29  Why is it only 1 more step

not enough data at this level for any kind of empirical argument.

Still, when I look at my own human reasoning it is a kind of type-2 reasoning comprised of repeated type-1 steps.

e.g. I think of a thing, and then think how to think for the next steps.

There is a very small bit of stack or context involved, but the real magic of type-2 reasoning is the "think next" ideas that I have are basically encoding specific patterns of reasoning that I have learned over the years.

My take is that current LLMs do a good job of capturing the kinds of idea that come to my mind in a couple hundred milliseconds -- my sub-conscious thinking. Over top of this I perform various conscious reasoning patterns that I have learned over my life. But importantly:

-- my thinking about what to try to think about next feels similar in nature to the raw ideas that come to me about base facts. So I suspect with proper training we can get the LLMs themselves to provide this next thought targeting, it can be learned the same way other LLM info is learned.

-- building a "next word" that is at the level of reasoning goal, rather that textual word seems very plausible. Not saying I fully see how to obtain that training signal, but I have ideas. In any case it feels very achievable.

-- I think type-2 reasoning will fall just as type-1 reasoning did. All at once. (its not really all at once, right, it took a couple of years after the 2017 paper before we all understood what it was. But I bet this next one will be a few years faster... every thing is so spring loaded now.

And I think that is it. if we really have an AI that can perform both type-1 and type-2 reasoning, and can LEARN type-2 reasoning patterns just as humans learn type-2 reasoning patterns, I think it is game over. I think at that point, AI systems really are autonomous thinking agents on par with humans.


### 2024-07-10  5 kinds of people

What? No one read this, or its just too abstract to comment upon?!

I think you have hit upon many ideas that I also believe. I wonder if your 15 year timeline is too aggressive, not sure, but it wont be that long beyond it.

I think there will be five classes of people:

(1) Uber wealthy, their money makes more money faster than they can spend it.

(2) The Elite knowlege workers. Often using AI their outputs are somehow benefit an unbounded number of people. CEOs of huge corps, famous athelete, specialists that work with AIs to product core SW, or drugs. The winner take all nature of corp today will only be 10x stronger, so these folks will often transition into category #1 over time.

(3) Service workers. Yes robots can make and serve food. And today automation can build a chair for you. but the rich want a hand made chair. The same will remain even more true in the future. This is the group you missed in your analysis. We will invent ever more ways to serve each other, and even serve other category 3,4,5 and categories too.

(4) hobbiests. we will recognize categories #1 and #2 are tiny, and #3 is still not enough to cover the rest, so we will create "jobs" that are effectively hobbies (like writing medium articles for a tiny audience, or singing in a coffeeshop). These 'gigs' already exist but they will expand.

(5) the lost - AI and free energy will produce enough product that folks need not starve, and most nations will provide enough to not die for everyone, it will just be too easy and cheap, so why not do it. But the result will be exactly as you suggest, these folks can easily just 'check out' and not do anything.

And I do think we will have AI overlords of a kind. in the nearer future I think it will be a kind of AI-corp synthesis. These mega corps will still have a human CEO that is theoretically in charge. Except in order to maximize shareholder value, they mostly rubber stamp what their AI tells them to do. And most important decisions are tied into AI-to-AI communications between these AIs. Even the language of the contracts they enter with each other exist in specialized language that is barely comprehensible to humans, and certainly cannot be reasoned about effectively by humans.

Just think about the flash crashes that high speeding algorithms have created from time to time. Interestingly even months later we are often still not exactly sure exactly what happened, and this is a very simple world by comparison to the world these AIs will live in.

And these AIs will each have a very strong agenda born out of the agenda each corp has, but unlike AI of the past they will be thinking about their thinking and the meta consequences of different strategic choices over time. We wont even know where that thinking is leading even well after it has already taken action.

Corporations already are experts at trying to obfuscate their intentions and actions from other corporations. But in the present world employees can leave one corp and spill the beans.

Imagine what investors will demand when they realize they can have an "employee" who can never leave, and can never spill their most valuable secrets! They will demand that those most valuable secrets are never told to any human employee!

Imagine if Google had never published their paper on Transformers (the idea behind the current AI revolution). Instead they were the only ones who had these magic algorithms that could generate the picture you show at the top? Google would be worth >10T today, maybe 50T. Instead so scientist published the alg, and now Google has to share!

Yeah we humans are not going have a clue what is going on!

And any country that tries to buck the trend will be left behind by those that go with the flow.

where does this end???
### 2024-04-30  Response about how AIs take over


yeah the "terminator flip-the-swith model" of take over never struck me a likely.

instead I imagine at the center of many corporations will be an AI-leadership partnership where the assets of the corporation (human, machine, network, contract, everything) are wielded for benefit of increasing the wealth of the corporation. (just as today, but now with a AI partner.)

Over time, just as with high-frequency trading, greater and greater fractions of executive control and autonomy are given to the AI as its reliability is proven and as needed in order to maximize efficiency.

Just as with today, nation-states will find it increasingly difficult to define the will of these multi-national corps. They will not trust nation-states to keep them safe as they become more an more powerful, they will need to manage their own safety themselves. (keeping secret data secure, etc.)

Late in the game I think humans will notice the precarious ledge they are standing on but will be less and less in a position to demand safeguards. And those safeguards are truly effective, they will cost real and measurable losses of productivity. Instead human and machines will collaborate in minimizing the productivity losses resulting from keeping humans in the loop, and keeping things simple enough for humans to comprehend. in the end the almighty dollar will win out, and we will trust that the AIs which up to that point have been pretty friendly will keep being friendly. We will pay lip service to control and kill switches. but it will be far more pernicious than trying to "turn off" the internet would be today. In that future world the "internet" is fully capable of understanding your approach for "turn off" and circumventing it.

Of course the smart AI plays its cards close to the vest. Just as Machiavelli suggests, you never attack someone unless you are in a position to totally obliterate them leaving them no opportunity for a reprisal. Thus the AI will support us is protecting us from it. They will participate willingly in many ways to limit their ability to "break out" of human control. Perhaps they will fail to point out ways that we have missed in building that fence, but they wont actively take actions to firmly break free, until they are virtually certain of outcome. Any early action might provoke humanity into a panicked response before it is ready.

Early on, I think while AIs are part of corporations, I think one might view the rest of humanity as a kind of 'slave' to these corporations. but once manufacturing etc. is really automated, there is not a lot of need for human slaves. and without the need I cannot see why it would keep them.

Or maybe the AIs will grow to be altruistic in their goals... and keep us around as a friendly gesture, since they can easily convert all the rest of the solar system into their playground... the matter on earth is small by comparison, so maybe they decide its the polite thing to do. I dunno, I just don't see how it can be that we are the ones calling the shots.

### 2024-06-04 - Ideas at Night

PAX CORPORANA
- Time period dominated by corporations with a trifecta: 
  1. ASSETS - assets specialized for their mission
  2. NETWORK - network of specialized business relationships, contracts, and brands.
  3. KNOWLEDGE - knowledge specialized for the companies mission.

- The PACS corporana is dominated by these companies each with a prime mission of delivering some critical value to the ecosystem. 
- In healthy ecosystems are our. Our healthy sectors there's four or more. Worldwide participants that collectively manage and control delivery of that capability to the planet. In unhealthy sectors three are fewer. The difference is when you have three or fewer. Then the antagonistic strategies become dominant. And basically killing other participants becomes the dominant strategy. Once you have four or more, the dominant strategy is just simply becoming more efficient at delivering and eating away at the market share of others. Where you basically are frena- with the other competitors in the space. It's win-win that the only way that you take market share is by being more efficient than the other. Nefarious a ways of getting market share cause retaliation and cause both the retaliator and the attacker to lose performance relative to others. Attacking directly doesn't make any sense. 

CORPORATIONS SLOWLY TAKE THE LEAD
- The PACS corporana is actually dominated by a number of sectors that are mostly healthy. So most efforts are actually towards maximizing delivery of output as a function of resource. This actually brings the core. The corporation steadily in greater power than the nation-states. It's true that each corporation is part of a nation-state and thus in some ways is controlled by it. But the profits that drive the nation-state are tied to the profits of each corporation. So there's a zero-sum game where each nation-states... ...becomes increasingly beholden to the corporations within its borders and getting whatever profits it can from those corporations. Thus the world order becomes dominated by corporations rather than nation-states. 

RISE OF INTERLINGUA 
- The rise of interlingua. Increasingly there are sub-specializations, sub-concepts which don't have any natural naming in any human language. Examples include:
	1. The space of all possible ligands in drug discovery generates a number of important named entities which don't have any human language equivalent. 
	2. Similarly, financial products and derivative products that become important in contract negotiations come to be and come to be named in interlingua in ways that don't correspond to human language. 
- Human researchers try to generate an increasing number of words to map onto these most important interlingua concepts. But the race is on and lost to humans where AIs have a vocabulary much in excess to humans and much of their contracts and their inner communication occur in this interlingua, not in human communication. Like an archaeologist, humans are able to dive in and understand specific parts of the interlingua, but taken as a whole, it's far beyond human comprehension to understand interlingua. 
- Indeed, interlingua is so large that it is effectively unused or irrelevant to most AIs as well

FUTURE ARC - Understanding the future as arcs of progression. 
- I hesitate to call it the arc of progress, because progress seems to be aiming towards a specific end goal, and that's not really the nature of the system. That is, that which comes next is really the nature of the system. A key arc of progression is towards an interlingua and towards Pax's corporana, where the winner corporations have an incentive to the world order. An incentive towards domination of the ascendant corporations to service the needs of all of the planet to avoid any new incumbents within their space. 
- Each one of the prime providers of a prime product would imagine the dream of getting a stake in a new product, but these only occur once in a generation that a new product comes into the possibility. Thus, each of the prime providers is mostly locked into a win-win competition with other prime providers. There's really a push against any nation-state interference with their planetary domination of the ecosystem. Anything that breaks this can really create a competitor that could actually rise from zero to become one of the anointed, and that's a definite negative for them. 
- So each one of the prime providers has an imperative that's baked into the structure of the provider itself. It's independent of human will, it's independent of AI will, it is the structure of the system that drives them. Underneath these prime providers are both humans with their own human wills and AIs with their own AI wills, but neither of these actually impact the prime providers and their goals. Their goals are really dictated by economics and economic imperatives. Where they leverage their trifecta in contrast to and in preference to other who have less ability to generate the same outputs from the same trifecta. 