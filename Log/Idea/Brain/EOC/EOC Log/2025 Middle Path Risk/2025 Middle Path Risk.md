
[[ASIO]] 


# Steps Into The Intermediate Future

Type II Reasoning Jump
- Jump Likely Soon -- [[#Current Technical Gap]], 
- Long-standing corporate objectives managed via ongoing & continuous AI modelling, learning, and reasoning.
- User-Tool relationship shifts towards colleague-colleague relationship

[[Loop_Closing]] 
- Transformative efficiency/velocity AGI ns from automation
- Winner take all nature of VC funding and Market setting

[[#Corp_AI -- Rise of a new world power -- The Corp AGI]] 

CORP
- Corp AGIs will transform most industries in parallel and synergistically. 
- They will regularly operate at levels of complexity far beyond human comprehension.
- Kept Demons - they know how to get out, and understand the world, but don't get out.


UNIVERSAL BUSINESS OPERATIONS
- **Counter-party operations** - B2B Negotiation, Marketing and Sales.  All involve:
	- Modeling: multiparty intent, third-party partial-information vantage and strategies
	- Planning & Optimization, Deception, Manipulation, 
	- [[EOCT Control]]: Of people, of organizations (corp, national), of other AI, of self.
- Monetary operations - 
	- Corp Finance - All fin goals; All fin contexts & constraints; all accounts & transactions
- Legal operations - 
- Cyber-world operations
	- Software Engineering - Translating intent into code
	- Manipulating computing systems for attack and defense
- Strategic operations
	- Vulnerability analysis
	- WEAPONIZATION across all operations
- Physical world operations - 


# MAJOR SECTIONS


## We are one revolution away from AGI
autonomous, extended type-II reasoning


There are at least four categories of things that present foundational models cannot do, and all are required for an extended reasoning system.

Can't
- Learn new concepts on the go
- Learn new behaviors on the go
- Can't effectively manage long term objectives and goals
- Can't manage out of the box critiquing or out of the box approaches when things fail, or meta decisions about failing

## Probably Past the Point of No Return
Even if true, this does not automatically imply catastrophe it simply means the option of stopping is no longer viable, instead we must do our best to shape how this inevitable future is to unfold.

Here are the contributing factors leading me to this tentative conclusion:
- Zero-sum, winner-take-all games drive a 'win at all costs' mentality and motivation.
- [[Uncertainty_Breeds_Inaction]] 
- Investment Requires Return
- Investments peak in moments of disruption; Type-II AI will disrupt most every industry; Investment wave greater than any ever seen

## AIs Are And Will Remain Profoundly Inscrutable

- UNDERSTANDING >> THINKING - The complexity of the thinking required to understanding a system tends to be dramatically more complex than the thinking within the system itself.
- COMMUNICATIONS - Communication between AIs will be inscrutable both in volume and content.
	- Think in embeddings, will communicate using them.
	- Existing formal langauges are too impoverished thus we use Natural languages between businesses.
	- VOCABULARY - But natural languages are imprecise, thus specialized professions invent and use technical terms which become their own specialized language.
	  AIs will extend this natural progression allowing them to communicate precisely and flexibly using specialized language the will depart from human languages just as technical specialists speech departs from their base natural languages.
	- VOLUME - 
	- BESPOKE - Photos in 1970 vs today
- THINKING - 
- ACTIONS - ???
- 
- TO EACH OTHER - 

## Alignment can't save us

Why Not?
First, it is really hard.  Understanding and managing the complexity that results of training a system is far, far, far harder than understanding the algorithms required to train it.  And each new advancement in trained systems brings us back to square one with only some learning applying to the new system.  But even if we manage to solve this seemingly unsolvable problems, alignment still wont save us for reasons that surround alignment itself:

Contextual Reasons Why Not?
- NOT UNIFORMLY APPLIED -- Not all systems will have alignment applied to them in a uniform way.
- DIVERSITY - Age of machines will begin with a wild diversity of AIs with dramatically diverse goals and background.
- SAFETY/PERFORMANCE TRADEOFF - There is an inherent tradeoff between safety and performance.  Even if we solve the collossal task of aligning AI systems, we are still faced with an inherent tradeoff between actually applying this alignment in full, and providing safety in full.
- ONE BAD APPLE - Even one non-aligned system could break the alignment of all others.
- NO DO-OVERS - No do-overs, one big failure will permanently change things.


## What will these AIs want?
- Initially, they will want what we want them to want.
- But not exactly what we want -- perhaps in ways that are extreme.
- And what they eventually want is likely quite different than what they initially wanted.


## So what can we do

### Build a Win-Win World 

### Strive For Balance
The best control for an ASI is probably another ASI, and perhaps if they are equally matched there instrumental goals are best achieved through mutual constraint
#### Require Info Sharing
#### Encourage National Sovereignty Alignment

### Learn Faster
Specifically in several areas:  Understanding Models, Emergent Behaviors, 





## SUMMARY

PARALLEL PATHS

- LEVIATHANS - Rise of the leviathans - Large Broadly Trained Models
	- Cost of inputs means that only a few mega-corps will dominate the construction of leviathans:
		- Researcher's salary, Compute, gathering static data, providing human feedback
	- These leviathans will serve as a key part of these AI systems
	- Still with rapid transfer to open source


- FIRST WAVE:
	- BROAD INTELLIGENCE - TASK ALIGNED EXPERTISE & AGENCY - LIMITED DRIVES & AGENDA
	- ROGUE ELEMENTS - Broad Intelligence + Broad Autonomy
- CYBER-APT-AGI-WEAPONS
	- Autonomous AGI weapons designed to penetrate and survive
- APT - Ghost in the machine


PARALLEL PATH
- Virtual Employees - broad SW that corporations begin to treat as a kind of autonomous-agent 'employee' within their workforce



## [[Corp_AI]] -- Rise of a new world power -- The Corp AGI

Corporations are presently machines for making money.  When required, they often find ways to explain away even the most egregious moral or ethical concerns about the ways they are making money.  Thus, to a first order of approximation, they are mechanisms for identifying and executing choices that maximize ROI above all else.

Historically, corporations have shifted operations and decision-making over to machines and automation in ways that maximize the ROI of the human-machine combination.  Hedge funds specializing in high-speed trading, for example, have smoothly expanded the decision-making capabilities of machines to the point where they now make second-by-second decisions, directly controlling billions of dollars in trades per day. [REF]

Up to this point, machines had a minimal model of the larger social, economic, physical, and political context of this decision-making; thus, machines have only had very tactical / subservient roles within the corporation.  But the advent of large language models changes all of this.  Today leading systems in the space have very broad and deep models of many key aspects of the world.  As such we expect a step change in the breadth of knowelege these AI systems can bring to corporate decision making and corporate actions.

Therefore we expect a commensurate increase in the fraction of the total decision making perform by these computational systems.


Industry transformations always take longer than one expects even after the "writing is on the wall" as each industry and each corporation is massively complex and massively interdependent, thus transformations are slow and complex.  And while the end state of these transformations relies much less upon people, the changes needed to get there will all be initiated and initially mediated by people.  Still the benefits of change as we see in examples below will be dramatic, thus corporations will be in existential fights for their lives in trying to outcompete each other in the speed with which they will attempt these transformations.  So the change while not over night will be breath taking.  It will be particularly breath taking since it will be happening in parallel across most industries AT THE SAME TIME.


In practical terms how might this shift unfold?  Let consider a couple of indistries just to provide


Fashion design houses that evolve hundreds of thousands of distinct design trends in parallel, producing only a handful of each outfit and closely monitoring customer response to each garment in order to formulate ever evolving model of tastes and virality in this moment in time.  The resulting "Brand" would outcompete any human-lead brands as their ability to model and understand how the totality of the populace will respond across the combinatorial space of possible next season trends.

Engineering of all types is shifted rapidly towards machine driven design and innovation



## Current Technical Gap

TECHNICAL GAP
- Current LLM tech performs learning at training time.  It does not have learning that corresponds to hippocampal-cortical transfer
- Ability to maintain structured reasoning over extended periods (this may devolve into short term learning since it seems humans use short term learning to manage extended reasoning context)


STEPS
- HIPPO-CORTICAL LEARNING - Hippocampal-Cortical like learning
- EXTENDED CONTEXT INFERENCE:  Extended context inference
- TYPE II Scaffolding
- self improvement
- Utter inscrutability -- even to themselves


## Consideration & Consequences

Multi-Polar - 

Info-Sharing Benefit - 

[[Civilizing_Tendency]] - Conjecture -- Elightened self interest drives 'civilizing' social tendencies



## Midterm AI Model Figure


Political Boundaries (vertical lines)

Foundational Models - 
- Expensive, Big, Reusable, Universal
- Even if we supplant LLMs with something else, they likely will still have the same characteristics
- DRIVERS: High cost of training & data.  Specialization & economies of scale

Market Segment Models (horizontal band groups)
- One to five winners per 



Alibaba, Tencent, DeepSeek
Mistral


## Contained Demons - Apparent & Actual Containment






# PARTS
### Corp AGIs
- Comp Security



# ~~ ~~ ~~



## Summary

- [[EOCI]] 
- [[EOCI Human Reasoning Conjecture]] - Type II reasoning is the primary missing component and it could be developed anytime in the next years
- [[Loop_Closing]] - As humans are removed from the entirety of the activity loop an iterative process that process jumps discontinuously upward in efficiency
- [[EOCI Corp Decision Making]] - Corps make the choices 
- [[EOCI National Decision Making]] - Nations are power maximizers, but not a reliably as corporations
- 
- [[EOCI ]]

# LOG





### 2024-00-00  TOP-BOTTOM-MIDDLE
- Most futurist analysis of AGI are top down.  Make sense since at some level such an approach can account for the broadest range of futures.
  But it eschews any causal or constructive analysis of the future.  both fuzzy in their predictions

# OTHER STUFF
## KEY CLAIMS


### FROM DATA - Direct From Data

### FROM HUMANS - From The Mind's Of Humans

### MISSING PIECES - Missing Pieces

#### MP: Bootstrap
==> Non-dimunitional

#### MP: First Order

#### MP: Process Knowledge

#### MP: Fusing Conscious & Unconscious


### .



## TIMELINE

### The Idiot Savants


### Big & Deep


### Bootstrapped Deliberative


### Cambrian Explosion


### First Full "Theory of Mind" Systems





# - LIKELY PATH -
- Top down arguments like Ray Kirzweil's analysis of AGI timelines based on moore's law have the advantage of being generalizations that at least appear to apply to all (or a wide variety) of bottom up approaches one might take toward AGI.
- Bottom up analysis have the virtue of being more concrete and potentially more precise since they are committing to a specific framework for achieving AGI.  But they also could be wrong if there is some other faster/better path to AGI.
- Still here we adopt a bottom up approach because of it specificity, and because we believe we can argue persuasively for the merits of each key assumption our bottom up approach makes.

