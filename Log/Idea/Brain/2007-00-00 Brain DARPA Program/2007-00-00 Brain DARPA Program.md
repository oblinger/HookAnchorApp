
# PAUL NOTE  -- NEvER SENT


* 130% agree.  Progress in bigAI is best served by focusing on satisficing not optimizing.
* Agree that:    Sophistication = Ability + Learning.
* Agree w. implicit stmt:  focus is not on new stats algs, but rather satisficing applications
                           of existing ones to addressing *range* of less sophisticated tasks.

I WANT TO ADD:

* Five year old capabilities are themselves a form of sophistication that was learned.  So
  FiveYoCaps = Sophistication = MorePrimativeAbilities + Learning.

* The kind of learning required to get to 5yo capabilities is of a same kind, and nature
  as the kinf of learning that takes 5yo abilities and builds adult sophistication.

* If anything the, the task of going from more primativeAbilities to 5yo is *easier* learning
  than going from 5yo to adult capabilities.  (the one issue is understanding the more
  primative abilities.)

* So I am happy to focus on 5yo capbilities, but I think nearly 100% of the value of this
  investigation is not an investigation of the abilities needed, but rather the LEARNING
  needed to achieve those abilities.  (But when I imagine what I mean by "learning" here 
  is close to an ability itself.  it is NOT focused on increasing predictive accuracy,
  rather it is specifically tuned to other indicators.  (e.g. where are they looking right now?
  can I control that state property?  what does that remind me of?  what comes next?  can I do that again?
  etc.  I am framing it as learning, cause I am interesting in its climbing properties, not its
  static task performance properties.  That said, I am happy that one should measure whether one has
  achieve learning, by testing on an unbounded number of 5yo tasks.

* So as long as all tasks addressed by our 5yo are address by applying ONE SIGNIFICANT LAYER
  of learning over top of some more primative ability, then we are making progress.
  We are getting closer when we can string together multiple layers of satisficing leraning 
  to achieve increasing task sophistication.  

* But I reject the idea that we are so lost in the wilderness, that we dont even know how to
  do the learning, and we must start by simply hand coding abilities.
  You are smarter than I, and I know that I can generate single layer satisficing learning 
  step to underly any hand coded task ability that you might want to encode.
  (I am not sure if you ever meant to imply that we didn't know how to write the learning layer or not
   but my position is that we are not lost in the wilderness, that we do know how to write many 
   SATISFICING specialized learners.)

* Thus, given that nearly 100% of the value comes from understanding the learning step, not including
  the learning step is not indicated.  That said, it is not important that we start from raw features.
  Don't take me as implying that there is some magic that comes from rawness.  That said, below I 
  consider the pros and cons of working at the raw feature level.



WHAT TO SAY ABOUT LEARNING FROM RAW FEATURES

* There is nothing magic about raw features.  If one can learn many layers on high level 
  features in a general way, then either you can do so from low level features, or with
  small change to your basic leraning you can do so.
 
* Indeed raw features are such a redundant representation, of such simple concepts, that 
  learning at this lowest level is EASIER 

* Thus if one really solves the learning task, then the task of going from raw inputs,
  is not harder (execept a constant multiple increase in computation at the low end).
  If it turns out it is harder, then we have not really built a complete enough learning
  step.

