# 07-neal.Figure-descriptions --

    Note to Deb&Neal: technically these figures are a data flow, not control 
    flow (but perhaps that is too minor of a technicality to be noticed. 
     
     
     
    PATENT #3 
     
    The overall process described by Figure 1, is the process of annotating 
    a specified user interaction state with a set of context parameters. 
     
    User Interaction Records (#1) contain traces of previous interactions 
    with users of the system.  These traces include all types raw context 
    information (#5) that were available during those interactions, as 
    well as the user validated contexts that were applicable during those 
    sessions. 
     
    This information serves as a training set for learning of context 
    mapping functions (#2).  During this step, a function is learned for each 
    context term.  These functions compute a value for each context term, given  
    the raw context information (#5).  The list of context attributes to be 
    learned is listed as (#3), and this is generated by patent #7 (NOTE: use 
    real patent number).  The output of this process is the set of derived 
    context attribute functions (#4).  This entire sequence of action, may  
    occur offline. 
     
    Context application (#7), on the other hand, must occur interactively,  
    during user interaction.  In this step, the raw context (#5) is used to 
    compute the value of all context terms for a particular user interaction. 
    This process executes mapping functions in (#4) against the raw context 
    information (#5) generating a set of context attribute values (#8) associated 
    with each raw context (#5). 
     
    Note that the raw context (#6) is simply a convenient way of collectively 
    referring to the individual sources of context information shown in #5. 
     
     
    PATENT 4 
     
    The overall process described by Figure 1, is the process of finding a set 
    of resources that match a users request in a given context. 
     
    User Interaction Records (#1) contain traces of previous interactions 
    with users of the system.  These traces include all types context 
    information (#3) that were available during those interactions, as 
    well as the feedback generated by the user regarding the resources 
    that were provided during those sessions.  (Feedback, for example, 
    could be a specification of which resource were chosen by the user 
    given a list of displayed resources.) 
     
    This information serves as a training set for learning the indexing 
    function (#6).  During the adaptive indexing step (#2), a function is 
    learned that maps user context vectors (#3) obtained from Patent-3, 
    with the user query (#4) onto a response set (#8).  The adaptive 
    indexing optimizes the performance of function (#6) as measured by the 
    feedback in the interaction records (#1).  For example, one measure of 
    performance is the coverage of the resources eventually selected by a 
    user give a query/context pair.  In this case an indexing function has high 
    performance if the result sets it suggest often contain all resources selected 
    by the user.  This adaptive process (#2) need not be interactive, but can 
    be done offline. 
     
    Resource lookup on the other hand, is executed interactively, it is 
    the application of the lookup function (#6) to the user context (#3), 
    user query (#4), and Resource Library (#5).  The results of the process 
    is the selection of a subset of (#5) that are relevant to the users 
    query.  The subset is called the response set (#8).  These are the 
    resources presented to the user. 
     
     
    PATENT 5 
     
    The overall process described by Figure 1, is the process of ordering a  
    result set. 
     
    User Interaction Records (#1) contain traces of previous interactions 
    with users of the system.  These traces include all types raw context 
    information (#3) that were available during those interactions, as 
    well as the feedback generated by the user regarding the resources 
    that were provided during those sessions.  (Feedback, for example, 
    could be a specification of which resource were chosen by the user 
    given a list of displayed resources.) 
     
    This information serves as a training set for learning the ordering 
    function (#7).  During the ordering annotation step (#6), a function 
    is learned that maps the response set (#3) obtained from Patent-4, 
    with the user context (#4) onto an annotated response set (#8).  The 
    adaptive annotation algorithm (#2) optimizes the annotation scoring 
    metric (#5) as measured by the feedback in the interaction records 
    (#1).  For example, one measure of performance is closeness of the 
    selected items to the top of the response set (assuming that the 
    annotations of the response set specify an ordering of the response 
    set).  This adaptive process (#2) need not be interactive, but can be 
    done offline. 
     
    Ordering and Annotation (#6), on the other hand, is executed 
    interactively, it is the application of the ordering and annotation 
    function (#7) to the user context (#3), and response set (#4).  The 
    result of the process is annotations (#8) for the responses in the 
    input response set.  These annotations control the presentation of the 
    resources to the user. 
     
     
    PATENT 7 
     
     
    The overall process described by Figure 1, is the process of discovering and 
    selecting among new user context terms. 
     
    User Interaction Records (#1) contain traces of previous interactions 
    with users of the system.  These traces include all types raw context 
    information (REF from main patent), user queries (???), the system's 
    responses (???), and user feedback.  These interaction records are 
    clustered according to an externally specified distance metric (#7). 
    This metric defines how similar two user interactions are.  It may 
    focus on how similar the result sets are given that this is the part 
    of the user interaction we are using contexts to predict.  Similarity 
    of other aspects of the interaction, for example, bandwidth of user's 
    connection might be an aspect of the user's raw context that might be 
    relevant to clustering user interactions as well.  Thus the clustering 
    metric should measure distance with respect to all of these parameters 
    of the user interaction records. 
     
    The output of the clustering step (#2) is a set of potential new 
    context attributes (#3).  Each potential attribute is a set of related 
    user interactions.  User context administration (#4) provides a 
    browser where a human administrator can view these sets, and when 
    appropriate, create new context attribute names, and then assign 
    records in #1 values for those attributes.  These context assignments 
    serve as the training data for patent-3.  The output of the user context  
    administration (#5) is these updates to the user interaction records (#1), 
    and a new set of derived user context attributes (#5). 
     
     
